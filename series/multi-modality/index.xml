<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Multi-Modality on 特倫蘇的日與夜</title>
    <link>http://localhost:1313/series/multi-modality/</link>
    <description>Recent content in Multi-Modality on 特倫蘇的日與夜</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en</language>
    <lastBuildDate>Mon, 05 Aug 2024 13:43:06 +0100</lastBuildDate>
    <atom:link href="http://localhost:1313/series/multi-modality/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>🧑🏿‍💻 Multimodal Representation Leraning from both Text and Image</title>
      <link>http://localhost:1313/post/scientia/multi-modality/</link>
      <pubDate>Thu, 08 Feb 2024 00:15:00 +0000</pubDate>
      <guid>http://localhost:1313/post/scientia/multi-modality/</guid>
      <description>Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.</description>
    </item>
  </channel>
</rss>
