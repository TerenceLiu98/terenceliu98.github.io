
@article{cai_msgnet_2023,
	title = {{MSGNet}: {Learning} {Multi}-{Scale} {Inter}-{Series} {Correlations} for {Multivariate} {Time} {Series} {Forecasting}},
	shorttitle = {{MSGNet}},
	url = {http://arxiv.org/abs/2401.00423},
	abstract = {Multivariate time series forecasting poses an ongoing challenge across various disciplines. Time series data often exhibit diverse intra-series and inter-series correlations, contributing to intricate and interwoven dependencies that have been the focus of numerous studies. Nevertheless, a significant research gap remains in comprehending the varying inter-series correlations across different time scales among multiple time series, an area that has received limited attention in the literature. To bridge this gap, this paper introduces MSGNet, an advanced deep learning model designed to capture the varying inter-series correlations across multiple time scales using frequency domain analysis and adaptive graph convolution. By leveraging frequency domain analysis, MSGNet effectively extracts salient periodic patterns and decomposes the time series into distinct time scales. The model incorporates a self-attention mechanism to capture intra-series dependencies, while introducing an adaptive mixhop graph convolution layer to autonomously learn diverse inter-series correlations within each time scale. Extensive experiments are conducted on several real-world datasets to showcase the effectiveness of MSGNet. Furthermore, MSGNet possesses the ability to automatically learn explainable multi-scale inter-series correlations, exhibiting strong generalization capabilities even when applied to out-of-distribution samples.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Cai, Wanlin and Liang, Yuxuan and Liu, Xianggen and Feng, Jianshuai and Wu, Yuankai},
	month = dec,
	year = {2023},
	note = {arXiv:2401.00423 [cs]},
	keywords = {/unread, Computer Science - Machine Learning},
	annote = {Comment: 13 pages, 12 figures},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/II22H2X7/Cai et al. - 2023 - MSGNet Learning Multi-Scale Inter-Series Correlations for Multivariate Time Series Forecasting.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/YMREG4FR/2401.html:text/html},
}

@article{rout_generative_2022,
	title = {Generative {Modeling} with {Optimal} {Transport} {Maps}},
	url = {http://arxiv.org/abs/2110.02999},
	abstract = {With the discovery of Wasserstein GANs, Optimal Transport (OT) has become a powerful tool for large-scale generative modeling tasks. In these tasks, OT cost is typically used as the loss for training GANs. In contrast to this approach, we show that the OT map itself can be used as a generative model, providing comparable performance. Previous analogous approaches consider OT maps as generative models only in the latent spaces due to their poor performance in the original high-dimensional ambient space. In contrast, we apply OT maps directly in the ambient space, e.g., a space of high-dimensional images. First, we derive a min-max optimization algorithm to efficiently compute OT maps for the quadratic cost (Wasserstein-2 distance). Next, we extend the approach to the case when the input and output distributions are located in the spaces of different dimensions and derive error bounds for the computed OT map. We evaluate the algorithm on image generation and unpaired image restoration tasks. In particular, we consider denoising, colorization, and inpainting, where the optimality of the restoration map is a desired attribute, since the output (restored) image is expected to be close to the input (degraded) one.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Rout, Litu and Korotin, Alexander and Burnaev, Evgeny},
	month = mar,
	year = {2022},
	note = {arXiv:2110.02999 [cs]},
	keywords = {/unread, Computer Science - Machine Learning},
	annote = {Comment: ICLR 2022},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/9Q9EXZYX/Rout et al. - 2022 - Generative Modeling with Optimal Transport Maps.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/9L752P49/2110.html:text/html},
}

@article{li_clipsam_2024,
	title = {{ClipSAM}: {CLIP} and {SAM} {Collaboration} for {Zero}-{Shot} {Anomaly} {Segmentation}},
	shorttitle = {{ClipSAM}},
	url = {http://arxiv.org/abs/2401.12665},
	abstract = {Recently, foundational models such as CLIP and SAM have shown promising performance for the task of Zero-Shot Anomaly Segmentation (ZSAS). However, either CLIP-based or SAM-based ZSAS methods still suffer from non-negligible key drawbacks: 1) CLIP primarily focuses on global feature alignment across different inputs, leading to imprecise segmentation of local anomalous parts; 2) SAM tends to generate numerous redundant masks without proper prompt constraints, resulting in complex post-processing requirements. In this work, we innovatively propose a CLIP and SAM collaboration framework called ClipSAM for ZSAS. The insight behind ClipSAM is to employ CLIP's semantic understanding capability for anomaly localization and rough segmentation, which is further used as the prompt constraints for SAM to refine the anomaly segmentation results. In details, we introduce a crucial Unified Multi-scale Cross-modal Interaction (UMCI) module for interacting language with visual features at multiple scales of CLIP to reason anomaly positions. Then, we design a novel Multi-level Mask Refinement (MMR) module, which utilizes the positional information as multi-level prompts for SAM to acquire hierarchical levels of masks and merges them. Extensive experiments validate the effectiveness of our approach, achieving the optimal segmentation performance on the MVTec-AD and VisA datasets.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Li, Shengze and Cao, Jianjian and Ye, Peng and Ding, Yuhan and Tu, Chongjun and Chen, Tao},
	month = jan,
	year = {2024},
	note = {arXiv:2401.12665 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computer Vision and Pattern Recognition},
	annote = {Comment: 17 pages,17 figures},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/6X475FTS/Li et al. - 2024 - ClipSAM CLIP and SAM Collaboration for Zero-Shot Anomaly Segmentation.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/28IIWMRU/2401.html:text/html},
}

@article{janati_averaging_2022,
	title = {Averaging {Spatio}-temporal {Signals} using {Optimal} {Transport} and {Soft} {Alignments}},
	url = {http://arxiv.org/abs/2203.05813},
	abstract = {Several fields in science, from genomics to neuroimaging, require monitoring populations (measures) that evolve with time. These complex datasets, describing dynamics with both time and spatial components, pose new challenges for data analysis. We propose in this work a new framework to carry out averaging of these datasets, with the goal of synthesizing a representative template trajectory from multiple trajectories. We show that this requires addressing three sources of invariance: shifts in time, space, and total population size (or mass/amplitude). Here we draw inspiration from dynamic time warping (DTW), optimal transport (OT) theory and its unbalanced extension (UOT) to propose a criterion that can address all three issues. This proposal leverages a smooth formulation of DTW (Soft-DTW) that is shown to capture temporal shifts, and UOT to handle both variations in space and size. Our proposed loss can be used to define spatio-temporal barycenters as Fr{\textbackslash}'echet means. Using Fenchel duality, we show how these barycenters can be computed efficiently, in parallel, via a novel variant of entropy-regularized debiased UOT. Experiments on handwritten letters and brain imaging data confirm our theoretical findings and illustrate the effectiveness of the proposed loss for spatio-temporal data.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
	month = apr,
	year = {2022},
	note = {arXiv:2203.05813 [cs, stat]},
	keywords = {/unread, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/P3ZEJ7IA/Janati et al. - 2022 - Averaging Spatio-temporal Signals using Optimal Transport and Soft Alignments.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/4PJXMZIE/2203.html:text/html},
}

@article{janati_spatio-temporal_nodate,
	title = {Spatio-{Temporal} {Alignments}: {Optimal} transport through space and time},
	abstract = {Comparing data deﬁned over space and time is notoriously hard. It involves quantifying both spatial and temporal variability while taking into account the chronological structure of the data. Dynamic Time Warping (DTW) computes a minimal cost alignment between time series that preserves the chronological order but is inherently blind to spatiotemporal shifts. In this paper, we propose Spatio-Temporal Alignments (STA), a new diﬀerentiable formulation of DTW that captures spatial and temporal variability. Spatial diﬀerences between time samples are captured using regularized Optimal transport. While temporal alignment cost exploits a smooth variant of DTW called soft-DTW. We show how smoothing DTW leads to alignment costs that increase quadratically with time shifts. The costs are expressed using an unbalanced Wasserstein distance to cope with observations that are not probabilities. Experiments on handwritten letters and brain imaging data conﬁrm our theoretical ﬁndings and illustrate the eﬀectiveness of STA as a dissimilarity for spatio-temporal data.},
	language = {en},
	author = {Janati, Hicham and Cuturi, Marco and Gramfort, Alexandre},
	keywords = {/unread},
	file = {Janati et al. - Spatio-Temporal Alignments Optimal transport through space and time.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/MWJ3Q7Z3/Janati et al. - Spatio-Temporal Alignments Optimal transport through space and time.pdf:application/pdf},
}

@article{maretic_got_nodate,
	title = {{GOT}: {An} {Optimal} {Transport} framework for {Graph} comparison},
	abstract = {We present a novel framework based on optimal transport for the challenging problem of comparing graphs. Speciﬁcally, we exploit the probabilistic distribution of smooth graph signals deﬁned with respect to the graph topology. This allows us to derive an explicit expression of the Wasserstein distance between graph signal distributions in terms of the graph Laplacian matrices. This leads to a structurally meaningful measure for comparing graphs, which is able to take into account the global structure of graphs, while most other measures merely observe local changes independently. Our measure is then used for formulating a new graph alignment problem, whose objective is to estimate the permutation that minimizes the distance between two graphs. We further propose an efﬁcient stochastic algorithm based on Bayesian exploration to accommodate for the nonconvexity of the graph alignment problem. We ﬁnally demonstrate the performance of our novel framework on different tasks like graph alignment, graph classiﬁcation and graph signal prediction, and we show that our method leads to signiﬁcant improvement with respect to the state-of-art algorithms.},
	language = {en},
	author = {Maretic, Hermina Petric and Gheche, Mireille EL},
	keywords = {/unread},
	file = {Maretic and Gheche - GOT An Optimal Transport framework for Graph comparison.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/VEG36VWX/Maretic and Gheche - GOT An Optimal Transport framework for Graph comparison.pdf:application/pdf},
}

@inproceedings{zerveas_transformer-based_2021,
	address = {Virtual Event Singapore},
	title = {A {Transformer}-based {Framework} for {Multivariate} {Time} {Series} {Representation} {Learning}},
	isbn = {978-1-4503-8332-5},
	url = {https://dl.acm.org/doi/10.1145/3447548.3467401},
	doi = {10.1145/3447548.3467401},
	language = {en},
	urldate = {2024-02-02},
	booktitle = {Proceedings of the 27th {ACM} {SIGKDD} {Conference} on {Knowledge} {Discovery} \& {Data} {Mining}},
	publisher = {ACM},
	author = {Zerveas, George and Jayaraman, Srideepika and Patel, Dhaval and Bhamidipaty, Anuradha and Eickhoff, Carsten},
	month = aug,
	year = {2021},
	keywords = {/unread},
	pages = {2114--2124},
	file = {Submitted Version:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/DIKH3JTY/Zerveas et al. - 2021 - A Transformer-based Framework for Multivariate Time Series Representation Learning.pdf:application/pdf},
}

@article{lei_similarity_2019,
	title = {Similarity {Preserving} {Representation} {Learning} for {Time} {Series} {Clustering}},
	url = {http://arxiv.org/abs/1702.03584},
	abstract = {A considerable amount of clustering algorithms take instance-feature matrices as their inputs. As such, they cannot directly analyze time series data due to its temporal nature, usually unequal lengths, and complex properties. This is a great pity since many of these algorithms are effective, robust, efficient, and easy to use. In this paper, we bridge this gap by proposing an efficient representation learning framework that is able to convert a set of time series with various lengths to an instance-feature matrix. In particular, we guarantee that the pairwise similarities between time series are well preserved after the transformation, thus the learned feature representation is particularly suitable for the time series clustering task. Given a set of \$n\$ time series, we first construct an \$n{\textbackslash}times n\$ partially-observed similarity matrix by randomly sampling \${\textbackslash}mathcal\{O\}(n {\textbackslash}log n)\$ pairs of time series and computing their pairwise similarities. We then propose an efficient algorithm that solves a non-convex and NP-hard problem to learn new features based on the partially-observed similarity matrix. By conducting extensive empirical studies, we show that the proposed framework is more effective, efficient, and flexible, compared to other state-of-the-art time series clustering methods.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Lei, Qi and Yi, Jinfeng and Vaculin, Roman and Wu, Lingfei and Dhillon, Inderjit S.},
	month = jun,
	year = {2019},
	note = {arXiv:1702.03584 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/HPKPDSJZ/Lei et al. - 2019 - Similarity Preserving Representation Learning for Time Series Clustering.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/APC7MDKT/1702.html:text/html},
}

@article{li_learning_2021,
	title = {Learning {Disentangled} {Representations} for {Time} {Series}},
	url = {http://arxiv.org/abs/2105.08179},
	abstract = {Time-series representation learning is a fundamental task for time-series analysis. While significant progress has been made to achieve accurate representations for downstream applications, the learned representations often lack interpretability and do not expose semantic meanings. Different from previous efforts on the entangled feature space, we aim to extract the semantic-rich temporal correlations in the latent interpretable factorized representation of the data. Motivated by the success of disentangled representation learning in computer vision, we study the possibility of learning semantic-rich time-series representations, which remains unexplored due to three main challenges: 1) sequential data structure introduces complex temporal correlations and makes the latent representations hard to interpret, 2) sequential models suffer from KL vanishing problem, and 3) interpretable semantic concepts for time-series often rely on multiple factors instead of individuals. To bridge the gap, we propose Disentangle Time Series (DTS), a novel disentanglement enhancement framework for sequential data. Specifically, to generate hierarchical semantic concepts as the interpretable and disentangled representation of time-series, DTS introduces multi-level disentanglement strategies by covering both individual latent factors and group semantic segments. We further theoretically show how to alleviate the KL vanishing problem: DTS introduces a mutual information maximization term, while preserving a heavier penalty on the total correlation and the dimension-wise KL to keep the disentanglement property. Experimental results on various real-world benchmark datasets demonstrate that the representations learned by DTS achieve superior performance in downstream applications, with high interpretability of semantic concepts.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Li, Yuening and Chen, Zhengzhang and Zha, Daochen and Du, Mengnan and Zhang, Denghui and Chen, Haifeng and Hu, Xia},
	month = may,
	year = {2021},
	note = {arXiv:2105.08179 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/ZEHXP7RK/Li et al. - 2021 - Learning Disentangled Representations for Time Series.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/NBMJ5HTY/2105.html:text/html},
}

@article{lee_toward_2022,
	title = {Toward {Interpretable} {Semantic} {Textual} {Similarity} via {Optimal} {Transport}-based {Contrastive} {Sentence} {Learning}},
	url = {http://arxiv.org/abs/2202.13196},
	abstract = {Recently, finetuning a pretrained language model to capture the similarity between sentence embeddings has shown the state-of-the-art performance on the semantic textual similarity (STS) task. However, the absence of an interpretation method for the sentence similarity makes it difficult to explain the model output. In this work, we explicitly describe the sentence distance as the weighted sum of contextualized token distances on the basis of a transportation problem, and then present the optimal transport-based distance measure, named RCMD; it identifies and leverages semantically-aligned token pairs. In the end, we propose CLRCMD, a contrastive learning framework that optimizes RCMD of sentence pairs, which enhances the quality of sentence similarity and their interpretation. Extensive experiments demonstrate that our learning framework outperforms other baselines on both STS and interpretable-STS benchmarks, indicating that it computes effective sentence similarity and also provides interpretation consistent with human judgement. The code and checkpoint are publicly available at https://github.com/sh0416/clrcmd.},
	urldate = {2024-02-02},
	publisher = {arXiv},
	author = {Lee, Seonghyeon and Lee, Dongha and Jang, Seongbo and Yu, Hwanjo},
	month = apr,
	year = {2022},
	note = {arXiv:2202.13196 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence},
	annote = {Comment: ACL 2022 main + camera-ready version},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/EMZ4CADR/Lee et al. - 2022 - Toward Interpretable Semantic Textual Similarity via Optimal Transport-based Contrastive Sentence Le.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/XCPT5G7V/2202.html:text/html},
}

@article{he_webvoyager_2024,
	title = {{WebVoyager}: {Building} an {End}-to-{End} {Web} {Agent} with {Large} {Multimodal} {Models}},
	shorttitle = {{WebVoyager}},
	url = {http://arxiv.org/abs/2401.13919},
	abstract = {The advancement of large language models (LLMs) leads to a new era marked by the development of autonomous applications in the real world, which drives innovation in the creation of advanced web-based agents. Existing web agents typically only handle one input modality and are evaluated only in simplified web simulators or static web snapshots, greatly limiting their applicability in real-world scenarios. To bridge this gap, we introduce WebVoyager, an innovative Large Multimodal Model (LMM) powered web agent that can complete user instructions end-to-end by interacting with real-world websites. Moreover, we propose a new evaluation protocol for web agents to address the challenges of automatic evaluation of open-ended web agent tasks, leveraging the robust multimodal comprehension capabilities of GPT-4V. We create a new benchmark by gathering real-world tasks from 15 widely used websites to evaluate our agents. We show that WebVoyager achieves a 55.7\% task success rate, significantly surpassing the performance of both GPT-4 (All Tools) and the WebVoyager (text-only) setups, underscoring the exceptional capability of WebVoyager in practical applications. We found that our proposed automatic evaluation achieves 85.3\% agreement with human judgment, paving the way for further development of web agents in a real-world setting.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {He, Hongliang and Yao, Wenlin and Ma, Kaixin and Yu, Wenhao and Dai, Yong and Zhang, Hongming and Lan, Zhenzhong and Yu, Dong},
	month = jan,
	year = {2024},
	note = {arXiv:2401.13919 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/65KVXG7D/He et al. - 2024 - WebVoyager Building an End-to-End Web Agent with .pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/9YVXNTYL/2401.html:text/html},
}

@article{ji_ai_2024,
	title = {{AI} {Alignment}: {A} {Comprehensive} {Survey}},
	shorttitle = {{AI} {Alignment}},
	url = {http://arxiv.org/abs/2310.19852},
	abstract = {AI alignment aims to make AI systems behave in line with human intentions and values. As AI systems grow more capable, so do risks from misalignment. To provide a comprehensive and up-to-date overview of the alignment field, in this survey, we delve into the core concepts, methodology, and practice of alignment. First, we identify four principles as the key objectives of AI alignment: Robustness, Interpretability, Controllability, and Ethicality (RICE). Guided by these four principles, we outline the landscape of current alignment research and decompose them into two key components: forward alignment and backward alignment. The former aims to make AI systems aligned via alignment training, while the latter aims to gain evidence about the systems' alignment and govern them appropriately to avoid exacerbating misalignment risks. On forward alignment, we discuss techniques for learning from feedback and learning under distribution shift. On backward alignment, we discuss assurance techniques and governance practices. We also release and continually update the website (www.alignmentsurvey.com) which features tutorials, collections of papers, blog posts, and other resources.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Ji, Jiaming and Qiu, Tianyi and Chen, Boyuan and Zhang, Borong and Lou, Hantao and Wang, Kaile and Duan, Yawen and He, Zhonghao and Zhou, Jiayi and Zhang, Zhaowei and Zeng, Fanzhi and Ng, Kwan Yee and Dai, Juntao and Pan, Xuehai and O'Gara, Aidan and Lei, Yingshan and Xu, Hua and Tse, Brian and Fu, Jie and McAleer, Stephen and Yang, Yaodong and Wang, Yizhou and Zhu, Song-Chun and Guo, Yike and Gao, Wen},
	month = jan,
	year = {2024},
	note = {arXiv:2310.19852 [cs]},
	keywords = {Computer Science - Artificial Intelligence},
	annote = {Comment: Continually updated. 56 pages (excluding bibliography), 882 references. Abstract on arXiv webpage is abridged},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/PKMTDRBC/Ji et al. - 2024 - AI Alignment A Comprehensive Survey.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/QHJKLN6T/2310.html:text/html},
}

@article{noauthor_addon_nodate,
	title = {Addon {Item}},
}

@article{yue_ts2vec_2022,
	title = {{TS2Vec}: {Towards} {Universal} {Representation} of {Time} {Series}},
	volume = {36},
	issn = {2374-3468, 2159-5399},
	shorttitle = {{TS2Vec}},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/20881},
	doi = {10.1609/aaai.v36i8.20881},
	abstract = {This paper presents TS2Vec, a universal framework for learning representations of time series in an arbitrary semantic level. Unlike existing methods, TS2Vec performs contrastive learning in a hierarchical way over augmented context views, which enables a robust contextual representation for each timestamp. Furthermore, to obtain the representation of an arbitrary sub-sequence in the time series, we can apply a simple aggregation over the representations of corresponding timestamps. We conduct extensive experiments on time series classiﬁcation tasks to evaluate the quality of time series representations. As a result, TS2Vec achieves signiﬁcant improvement over existing SOTAs of unsupervised time series representation on 125 UCR datasets and 29 UEA datasets. The learned timestamp-level representations also achieve superior results in time series forecasting and anomaly detection tasks. A linear regression trained on top of the learned representations outperforms previous SOTAs of time series forecasting. Furthermore, we present a simple way to apply the learned representations for unsupervised anomaly detection, which establishes SOTA results in the literature. The source code is publicly available at https://github.com/yuezhihan/ts2vec.},
	language = {en},
	number = {8},
	urldate = {2024-02-06},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Yue, Zhihan and Wang, Yujing and Duan, Juanyong and Yang, Tianmeng and Huang, Congrui and Tong, Yunhai and Xu, Bixiong},
	month = jun,
	year = {2022},
	keywords = {/unread},
	pages = {8980--8987},
	file = {Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time Series.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-terencelau/Research/ZOTERO-DATABASE/storage/BDQYSH23/Yue et al. - 2022 - TS2Vec Towards Universal Representation of Time Series.pdf:application/pdf},
}


@inproceedings{li_clip-event_2022,
	address = {New Orleans, LA, USA},
	title = {{CLIP}-{Event}: {Connecting} {Text} and {Images} with {Event} {Structures}},
	isbn = {978-1-66546-946-3},
	shorttitle = {{CLIP}-{Event}},
	url = {https://ieeexplore.ieee.org/document/9879585/},
	doi = {10.1109/CVPR52688.2022.01593},
	abstract = {Vision-language (V+L) pretraining models have achieved great success in supporting multimedia applications by understanding the alignments between images and text. While existing vision-language pretraining models primarily focus on understanding objects in images or entities in text, they often ignore the alignment at the level of events and their argument structures. In this work, we propose a contrastive learning framework to enforce vision-language pretraining models to comprehend events and associated argument (participant) roles. To achieve this, we take advantage of text information extraction technologies to obtain event structural knowledge, and utilize multiple prompt functions to contrast difficult negative descriptions by manipulating event structures. We also design an event graph alignment loss based on optimal transport to capture event argument structures. In addition, we collect a large event-rich dataset (106,875 images) for pretraining, which provides a more challenging image retrieval benchmark to assess the understanding of complicated lengthy sentences1. Experiments show that our zero-shot CLIP-Event outperforms the state-of-the-art supervised model in argument extraction on Multimedia Event Extraction, achieving more than 5\% absolute F-score gain in event extraction, as well as significant improvements on a variety of downstream tasks under zero-shot settings.},
	language = {en},
	urldate = {2023-12-14},
	booktitle = {2022 {IEEE}/{CVF} {Conference} on {Computer} {Vision} and {Pattern} {Recognition} ({CVPR})},
	publisher = {IEEE},
	author = {Li, Manling and Xu, Ruochen and Wang, Shuohang and Zhou, Luowei and Lin, Xudong and Zhu, Chenguang and Zeng, Michael and Ji, Heng and Chang, Shih-Fu},
	month = jun,
	year = {2022},
	keywords = {/unread, CLIP-model, dataset},
	pages = {16399--16408},
	file = {Li et al. - 2022 - CLIP-Event Connecting Text and Images with Event .pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/ET39IGG6/Li et al. - 2022 - CLIP-Event Connecting Text and Images with Event .pdf:application/pdf},
}

@article{moghimifar_few-shot_2023,
	title = {Few-shot {Domain}-{Adaptive} {Visually}-fused {Event} {Detection} from {Text}},
	url = {http://arxiv.org/abs/2305.03517},
	abstract = {Incorporating auxiliary modalities such as images into event detection models has attracted increasing interest over the last few years. The complexity of natural language in describing situations has motivated researchers to leverage the related visual context to improve event detection performance. However, current approaches in this area suffer from data scarcity, where a large amount of labelled text-image pairs are required for model training. Furthermore, limited access to the visual context at inference time negatively impacts the performance of such models, which makes them practically ineffective in real-world scenarios. In this paper, we present a novel domain-adaptive visually-fused event detection approach that can be trained on a few labelled image-text paired data points. Specifically, we introduce a visual imaginator method that synthesises images from text in the absence of visual context. Moreover, the imaginator can be customised to a specific domain. In doing so, our model can leverage the capabilities of pre-trained vision-language models and can be trained in a few-shot setting. This also allows for effective inference where only single-modality data (i.e. text) is available. The experimental evaluation on the benchmark M2E2 dataset shows that our model outperforms existing state-of-the-art models, by up to 11 points.},
	language = {en},
	urldate = {2023-12-17},
	publisher = {arXiv},
	author = {Moghimifar, Farhad and Shiri, Fatemeh and Nguyen, Van and Haffari, Reza and Li, Yuan-Fang},
	month = jun,
	year = {2023},
	note = {arXiv:2305.03517 [cs]},
	keywords = {/unread, Computer Science - Artificial Intelligence, Computer Science - Computation and Language},
	file = {Moghimifar et al. - 2023 - Few-shot Domain-Adaptive Visually-fused Event Dete.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/AL7HAQCF/Moghimifar et al. - 2023 - Few-shot Domain-Adaptive Visually-fused Event Dete.pdf:application/pdf},
}

@article{du_training_2023,
	title = {Training {Multimedia} {Event} {Extraction} {With} {Generated} {Images} and {Captions}},
	url = {http://arxiv.org/abs/2306.08966},
	abstract = {Contemporary news reporting increasingly features multimedia content, motivating research on multimedia event extraction. However, the task lacks annotated multimodal training data and artificially generated training data suffer from distribution shift from real-world data. In this paper, we propose Cross-modality Augmented Multimedia Event Learning (CAMEL), which successfully utilizes artificially generated multimodal training data and achieves state-of-the-art performance. We start with two labeled unimodal datasets in text and image respectively, and generate the missing modality using off-the-shelf image generators like Stable Diffusion [45] and image captioners like BLIP [24]. After that, we train the network on the resultant multimodal datasets. In order to learn robust features that are effective across domains, we devise an iterative and gradual training strategy. Substantial experiments show that CAMEL surpasses state-of-the-art (SOTA) baselines on the M2E2 benchmark. On multimedia events in particular, we outperform the prior SOTA by 4.2\% F1 on event mention identification and by 9.8\% F1 on argument identification, which indicates that CAMEL learns synergistic representations from the two modalities. Our work demonstrates a recipe to unleash the power of synthetic training data in structured prediction.},
	language = {en},
	urldate = {2024-01-30},
	publisher = {arXiv},
	author = {Du, Zilin and Li, Yunxin and Guo, Xu and Sun, Yidan and Li, Boyang},
	month = aug,
	year = {2023},
	note = {arXiv:2306.08966 [cs]},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Multimedia},
	file = {Du et al. - 2023 - Training Multimedia Event Extraction With Generate.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/I7W8VYHP/Du et al. - 2023 - Training Multimedia Event Extraction With Generate.pdf:application/pdf},
}

@inproceedings{wang_maven-ere_2022,
	address = {Abu Dhabi, United Arab Emirates},
	title = {{MAVEN}-{ERE}: {A} {Unified} {Large}-scale {Dataset} for {Event} {Coreference}, {Temporal}, {Causal}, and {Subevent} {Relation} {Extraction}},
	shorttitle = {{MAVEN}-{ERE}},
	url = {https://aclanthology.org/2022.emnlp-main.60},
	doi = {10.18653/v1/2022.emnlp-main.60},
	abstract = {The diverse relationships among real-world events, including coreference, temporal, causal, and subevent relations, are fundamental to understanding natural languages. However, two drawbacks of existing datasets limit event relation extraction (ERE) tasks: (1) Small scale. Due to the annotation complexity, the data scale of existing datasets is limited, which cannot well train and evaluate data-hungry models. (2) Absence of unified annotation. Different types of event relations naturally interact with each other, but existing datasets only cover limited relation types at once, which prevents models from taking full advantage of relation interactions. To address these issues, we construct a unified large-scale human-annotated ERE dataset MAVEN-ERE with improved annotation schemes. It contains 103, 193 event coreference chains, 1, 216, 217 temporal relations, 57, 992 causal relations, and 15, 841 subevent relations, which is larger than existing datasets of all the ERE tasks by at least an order of magnitude. Experiments show that ERE on MAVEN-ERE is quite challenging, and considering relation interactions with joint learning can improve performances. The dataset and source codes can be obtained from https://github.com/THU-KEG/MAVEN-ERE.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2022 {Conference} on {Empirical} {Methods} in {Natural} {Language} {Processing}},
	publisher = {Association for Computational Linguistics},
	author = {Wang, Xiaozhi and Chen, Yulin and Ding, Ning and Peng, Hao and Wang, Zimu and Lin, Yankai and Han, Xu and Hou, Lei and Li, Juanzi and Liu, Zhiyuan and Li, Peng and Zhou, Jie},
	year = {2022},
	keywords = {/unread},
	pages = {926--941},
	file = {Wang et al. - 2022 - MAVEN-ERE A Unified Large-scale Dataset for Event.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/QBPZNHC5/Wang et al. - 2022 - MAVEN-ERE A Unified Large-scale Dataset for Event.pdf:application/pdf},
}

@article{huang_multilingual_2022,
	title = {Multilingual {Generative} {Language} {Models} for {Zero}-{Shot} {Cross}-{Lingual} {Event} {Argument} {Extraction}},
	url = {http://arxiv.org/abs/2203.08308},
	abstract = {We present a study on leveraging multilingual pre-trained generative language models for zero-shot cross-lingual event argument extraction (EAE). By formulating EAE as a language generation task, our method effectively encodes event structures and captures the dependencies between arguments. We design language-agnostic templates to represent the event argument structures, which are compatible with any language, hence facilitating the cross-lingual transfer. Our proposed model ﬁnetunes multilingual pre-trained generative language models to generate sentences that ﬁll in the language-agnostic template with arguments extracted from the input passage. The model is trained on source languages and is then directly applied to target languages for event argument extraction. Experiments demonstrate that the proposed model outperforms the current state-of-the-art models on zero-shot cross-lingual EAE. Comprehensive studies and error analyses are presented to better understand the advantages and the current limitations of using generative language models for zero-shot cross-lingual transfer EAE.},
	language = {en},
	urldate = {2023-12-02},
	publisher = {arXiv},
	author = {Huang, Kuan-Hao and Hsu, I.-Hung and Natarajan, Premkumar and Chang, Kai-Wei and Peng, Nanyun},
	month = mar,
	year = {2022},
	note = {arXiv:2203.08308 [cs]},
	keywords = {/unread, Computer Science - Computation and Language},
	annote = {Comment: ACL 2022. Our code is available at https://github.com/PlusLabNLP/X-Gear},
	file = {Huang et al. - 2022 - Multilingual Generative Language Models for Zero-S.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/U7GEGZ7U/Huang et al. - 2022 - Multilingual Generative Language Models for Zero-S.pdf:application/pdf},
}

@inproceedings{hu_conflibert_2022,
	address = {Seattle, United States},
	title = {{ConfliBERT}: {A} {Pre}-trained {Language} {Model} for {Political} {Conflict} and {Violence}},
	shorttitle = {{ConfliBERT}},
	url = {https://aclanthology.org/2022.naacl-main.400},
	doi = {10.18653/v1/2022.naacl-main.400},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Hu, Yibo and Hosseini, MohammadSaleh and Skorupa Parolin, Erick and Osorio, Javier and Khan, Latifur and Brandt, Patrick and D’Orazio, Vito},
	year = {2022},
	keywords = {/unread},
	pages = {5469--5482},
	file = {Hu et al. - 2022 - ConfliBERT A Pre-trained Language Model for Polit.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/9CDQ74NJ/Hu et al. - 2022 - ConfliBERT A Pre-trained Language Model for Polit.pdf:application/pdf},
}

@inproceedings{tong_docee_2022,
	address = {Seattle, United States},
	title = {{DocEE}: {A} {Large}-{Scale} and {Fine}-grained {Benchmark} for {Document}-level {Event} {Extraction}},
	shorttitle = {{DocEE}},
	url = {https://aclanthology.org/2022.naacl-main.291},
	doi = {10.18653/v1/2022.naacl-main.291},
	abstract = {Event extraction aims to identify an event and then extract the arguments participating in the event. Despite the great success in sentencelevel event extraction, events are more naturally presented in the form of documents, with event arguments scattered in multiple sentences. However, a major barrier to promote documentlevel event extraction has been the lack of large-scale and practical training and evaluation datasets. In this paper, we present DocEE, a new document-level event extraction dataset including 27,000+ events, 180,000+ arguments. We highlight three features: largescale manual annotations, fine-grained argument types and application-oriented settings. Experiments show that there is still a big gap between state-of-the-art models and human beings (41\% Vs 85\% in F1 score), indicating that DocEE is an open issue. DocEE is now available at https://github.com/ tongmeihan1995/DocEE.git.},
	language = {en},
	urldate = {2023-12-02},
	booktitle = {Proceedings of the 2022 {Conference} of the {North} {American} {Chapter} of the {Association} for {Computational} {Linguistics}: {Human} {Language} {Technologies}},
	publisher = {Association for Computational Linguistics},
	author = {Tong, MeiHan and Xu, Bin and Wang, Shuai and Han, Meihuan and Cao, Yixin and Zhu, Jiangqi and Chen, Siyu and Hou, Lei and Li, Juanzi},
	year = {2022},
	keywords = {/unread, dataset},
	pages = {3970--3982},
	file = {Tong et al. - 2022 - DocEE A Large-Scale and Fine-grained Benchmark fo.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/SYLQMKUG/Tong et al. - 2022 - DocEE A Large-Scale and Fine-grained Benchmark fo.pdf:application/pdf},
}

@article{hamborg_giveme5w1h_2019,
	title = {{Giveme5W1H}: {A} {Universal} {System} for {Extracting} {Main} {Events} from {News} {Articles}},
	abstract = {Event extraction from news articles is a commonly required prerequisite for various tasks, such as article summarization, article clustering, and news aggregation. Due to the lack of universally applicable and publicly available methods tailored to news datasets, many researchers redundantly implement event extraction methods for their own projects. The journalistic 5W1H questions are capable of describing the main event of an article, i.e., by answering who did what, when, where, why, and how. We provide an in-depth description of an improved version of Giveme5W1H, a system that uses syntactic and domain-specific rules to automatically extract the relevant phrases from English news articles to provide answers to these 5W1H questions. Given the answers to these questions, the system determines an article’s main event. In an expert evaluation with three assessors and 120 articles, we determined an overall precision of p=0.73, and p=0.82 for answering the first four W questions, which alone can sufficiently summarize the main event reported on in a news article. We recently made our system publicly available, and it remains the only universal open-source 5W1H extractor capable of being applied to a wide range of use cases in news analysis.},
	language = {en},
	author = {Hamborg, Felix and Breitinger, Corinna and Gipp, Bela},
	year = {2019},
	keywords = {/unread, dataset},
	file = {Hamborg et al. - 2019 - Giveme5W1H A Universal System for Extracting Main.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/U3R6U3TV/Hamborg et al. - 2019 - Giveme5W1H A Universal System for Extracting Main.pdf:application/pdf},
}

@article{lu_text2event_2021,
	title = {{Text2Event}: {Controllable} {Sequence}-to-{Structure} {Generation} for {End}-to-end {Event} {Extraction}},
	shorttitle = {{Text2Event}},
	url = {http://arxiv.org/abs/2106.09232},
	abstract = {Event extraction is challenging due to the complex structure of event records and the semantic gap between text and event. Traditional methods usually extract event records by decomposing the complex structure prediction task into multiple subtasks. In this paper, we propose Text2Event, a sequence-to-structure generation paradigm that can directly extract events from the text in an end-to-end manner. Specifically, we design a sequence-to-structure network for unified event extraction, a constrained decoding algorithm for event knowledge injection during inference, and a curriculum learning algorithm for efficient model learning. Experimental results show that, by uniformly modeling all tasks in a single model and universally predicting different labels, our method can achieve competitive performance using only record-level annotations in both supervised learning and transfer learning settings.},
	language = {en},
	urldate = {2023-12-02},
	publisher = {arXiv},
	author = {Lu, Yaojie and Lin, Hongyu and Xu, Jin and Han, Xianpei and Tang, Jialong and Li, Annan and Sun, Le and Liao, Meng and Chen, Shaoyi},
	month = jun,
	year = {2021},
	note = {arXiv:2106.09232 [cs]},
	keywords = {/unread, Computer Science - Computation and Language},
	annote = {Comment: Accepted to ACL2021 (main conference)},
	file = {Lu et al. - 2021 - Text2Event Controllable Sequence-to-Structure Gen.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/XHC9YYGU/Lu et al. - 2021 - Text2Event Controllable Sequence-to-Structure Gen.pdf:application/pdf},
}

@article{zhou_survey_2020,
	title = {A survey on multi-modal social event detection},
	volume = {195},
	issn = {09507051},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0950705120301271},
	doi = {10.1016/j.knosys.2020.105695},
	abstract = {Due to the prevalence of social media sites, users are allowed to conveniently share their ideas and activities anytime and anywhere. Therefore, these sites hold substantial real-world event related data. Different from traditional social event detection methods which mainly focus on single-media, multimodal social event detection aims at discovering events in vast heterogeneous data such as texts, images and video clips. These data denote real-world events from multiple dimensions simultaneously so that they can provide comprehensive and complementary understanding of social event. In recent years, multi-modal social event detection has attracted intensive attentions. This paper concentrates on conducting a comprehensive survey of extant works. Two current attempts in this field are firstly reviewed: event feature learning and event inference. Particularly, event feature learning is a prerequisite because of its ability on translating social media data into computer-friendly numerical form. Event inference aims at deciding whether a sample belongs to a social event. Then, several public datasets in the community are introduced and the comparison results are also provided. At the end of this paper, a general discussion of the insights is delivered to promote the development of multi-modal social event detection.},
	language = {en},
	urldate = {2024-01-30},
	journal = {Knowledge-Based Systems},
	author = {Zhou, Han and Yin, Hongpeng and Zheng, Hengyi and Li, Yanxia},
	month = may,
	year = {2020},
	keywords = {/unread, SURVEY},
	pages = {105695},
	file = {Zhou et al. - 2020 - A survey on multi-modal social event detection.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/IVHGLMMA/Zhou et al. - 2020 - A survey on multi-modal social event detection.pdf:application/pdf},
}

@article{yang_survey_2022,
	title = {A survey on extraction of causal relations from natural language text},
	volume = {64},
	issn = {0219-1377, 0219-3116},
	url = {https://link.springer.com/10.1007/s10115-022-01665-w},
	doi = {10.1007/s10115-022-01665-w},
	abstract = {As an essential component of human cognition, cause–effect relations appear frequently in text, and curating cause–effect relations from text helps in building causal networks for predictive tasks. Existing causality extraction techniques include knowledge-based, statistical machine learning (ML)-based, and deep learning-based approaches. Each method has its advantages and weaknesses. For example, knowledge-based methods are understandable but require extensive manual domain knowledge and have poor cross-domain applicability. Statistical machine learning methods are more automated because of natural language processing (NLP) toolkits. However, feature engineering is labor-intensive, and toolkits may lead to error propagation. In the past few years, deep learning techniques attract substantial attention from NLP researchers because of its powerful representation learning ability and the rapid increase in computational resources. Their limitations include high computational costs and a lack of adequate annotated training data. In this paper, we conduct a comprehensive survey of causality extraction. We initially introduce primary forms existing in the causality extraction: explicit intra-sentential causality, implicit causality, and inter-sentential causality. Next, we list benchmark datasets and modeling assessment methods for causal relation extraction. Then, we present a structured overview of the three techniques with their representative systems. Lastly, we highlight existing open challenges with their potential directions.},
	language = {en},
	number = {5},
	urldate = {2024-01-30},
	journal = {Knowledge and Information Systems},
	author = {Yang, Jie and Han, Soyeon Caren and Poon, Josiah},
	month = may,
	year = {2022},
	keywords = {/unread, SURVEY},
	pages = {1161--1186},
	file = {Yang et al. - 2022 - A survey on extraction of causal relations from na.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/MG2DUJ8U/Yang et al. - 2022 - A survey on extraction of causal relations from na.pdf:application/pdf},
}

@article{zhao_event_2022,
	title = {Event {Prediction} in the {Big} {Data} {Era}: {A} {Systematic} {Survey}},
	volume = {54},
	issn = {0360-0300, 1557-7341},
	shorttitle = {Event {Prediction} in the {Big} {Data} {Era}},
	url = {https://dl.acm.org/doi/10.1145/3450287},
	doi = {10.1145/3450287},
	abstract = {Events are occurrences in specific locations, time, and semantics that nontrivially impact either our society or the nature, such as earthquakes, civil unrest, system failures, pandemics, and crimes. It is highly desirable to be able to anticipate the occurrence of such events in advance to reduce the potential social upheaval and damage caused. Event prediction, which has traditionally been prohibitively challenging, is now becoming a viable option in the big data era and is thus experiencing rapid growth, also thanks to advances in high performance computers and new Artificial Intelligence techniques. There is a large amount of existing work that focuses on addressing the challenges involved, including heterogeneous multi-faceted outputs, complex (e.g., spatial, temporal, and semantic) dependencies, and streaming data feeds. Due to the strong interdisciplinary nature of event prediction problems, most existing event prediction methods were initially designed to deal with specific application domains, though the techniques and evaluation procedures utilized are usually generalizable across different domains. However, it is imperative yet difficult to cross-reference the techniques across different domains, given the absence of a comprehensive literature survey for event prediction. This article aims to provide a systematic and comprehensive survey of the technologies, applications, and evaluations of event prediction in the big data era. First, systematic categorization and summary of existing techniques are presented, which facilitate domain experts’ searches for suitable techniques and help model developers consolidate their research at the frontiers. Then, comprehensive categorization and summary of major application domains are provided to introduce wider applications to model developers to help them expand the impacts of their research. Evaluation metrics and procedures are summarized and standardized to unify the understanding of model performance among stakeholders, model developers, and domain experts in various application domains. Finally, open problems and future directions are discussed. Additional resources related to event prediction are included in the paper website: http://cs.emory.edu/∼lzhao41/projects/event\_prediction\_site.html.},
	language = {en},
	number = {5},
	urldate = {2024-01-30},
	journal = {ACM Computing Surveys},
	author = {Zhao, Liang},
	month = jun,
	year = {2022},
	keywords = {/unread, SURVEY},
	pages = {1--37},
	file = {Zhao - 2022 - Event Prediction in the Big Data Era A Systematic.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/4IYBPNIR/Zhao - 2022 - Event Prediction in the Big Data Era A Systematic.pdf:application/pdf},
}

@article{tong_image_2020,
	title = {Image {Enhanced} {Event} {Detection} in {News} {Articles}},
	volume = {34},
	issn = {2374-3468, 2159-5399},
	url = {https://ojs.aaai.org/index.php/AAAI/article/view/6437},
	doi = {10.1609/aaai.v34i05.6437},
	abstract = {Event detection is a crucial and challenging sub-task of event extraction, which suffers from a severe ambiguity issue of trigger words. Existing works mainly focus on using textual context information, while there naturally exist many images accompanied by news articles that are yet to be explored. We believe that images not only reﬂect the core events of the text, but are also helpful for the disambiguation of trigger words. In this paper, we ﬁrst contribute an image dataset supplement to ED benchmarks (i.e., ACE2005) for training and evaluation. We then propose a novel Dual Recurrent Multimodal Model, DRMM, to conduct deep interactions between images and sentences for modality features aggregation. DRMM utilizes pre-trained BERT and ResNet to encode sentences and images, and employs an alternating dual attention to select informative features for mutual enhancements. Our superior performance compared to six state-of-art baselines as well as further ablation studies demonstrate the signiﬁcance of image modality and effectiveness of the proposed architecture. The code and image dataset are avaliable at https://github.com/ shuaiwa16/image-enhanced-event-extraction.},
	language = {en},
	number = {05},
	urldate = {2024-01-30},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tong, Meihan and Wang, Shuai and Cao, Yixin and Xu, Bin and Li, Juanzi and Hou, Lei and Chua, Tat-Seng},
	month = apr,
	year = {2020},
	keywords = {/unread},
	pages = {9040--9047},
	file = {Tong et al. - 2020 - Image Enhanced Event Detection in News Articles.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/LNYBBIHZ/Tong et al. - 2020 - Image Enhanced Event Detection in News Articles.pdf:application/pdf},
}

@article{martinez-rodriguez_information_2020,
	title = {Information extraction meets the {Semantic} {Web}: {A} survey},
	volume = {11},
	issn = {22104968, 15700844},
	shorttitle = {Information extraction meets the {Semantic} {Web}},
	url = {https://www.medra.org/servlet/aliasResolver?alias=iospress&doi=10.3233/SW-180333},
	doi = {10.3233/SW-180333},
	abstract = {We provide a comprehensive survey of the research literature that applies Information Extraction techniques in a Semantic Web setting. Works in the intersection of these two areas can be seen from two overlapping perspectives: using Semantic Web resources (languages/ontologies/knowledge-bases/tools) to improve Information Extraction, and/or using Information Extraction to populate the Semantic Web. In more detail, we focus on the extraction and linking of three elements: entities, concepts and relations. Extraction involves identifying (textual) mentions referring to such elements in a given unstructured or semi-structured input source. Linking involves associating each such mention with an appropriate disambiguated identiﬁer referring to the same element in a Semantic Web knowledge-base (or ontology), in some cases creating a new identiﬁer where necessary. With respect to entities, works involving (Named) Entity Recognition, Entity Disambiguation, Entity Linking, etc. in the context of the Semantic Web are considered. With respect to concepts, works involving Terminology Extraction, Keyword Extraction, Topic Modeling, Topic Labeling, etc., in the context of the Semantic Web are considered. Finally, with respect to relations, works involving Relation Extraction in the context of the Semantic Web are considered. The focus of the majority of the survey is on works applied to unstructured sources (text in natural language); however, we also provide an overview of works that develop custom techniques adapted for semi-structured inputs, namely markup documents and web tables.},
	language = {en},
	number = {2},
	urldate = {2024-01-30},
	journal = {Semantic Web},
	author = {Martinez-Rodriguez, Jose L. and Hogan, Aidan and Lopez-Arevalo, Ivan},
	editor = {Hotho, Andreas},
	month = feb,
	year = {2020},
	keywords = {/unread},
	pages = {255--335},
	file = {Martinez-Rodriguez et al. - 2020 - Information extraction meets the Semantic Web A s.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/7CBEZ5SU/Martinez-Rodriguez et al. - 2020 - Information extraction meets the Semantic Web A s.pdf:application/pdf},
}

@article{li_blip_nodate,
	title = {{BLIP}: {Bootstrapping} {Language}-{Image} {Pre}-training for  {Unified} {Vision}-{Language} {Understanding} and {Generation}},
	abstract = {Vision-Language Pre-training (VLP) has advanced the performance for many vision-language tasks. However, most existing pre-trained models only excel in either understanding-based tasks or generation-based tasks. Furthermore, performance improvement has been largely achieved by scaling up the dataset with noisy image-text pairs collected from the web, which is a suboptimal source of supervision. In this paper, we propose BLIP, a new VLP framework which transfers flexibly to both vision-language understanding and generation tasks. BLIP effectively utilizes the noisy web data by bootstrapping the captions, where a captioner generates synthetic captions and a filter removes the noisy ones. We achieve state-of-the-art results on a wide range of vision-language tasks, such as image-text retrieval (+2.7\% in average recall@1), image captioning (+2.8\% in CIDEr), and VQA (+1.6\% in VQA score). BLIP also demonstrates strong generalization ability when directly transferred to video-language tasks in a zero-shot manner. Code and models are available at https://github. com/salesforce/BLIP.},
	language = {en},
	author = {Li, Junnan and Li, Dongxu and Xiong, Caiming and Hoi, Steven},
	keywords = {/unread},
	file = {Li et al. - BLIP Bootstrapping Language-Image Pre-training fo.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/H479RWE4/Li et al. - BLIP Bootstrapping Language-Image Pre-training fo.pdf:application/pdf},
}

@article{radford_learning_nodate,
	title = {Learning {Transferable} {Visual} {Models} {From} {Natural} {Language} {Supervision}},
	abstract = {SOTA computer vision systems are trained to predict a ﬁxed set of predetermined object categories. This restricted form of supervision limits their generality and usability since additional labeled data is needed to specify any other visual concept. Learning directly from raw text about images is a promising alternative which leverages a much broader source of supervision. We demonstrate that the simple pre-training task of predicting which caption goes with which image is an efﬁcient and scalable way to learn SOTA image representations from scratch on a dataset of 400 million (image, text) pairs collected from the internet. After pre-training, natural language is used to reference learned visual concepts (or describe new ones) enabling zero-shot transfer of the model to downstream tasks. We study performance on over 30 different computer vision datasets, spanning tasks such as OCR, action recognition in videos, geo-localization, and many types of ﬁne-grained object classiﬁcation. The model transfers nontrivially to most tasks and is often competitive with a fully supervised baseline without the need for any dataset speciﬁc training. For instance, we match the accuracy of the original ResNet50 on ImageNet zero-shot without needing to use any of the 1.28 million training examples it was trained on. We release our code and pre-trained model weights at https://github.com/OpenAI/CLIP.},
	language = {en},
	author = {Radford, Alec and Kim, Jong Wook and Hallacy, Chris and Ramesh, Aditya and Goh, Gabriel and Agarwal, Sandhini and Sastry, Girish and Askell, Amanda and Mishkin, Pamela and Clark, Jack and Krueger, Gretchen and Sutskever, Ilya},
	file = {Radford et al. - Learning Transferable Visual Models From Natural L.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/QZJJSWF2/Radford et al. - Learning Transferable Visual Models From Natural L.pdf:application/pdf},
}

@article{li_blip-2_2023,
	title = {{BLIP}-2: {Bootstrapping} {Language}-{Image} {Pre}-training with {Frozen} {Image} {Encoders} and {Large} {Language} {Models}},
	shorttitle = {{BLIP}-2},
	url = {http://arxiv.org/abs/2301.12597},
	abstract = {The cost of vision-and-language pre-training has become increasingly prohibitive due to end-to-end training of large-scale models. This paper proposes BLIP-2, a generic and efficient pre-training strategy that bootstraps vision-language pre-training from off-the-shelf frozen pre-trained image encoders and frozen large language models. BLIP-2 bridges the modality gap with a lightweight Querying Transformer, which is pre-trained in two stages. The first stage bootstraps vision-language representation learning from a frozen image encoder. The second stage bootstraps vision-to-language generative learning from a frozen language model. BLIP-2 achieves state-of-the-art performance on various vision-language tasks, despite having significantly fewer trainable parameters than existing methods. For example, our model outperforms Flamingo80B by 8.7\% on zero-shot VQAv2 with 54x fewer trainable parameters. We also demonstrate the model's emerging capabilities of zero-shot image-to-text generation that can follow natural language instructions.},
	urldate = {2024-02-03},
	publisher = {arXiv},
	author = {Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
	month = jun,
	year = {2023},
	note = {arXiv:2301.12597 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/CABNVY64/Li et al. - 2023 - BLIP-2 Bootstrapping Language-Image Pre-training .pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/R7G8N2FB/2301.html:text/html},
}

@inproceedings{li_gaia_2020,
	address = {Online},
	title = {{GAIA}: {A} {Fine}-grained {Multimedia} {Knowledge} {Extraction} {System}},
	shorttitle = {{GAIA}},
	url = {https://www.aclweb.org/anthology/2020.acl-demos.11},
	doi = {10.18653/v1/2020.acl-demos.11},
	abstract = {We present the ﬁrst comprehensive, open source multimedia knowledge extraction system that takes a massive stream of unstructured, heterogeneous multimedia data from various sources and languages as input, and creates a coherent, structured knowledge base, indexing entities, relations, and events, following a rich, ﬁne-grained ontology. Our system, GAIA 1, enables seamless search of complex graph queries, and retrieves multimedia evidence including text, images and videos. GAIA achieves top performance at the recent NIST TAC SM-KBP2019 evaluation2. The system is publicly available at GitHub3 and DockerHub4, with complete documentation5.},
	language = {en},
	urldate = {2024-02-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}: {System} {Demonstrations}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Manling and Zareian, Alireza and Lin, Ying and Pan, Xiaoman and Whitehead, Spencer and Chen, Brian and Wu, Bo and Ji, Heng and Chang, Shih-Fu and Voss, Clare and Napierski, Daniel and Freedman, Marjorie},
	year = {2020},
	keywords = {/unread},
	pages = {77--86},
	file = {Li et al. - 2020 - GAIA A Fine-grained Multimedia Knowledge Extraction System.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/GJTY2F88/Li et al. - 2020 - GAIA A Fine-grained Multimedia Knowledge Extraction System.pdf:application/pdf},
}

@inproceedings{lin_joint_2020,
	address = {Online},
	title = {A {Joint} {Neural} {Model} for {Information} {Extraction} with {Global} {Features}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.713},
	doi = {10.18653/v1/2020.acl-main.713},
	abstract = {Most existing joint neural models for Information Extraction (IE) use local task-speciﬁc classiﬁers to predict labels for individual instances (e.g., trigger, relation) regardless of their interactions. For example, a VICTIM of a DIE event is likely to be a VICTIM of an ATTACK event in the same sentence. In order to capture such cross-subtask and cross-instance inter-dependencies, we propose a joint neural framework, ONEIE, that aims to extract the globally optimal IE result as a graph from an input sentence. ONEIE performs end-to-end IE in four stages: (1) Encoding a given sentence as contextualized word representations; (2) Identifying entity mentions and event triggers as nodes; (3) Computing label scores for all nodes and their pairwise links using local classiﬁers; (4) Searching for the globally optimal graph with a beam decoder. At the decoding stage, we incorporate global features to capture the cross-subtask and cross-instance interactions. Experiments show that adding global features improves the performance of our model and achieves new state-of-the-art on all subtasks. As ONEIE does not use any language-speciﬁc feature, we prove it can be easily applied to new languages or trained in a multilingual manner. Our code and models for English, Spanish and Chinese are publicly available for research purpose 1.},
	language = {en},
	urldate = {2024-02-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Lin, Ying and Ji, Heng and Huang, Fei and Wu, Lingfei},
	year = {2020},
	keywords = {/unread},
	pages = {7999--8009},
	file = {Lin et al. - 2020 - A Joint Neural Model for Information Extraction with Global Features.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/VWRY7V2S/Lin et al. - 2020 - A Joint Neural Model for Information Extraction with Global Features.pdf:application/pdf},
}

@inproceedings{li_cross-media_2020,
	address = {Online},
	title = {Cross-media {Structured} {Common} {Space} for {Multimedia} {Event} {Extraction}},
	url = {https://www.aclweb.org/anthology/2020.acl-main.230},
	doi = {10.18653/v1/2020.acl-main.230},
	abstract = {We introduce a new task, MultiMedia Event Extraction (M2E2), which aims to extract events and their arguments from multimedia documents. We develop the ﬁrst benchmark and collect a dataset of 245 multimedia news articles with extensively annotated events and arguments.1 We propose a novel method, Weakly Aligned Structured Embedding (WASE), that encodes structured representations of semantic information from textual and visual data into a common embedding space. The structures are aligned across modalities by employing a weakly supervised training strategy, which enables exploiting available resources without explicit cross-media annotation. Compared to unimodal state-of-the-art methods, our approach achieves 4.0\% and 9.8\% absolute F-score gains on text event argument role labeling and visual event extraction. Compared to stateof-the-art multimedia unstructured representations, we achieve 8.3\% and 5.0\% absolute Fscore gains on multimedia event extraction and argument role labeling, respectively. By utilizing images, we extract 21.4\% more event mentions than traditional text-only methods.},
	language = {en},
	urldate = {2024-02-06},
	booktitle = {Proceedings of the 58th {Annual} {Meeting} of the {Association} for {Computational} {Linguistics}},
	publisher = {Association for Computational Linguistics},
	author = {Li, Manling and Zareian, Alireza and Zeng, Qi and Whitehead, Spencer and Lu, Di and Ji, Heng and Chang, Shih-Fu},
	year = {2020},
	keywords = {/unread, dataset},
	pages = {2557--2568},
	file = {Li et al. - 2020 - Cross-media Structured Common Space for Multimedia.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/NFL2U2HR/Li et al. - 2020 - Cross-media Structured Common Space for Multimedia.pdf:application/pdf},
}

@article{ramesh_hierarchical_2022,
	title = {Hierarchical {Text}-{Conditional} {Image} {Generation} with {CLIP} {Latents}},
	url = {http://arxiv.org/abs/2204.06125},
	abstract = {Contrastive models like CLIP have been shown to learn robust representations of images that capture both semantics and style. To leverage these representations for image generation, we propose a two-stage model: a prior that generates a CLIP image embedding given a text caption, and a decoder that generates an image conditioned on the image embedding. We show that explicitly generating image representations improves image diversity with minimal loss in photorealism and caption similarity. Our decoders conditioned on image representations can also produce variations of an image that preserve both its semantics and style, while varying the non-essential details absent from the image representation. Moreover, the joint embedding space of CLIP enables language-guided image manipulations in a zero-shot fashion. We use diffusion models for the decoder and experiment with both autoregressive and diffusion models for the prior, ﬁnding that the latter are computationally more efﬁcient and produce higher-quality samples.},
	language = {en},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Ramesh, Aditya and Dhariwal, Prafulla and Nichol, Alex and Chu, Casey and Chen, Mark},
	month = apr,
	year = {2022},
	note = {arXiv:2204.06125 [cs]},
	keywords = {/unread, Computer Science - Computer Vision and Pattern Recognition},
	file = {Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/YV6FHM3B/Ramesh et al. - 2022 - Hierarchical Text-Conditional Image Generation with CLIP Latents.pdf:application/pdf},
}

@article{baltrusaitis_multimodal_2017,
	title = {Multimodal {Machine} {Learning}: {A} {Survey} and {Taxonomy}},
	shorttitle = {Multimodal {Machine} {Learning}},
	url = {http://arxiv.org/abs/1705.09406},
	abstract = {Our experience of the world is multimodal - we see objects, hear sounds, feel texture, smell odors, and taste flavors. Modality refers to the way in which something happens or is experienced and a research problem is characterized as multimodal when it includes multiple such modalities. In order for Artificial Intelligence to make progress in understanding the world around us, it needs to be able to interpret such multimodal signals together. Multimodal machine learning aims to build models that can process and relate information from multiple modalities. It is a vibrant multi-disciplinary field of increasing importance and with extraordinary potential. Instead of focusing on specific multimodal applications, this paper surveys the recent advances in multimodal machine learning itself and presents them in a common taxonomy. We go beyond the typical early and late fusion categorization and identify broader challenges that are faced by multimodal machine learning, namely: representation, translation, alignment, fusion, and co-learning. This new taxonomy will enable researchers to better understand the state of the field and identify directions for future research.},
	urldate = {2024-02-08},
	publisher = {arXiv},
	author = {Baltrušaitis, Tadas and Ahuja, Chaitanya and Morency, Louis-Philippe},
	month = aug,
	year = {2017},
	note = {arXiv:1705.09406 [cs]},
	keywords = {/unread, Computer Science - Machine Learning},
	file = {arXiv Fulltext PDF:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/6VKVZERG/Baltrušaitis et al. - 2017 - Multimodal Machine Learning A Survey and Taxonomy.pdf:application/pdf;arXiv.org Snapshot:/Users/terencelau/Library/CloudStorage/OneDrive-kingtime/Research/ZOTERO-DATABASE/storage/J5X5U5HP/1705.html:text/html},
}


@article{alayrac2022flamingo,
  title={Flamingo: a visual language model for few-shot learning},
  author={Alayrac, Jean-Baptiste and Donahue, Jeff and Luc, Pauline and Miech, Antoine and Barr, Iain and Hasson, Yana and Lenc, Karel and Mensch, Arthur and Millican, Katherine and Reynolds, Malcolm and others},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={23716--23736},
  year={2022}
}

@article{bao2022vlmo,
  title={Vlmo: Unified vision-language pre-training with mixture-of-modality-experts},
  author={Bao, Hangbo and Wang, Wenhui and Dong, Li and Liu, Qiang and Mohammed, Owais Khan and Aggarwal, Kriti and Som, Subhojit and Piao, Songhao and Wei, Furu},
  journal={Advances in Neural Information Processing Systems},
  volume={35},
  pages={32897--32912},
  year={2022}
}

@article{li2021align,
  title={Align before fuse: Vision and language representation learning with momentum distillation},
  author={Li, Junnan and Selvaraju, Ramprasaath and Gotmare, Akhilesh and Joty, Shafiq and Xiong, Caiming and Hoi, Steven Chu Hong},
  journal={Advances in neural information processing systems},
  volume={34},
  pages={9694--9705},
  year={2021}
}

@article{dosovitskiy2020image,
  title={An image is worth 16x16 words: Transformers for image recognition at scale},
  author={Dosovitskiy, Alexey and Beyer, Lucas and Kolesnikov, Alexander and Weissenborn, Dirk and Zhai, Xiaohua and Unterthiner, Thomas and Dehghani, Mostafa and Minderer, Matthias and Heigold, Georg and Gelly, Sylvain and others},
  journal={arXiv preprint arXiv:2010.11929},
  year={2020}
}

@article{devlin2018bert,
  title={Bert: Pre-training of deep bidirectional transformers for language understanding},
  author={Devlin, Jacob and Chang, Ming-Wei and Lee, Kenton and Toutanova, Kristina},
  journal={arXiv preprint arXiv:1810.04805},
  year={2018}
}

@article{li2023blip,
  title={Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models},
  author={Li, Junnan and Li, Dongxu and Savarese, Silvio and Hoi, Steven},
  journal={arXiv preprint arXiv:2301.12597},
  year={2023}
}

@marticle{dai2023instructblip,
      title={InstructBLIP: Towards General-purpose Vision-Language Models with Instruction Tuning}, 
      author={Wenliang Dai and Junnan Li and Dongxu Li and Anthony Meng Huat Tiong and Junqi Zhao and Weisheng Wang and Boyang Li and Pascale Fung and Steven Hoi},
      year={2023},
      eprint={2305.06500},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  journal={Advances in neural information processing systems},
  volume={30},
  year={2017}
}

@article{oord2018neural,
      title={Neural Discrete Representation Learning}, 
      author={Aaron van den Oord and Oriol Vinyals and Koray Kavukcuoglu},
      year={2018},
      eprint={1711.00937},
      archivePrefix={arXiv},
      primaryClass={cs.LG}
}

@article{zhu2023minigpt4,
      title={MiniGPT-4: Enhancing Vision-Language Understanding with Advanced Large Language Models}, 
      author={Deyao Zhu and Jun Chen and Xiaoqian Shen and Xiang Li and Mohamed Elhoseiny},
      year={2023},
      eprint={2304.10592},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}

@article{zheng2023minigpt5,
      title={MiniGPT-5: Interleaved Vision-and-Language Generation via Generative Vokens}, 
      author={Kaizhi Zheng and Xuehai He and Xin Eric Wang},
      year={2023},
      eprint={2310.02239},
      archivePrefix={arXiv},
      primaryClass={cs.CV}
}