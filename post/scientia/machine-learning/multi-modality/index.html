<!DOCTYPE html>
<html lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</title>
    <meta charset="utf-8">
    <meta name="description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the">
    <meta name="author" content="Junjie(Terence) LIU">
    <link rel="canonical" href="https://blog.cklau.cc/post/scientia/machine-learning/multi-modality/">
    <link rel="stylesheet" type="text/css" href="/hugo-cite.css" />
        <meta name="google-site-verification" content="GTM-MC97JQPW">

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PXB7GSCQBT"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-PXB7GSCQBT', { 'anonymize_ip': false });
}
</script>



    <meta property="og:title" content="üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image" />
<meta property="og:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.cklau.cc/post/scientia/machine-learning/multi-modality/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-02-08T00:15:00+00:00" />
<meta property="article:modified_time" content="2024-02-08T16:49:45+00:00" />



<meta name="twitter:card" content="summary"/>
<meta name="twitter:title" content="üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image"/>
<meta name="twitter:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the"/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "https://blog.cklau.cc/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "",
      "item": "https://blog.cklau.cc/post/scientia/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "Machine Learning/Deep Learning with Us",
      "item": "https://blog.cklau.cc/post/scientia/machine-learning/"
    }, 
    {
      "@type": "ListItem",
      "position":  5 ,
      "name": "üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image",
      "item": "https://blog.cklau.cc/post/scientia/machine-learning/multi-modality/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image",
  "name": "üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image",
  "description": "Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the",
  "keywords": [
    "Artificial Intelligence", "text-image", "multi-modality", "long-read"
  ],
  "articleBody": "Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.\nHowever, different data mode has different representation, especially for the machine, i.e., different mode of data has different representation. Some of the modality can be approximated by the others, e.g. audio can be transformed into images via the Fourier Transformation [^1].\nUsually, the multimodal learning includes these five aspects ( Citation: Baltru≈°aitis, Ahuja \u0026 al., 2017 Baltru≈°aitis, T., Ahuja, C. \u0026 Morency, L. (2017). Multimodal Machine Learning: A Survey and Taxonomy. arXiv. Retrieved from http://arxiv.org/abs/1705.09406 ) :\nRepresentation: It is important for us to know how to represent each unimodality or multimodality Translation: This is the mapping from one data mode to another, e.g. using spectrogram to transform audio to image Alignment or Registration: This is the direct relation between elements from two or more different modalities. If they are in the same modality, we would call it registration. Fusion: It is to join information from two or more modalities to perform a downstream task. Co-learning: Transforming knowledge from different modalities For this series, we are not shaped by the five main aspects, instead, we would organized by modalities. In this post, we are focusing on the Text and Image, with such amounts of researchs and works, I decided to follow on the two papers: CLIP ( Citation: Radford, Kim \u0026 al., 2021 Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G. \u0026 Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. ) and BLIP ( Citation: Li, Li \u0026 al., 2022 Li, J., Li, D., Xiong, C. \u0026 Hoi, S. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. ) , some of their variants would be mentioned.\nCLIP: Learning Transferable Visual Models From Natural Language Supervision CLIP‚Äôs key contribution is its ability to map text and image into a shared embedding space based on the seperated text/image encoder. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier. Not meaning that it can directly used in these tasks, but providing a idea that how to do the multimodal representation.\nReference Baltru≈°aitis, Ahuja \u0026 Morency (2017) Baltru≈°aitis, T., Ahuja, C. \u0026 Morency, L. (2017). Multimodal Machine Learning: A Survey and Taxonomy. arXiv. Retrieved from http://arxiv.org/abs/1705.09406 Li, Li, Xiong \u0026 Hoi (2022) Li, J., Li, D., Xiong, C. \u0026 Hoi, S. (2022). BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation. Radford, Kim, Hallacy, Ramesh, Goh, Agarwal, Sastry, Askell, Mishkin, Clark, Krueger \u0026 Sutskever (2021) Radford, A., Kim, J., Hallacy, C., Ramesh, A., Goh, G., Agarwal, S., Sastry, G., Askell, A., Mishkin, P., Clark, J., Krueger, G. \u0026 Sutskever, I. (2021). Learning Transferable Visual Models From Natural Language Supervision. [^1] For more information, pleace check Spectrogram and Mel Spectrogram\n",
  "wordCount" : "606",
  "inLanguage": "en",
  "datePublished": "2024-02-08T00:15:00Z",
  "dateModified": "2024-02-08T16:49:45Z",
  "author":{
    "@type": "Person",
    "name": "Junjie(Terence) LIU"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.cklau.cc/post/scientia/machine-learning/multi-modality/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.cklau.cc/favicon.ico"
    }
  }
}
</script>
    <link rel="icon" type="image/png" href="/images/icon.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/icon.png">

<link rel="manifest" href="/images/icon.png">
    
    
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.css"
    integrity="sha384-R4558gYOUz8mP9YWpZJjofhk+zx0AS11p36HnD2ZKj/6JR5z27gSSULCNHIRReVs" crossorigin="anonymous">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/katex.min.js"
    integrity="sha384-z1fJDqw8ZApjGO3/unPWUPsIymfsJmyrDVWC8Tv/a1HeOtGmkwNd/7xUS0Xcnvsx"
    crossorigin="anonymous"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.15.1/dist/contrib/auto-render.min.js"
    integrity="sha384-+XBljXPPiv+OzfbB3cVmLHf4hdUFHlWNZN5spNQ7rmHTXpd7WvJum6fIACpNNfIR"
    crossorigin="anonymous"></script>
<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true },
                { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                { left: "\\begin{align}", right: "\\end{align}", display: true },
                { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
                { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                { left: "\\[", right: "\\]", display: true }
            ],
            
            throwOnError: false,
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>
    

    
    
    
    <link rel="stylesheet" href="/css/main.min.2ea55aaafa2a39230056f2d122a89dd2aa2a3883be0a592340a1091d201175f9.css" integrity="sha256-LqVaqvoqOSMAVvLRIqid0qoqOIO&#43;ClkjQKEJHSARdfk=" crossorigin="anonymous" media="screen" />
    


    
    <link rel="stylesheet" href="/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css" />

    
    <script src="/js/highlight.min.min.c098d85b5396dec4707ea2cead1445b4dc2ff0fc56b8dbbd9049d0d1c50ad237.js"></script>
    <script>hljs.highlightAll();</script>

    <script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script>
    
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
</head>
<body>
      <main class="wrapper"><nav class="navigation">
    <section class="container">
        <a class="navigation-brand" href="/">
            CKLAU&#39;s WEBSITE
        </a>
        <input type="checkbox" id="menu-toggle" />
        <label class="menu-button float-right" for="menu-toggle">
            <span></span><span></span><span></span>
        </label>
        
        <ul class="navigation-list" id="navigation-list">
            
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/projects">Project</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/post">Post</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/friends">Friends</a>
            </li>
            
            

            <li class="navigation-item menu-separator">
                <span>|</span>
            </li>

            
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://terenceliu98.github.io/index.xml"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
            </li>
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://github.com/TerenceLiu98"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a>
            </li>
            
            

            <li class="navigation-item navigation-dark">
                <button id="mode" type="button" aria-label="toggle user light or dark theme">
                    <span class="toggle-dark"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span>
                    <span class="toggle-light"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></span>
                </button>
            </li>

            
            
            
            
            
            
            
            <li class="navigation-item navigation-language">
                <a href="https://blog.cklau.cc/zh/">‰∏≠</a>
            </li>
            
            
            
            
        </ul>
        
    </section>
</nav>
<div id="content">
<article class="blog-single">
  <header class="blog-title">
    <h1>üë®üèø‚Äç‚öïÔ∏è Multimodal Representation Leraning from both Text and Image</h1>
  </header>

  <p>
  <small>
    
    
    
    <u># <a href="https://blog.cklau.cc/tags/artificial-intelligence/">Artificial Intelligence</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/text-image/">text-image</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/multi-modality/">multi-modality</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/long-read/">long-read</a></u>
    
  </small>
  <br>
  <small>
    Published On: <u>February 8, 2024</u> (Last updated on: <u>February 8, 2024)</u> <br>
    606 words&nbsp;¬∑ 2 min</small>
</p>



  <div class="blog-toc">
    <nav id="TableOfContents">
  <ul>
    <li><a href="#before">Before</a></li>
    <li><a href="#clip-learning-transferable-visual-models-from-natural-language-supervision">CLIP: Learning Transferable Visual Models From Natural Language Supervision</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
  </div>

  <section class="blog-content"><h2 id="before">Before</h2>
<p>For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.</p>
<p>However, different data mode has different representation, especially for the machine, i.e., different mode of data has different representation. Some of the modality can be approximated by the others, e.g. audio can be transformed into images via the Fourier Transformation [^1].</p>
<p>Usually, the multimodal learning includes these five aspects 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#baltrusaitis_multimodal_2017"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Tadas"><span itemprop="familyName">Baltru≈°aitis</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Chaitanya"><span itemprop="familyName">Ahuja</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2017</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/CreativeWork"
      data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baltru≈°aitis</span>,&#32;
    <meta itemprop="givenName" content="Tadas" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ahuja</span>,&#32;
    <meta itemprop="givenName" content="Chaitanya" />
    C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Morency</span>,&#32;
    <meta itemprop="givenName" content="Louis-Philippe" />
    L.</span>
  &#32;
    (<span itemprop="datePublished">2017</span>).
  &#32;<span itemprop="name">
    <i>Multimodal Machine Learning: A Survey and Taxonomy</i></span>.
  &#32;
  <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
    <span itemprop="name">arXiv</span></span>.&#32;Retrieved from&#32;
  <a href="http://arxiv.org/abs/1705.09406"
     itemprop="identifier"
     itemtype="https://schema.org/URL">http://arxiv.org/abs/1705.09406</a></span>

</span></span>)</span>
:</p>
<ol>
<li>Representation: It is important for us to know how to represent each unimodality or multimodality</li>
<li>Translation: This is the mapping from one data mode to another, e.g. using spectrogram to transform audio to image</li>
<li>Alignment or Registration: This is the direct relation between elements from two or more different modalities. If they are in the same modality, we would call it <em>registration</em>.</li>
<li>Fusion: It is to join information from two or more modalities to perform a downstream task.</li>
<li>Co-learning: Transforming knowledge from different modalities</li>
</ol>
<p>For this series, we are not shaped by the five main aspects, instead, we would organized by modalities. In this post, we are focusing on the Text and Image, with such amounts of researchs and works, I decided to follow on the two papers: CLIP 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#radford_learning_nodate"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Alec"><span itemprop="familyName">Radford</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Jong Wook"><span itemprop="familyName">Kim</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2021</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
    <meta itemprop="givenName" content="Alec" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
    <meta itemprop="givenName" content="Jong Wook" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hallacy</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
    <meta itemprop="givenName" content="Aditya" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goh</span>,&#32;
    <meta itemprop="givenName" content="Gabriel" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Agarwal</span>,&#32;
    <meta itemprop="givenName" content="Sandhini" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
    <meta itemprop="givenName" content="Girish" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
    <meta itemprop="givenName" content="Amanda" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
    <meta itemprop="givenName" content="Pamela" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
    <meta itemprop="givenName" content="Gretchen" />
    G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
    <meta itemprop="givenName" content="Ilya" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">Learning Transferable Visual Models From Natural Language Supervision</span>.</span>




</span></span>)</span>
 and BLIP 




<span class="hugo-cite-intext"
        itemprop="citation">(<span class="hugo-cite-group">

          <a href="#li_blip_nodate"><span class="visually-hidden">Citation: </span><span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Junnan"><span itemprop="familyName">Li</span></span>,&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><meta itemprop="givenName" content="Dongxu"><span itemprop="familyName">Li</span></span>
                  <em>&amp; al.</em>,&#32;<span itemprop="datePublished">2022</span></a><span class="hugo-cite-citation"> 










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Junnan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Dongxu" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Caiming" />
    C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hoi</span>,&#32;
    <meta itemprop="givenName" content="Steven" />
    S.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</span>.</span>




</span></span>)</span>
, some of their variants would be mentioned.</p>
<h2 id="clip-learning-transferable-visual-models-from-natural-language-supervision">CLIP: Learning Transferable Visual Models From Natural Language Supervision</h2>
<p>CLIP&rsquo;s key contribution is its ability to map text and image into a shared embedding space based on the seperated text/image encoder. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier. Not meaning that it can directly used in these tasks, but providing a idea that how to do the multimodal representation.</p>
<!-- raw HTML omitted -->
<h2 id="reference">Reference</h2>

  

  










<section class="hugo-cite-bibliography">
  <dl>
    

      <div id="baltrusaitis_multimodal_2017">
        <dt>
          Baltru≈°aitis,&#32;
          Ahuja&#32;&amp;&#32;Morency

          
          (2017)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/CreativeWork"
      data-type="default"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Baltru≈°aitis</span>,&#32;
    <meta itemprop="givenName" content="Tadas" />
    T.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ahuja</span>,&#32;
    <meta itemprop="givenName" content="Chaitanya" />
    C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Morency</span>,&#32;
    <meta itemprop="givenName" content="Louis-Philippe" />
    L.</span>
  &#32;
    (<span itemprop="datePublished">2017</span>).
  &#32;<span itemprop="name">
    <i>Multimodal Machine Learning: A Survey and Taxonomy</i></span>.
  &#32;
  <span itemprop="publisher" itemtype="http://schema.org/Organization" itemscope="">
    <span itemprop="name">arXiv</span></span>.&#32;Retrieved from&#32;
  <a href="http://arxiv.org/abs/1705.09406"
     itemprop="identifier"
     itemtype="https://schema.org/URL">http://arxiv.org/abs/1705.09406</a></span>

</dd>

      </div>

      <div id="li_blip_nodate">
        <dt>
          Li,&#32;
          Li,&#32;
          Xiong&#32;&amp;&#32;Hoi

          
          (2022)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Junnan" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Li</span>,&#32;
    <meta itemprop="givenName" content="Dongxu" />
    D.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Xiong</span>,&#32;
    <meta itemprop="givenName" content="Caiming" />
    C.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hoi</span>,&#32;
    <meta itemprop="givenName" content="Steven" />
    S.</span>
  &#32;
    (<span itemprop="datePublished">2022</span>).
  &#32;<span itemprop="name">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</span>.</span>




</dd>

      </div>

      <div id="radford_learning_nodate">
        <dt>
          Radford,&#32;
          Kim,&#32;
          Hallacy,&#32;
          Ramesh,&#32;
          Goh,&#32;
          Agarwal,&#32;
          Sastry,&#32;
          Askell,&#32;
          Mishkin,&#32;
          Clark,&#32;
          Krueger&#32;&amp;&#32;Sutskever

          
          (2021)</dt>

        <dd>
          










<span itemscope
      itemtype="https://schema.org/Article"
      data-type="article"><span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Radford</span>,&#32;
    <meta itemprop="givenName" content="Alec" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Kim</span>,&#32;
    <meta itemprop="givenName" content="Jong Wook" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Hallacy</span>,&#32;
    <meta itemprop="givenName" content="Chris" />
    C.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Ramesh</span>,&#32;
    <meta itemprop="givenName" content="Aditya" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Goh</span>,&#32;
    <meta itemprop="givenName" content="Gabriel" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Agarwal</span>,&#32;
    <meta itemprop="givenName" content="Sandhini" />
    S.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sastry</span>,&#32;
    <meta itemprop="givenName" content="Girish" />
    G.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Askell</span>,&#32;
    <meta itemprop="givenName" content="Amanda" />
    A.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Mishkin</span>,&#32;
    <meta itemprop="givenName" content="Pamela" />
    P.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Clark</span>,&#32;
    <meta itemprop="givenName" content="Jack" />
    J.</span>,&#32;
  <span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Krueger</span>,&#32;
    <meta itemprop="givenName" content="Gretchen" />
    G.</span>&#32;&amp;&#32;<span itemprop="author" itemscope itemtype="https://schema.org/Person"><span itemprop="familyName">Sutskever</span>,&#32;
    <meta itemprop="givenName" content="Ilya" />
    I.</span>
  &#32;
    (<span itemprop="datePublished">2021</span>).
  &#32;<span itemprop="name">Learning Transferable Visual Models From Natural Language Supervision</span>.</span>




</dd>

      </div>
  </dl>
</section>



<p>[^1] For more information, pleace check <a href="https://en.wikipedia.org/wiki/Spectrogram">Spectrogram</a> and <a href="https://ieeexplore.ieee.org/document/9859621">Mel Spectrogram</a></p>
</section>

  
  
  <div class="paginator">
    
    
    <a class="next" href="https://blog.cklau.cc/post/specification/"><span>ü§î A General Introduction of ML/DL Project Management and Knowledge Management - Project Management</span><span>&nbsp;&nbsp;&rarr;</span></a>
    
  </div>
  

  
  
  

<div class="related-resources">
  <h3>Related Resources</h3>
  
    
    
    
  
</div>


</article>

        </div><footer class="footer">
<center>
  <p>&copy; 2024 <a href="https://blog.cklau.cc">ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</a> &nbsp;
    <a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">Á≤§ICPÂ§á2022102668Âè∑</a>
Ô∏è  </p>
</center>
</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"></line><polyline points="5 12 12 5 19 12"></polyline></svg>
</a>

<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>

<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copy';

        function copyingDone() {
            copybutton.innerHTML = 'Copied';
            setTimeout(() => {
                copybutton.innerHTML = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });
        codeblock.parentNode.appendChild(copybutton);
    });
</script></main>
    </body><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script>
      const images = Array.from(document.querySelectorAll(".blog-content img"));
      images.forEach(img => {
          mediumZoom(img, {
              margin: 10,  
              scrollOffset: 40,  
              container: null,  
              template: null,  
              background: 'rgba(0, 0, 0, 0.5)'
          });
      });
  </script>

  
  <script src="/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js" integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin="anonymous" defer></script></html>
