<!DOCTYPE html>
<html lang="en"><head>
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</title>
    <meta charset="utf-8">
    <meta name="description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.">
    <meta name="author" content="Junjie(Terence) LIU">
    <link rel="canonical" href="https://blog.cklau.cc/post/scientia/multi-modality/">
        <meta name="google-site-verification" content="GTM-MC97JQPW">

    
<script async src="https://www.googletagmanager.com/gtag/js?id=G-PXB7GSCQBT"></script>
<script>
var doNotTrack = false;
if (!doNotTrack) {
	window.dataLayer = window.dataLayer || [];
	function gtag(){dataLayer.push(arguments);}
	gtag('js', new Date());
	gtag('config', 'G-PXB7GSCQBT', { 'anonymize_ip': false });
}
</script>



    <meta property="og:title" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image" />
<meta property="og:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.cklau.cc/post/scientia/multi-modality/" /><meta property="article:section" content="post" />
<meta property="article:published_time" content="2024-02-08T00:15:00+00:00" />
<meta property="article:modified_time" content="2024-02-11T19:37:16+00:00" />




<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image"/>
<meta name="twitter:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio."/>


<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BreadcrumbList",
  "itemListElement": [, 
    {
      "@type": "ListItem",
      "position":  2 ,
      "name": "",
      "item": "https://blog.cklau.cc/post/"
    }, 
    {
      "@type": "ListItem",
      "position":  3 ,
      "name": "Scientia",
      "item": "https://blog.cklau.cc/post/scientia/"
    }, 
    {
      "@type": "ListItem",
      "position":  4 ,
      "name": "üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image",
      "item": "https://blog.cklau.cc/post/scientia/multi-modality/"
    }
  ]
}
</script>
<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "headline": "üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image",
  "name": "üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image",
  "description": "Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.",
  "keywords": [
    "Artificial Intelligence", "text-image", "multi-modality", "long-read"
  ],
  "articleBody": "Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.\nHowever, different data mode has different representation, especially for the machine, i.e., different mode of data has different representation. Some of the modality can be approximated by the others, e.g. audio can be transformed into images via the Fourier Transformation 1.\nUsually, the multimodal learning includes these five aspects [3]:\nRepresentation: It is important for us to know how to represent each unimodality or multimodality Translation: This is the mapping from one data mode to another, e.g. using spectrogram to transform audio to image Alignment or Registration: This is the direct relation between elements from two or more different modalities. If they are in the same modality, we would call it registration. Fusion: It is to join information from two or more modalities to perform a downstream task. Co-learning: Transforming knowledge from different modalities For this series, we are not shaped by the five main aspects, instead, we would organized by modalities. In this post, we are focusing on the Text and Image, with such amounts of researchs and works, First, we would introduce the big short from OpenAI: CLIP [4] as it can be seen as one of the milstones of multimodality studies. After the CLIP models, there are multiple different kinds of multimodal-based models, e.g. ALBEF[5], BLIP [6], VLMO [7], and Flamingo [8]. I chose BLIP [9] as it can be seen as the supplimentary of the CLIP model, as it not only consider the representation, but also the understanding of the multimodal data.\nCLIP: Learning Transferable Visual Models From Natural Language Supervision CLIP‚Äôs key contribution is its ability to map text and image into a shared embedding space based on the seperated text/image encoder. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier. Not meaning that it can directly used in these tasks, but providing a idea that how to do the multimodal representation.\nFig. 1. The architecture of CLIP model\nThe work shown above is Copyrighted to OPENAI.\nFig. 1. shows the approach of CLIP model. Text and image are encoded by two different encoders $f_{\\theta_i}$ and $g_{\\theta_t}$, let $\\mathbf{x} \\in \\mathbb{R}^{N \\times H \\times W \\times C}$ as one batch of image, $\\mathbf{y} \\in \\mathbb{R}^{N \\times S}$ as one batch of text data, the embedding of $\\mathbf{x}$ and $\\mathbf{y}$ then can be denoted as:\n$$\\begin{aligned} \\mathbf{f} \u0026= f_{\\theta_i}(\\mathbf{x}) \\in \\mathbf{R}^{N \\times D_i} \\Rightarrow \\mathbf{f}^e = L_i(\\mathbf{f}) \\cr \\mathbf{g} \u0026= g_{\\theta_t}(\\mathbf{y}) \\in \\mathbf{R}^{N \\times D_t} \\Rightarrow \\mathbf{g}^e = L_t(\\mathbf{g}) \\end{aligned}$$\nwhere $D_i$ and $D_t$ are the dimension of the image and text embedding, respectively. linear projectors $L_i$ and $L_t$ are used for mapping two embedding into the same dimension. The dot product between the image and text embedding is used to calculate the similarity between the text and image, i.e., $\\mathcal{F} = \\mathbf{f} \\cdot \\mathbf{g}$, where $\\mathcal{F} \\in \\mathbb{R}^{N \\times N}$ is the similarity matrix and the $\\mathcal{F} \\circ I_N$ is the positive sample set where $I_N$ is the identity matrix, and $\\mathcal{F} \\circ I_N$ is the matrix‚Äôs diagonal , and the others are the negative samples (In total, there are $N^2 - N$ negatives samples). The pseudocode of the CLIP model is shown in Figure 2 (original papers). The idea of CLIP model is relative naive, as it is a classic negative sampling method called batch negative sampling. However, intuitively, it is hard for us to do the model explanation, i.e., why does it work? Still, we could explan that the corresponding text and image are sharing the same ontology, but this kind of explanation is not grounded. More or less, the main contribution is that they create a huge dataset with the text and image pairs, including 400 million (image, text) pairs collected form of a variety of publicly available sources on the Internet. With this dataset, they don‚Äôt even need the pretrained encoder as the encoder can be trained simultaneously with the downstream alignment.\nImplementation Here is the simple implementation of CLIP:\nImplementation of Encoders import torch import torch.nn as nn from transformers import AutoConfig, AutoModel, AutoModelForImageClassification import numpy as np class TextEncoder(nn.Module): def __init__(self, model_name=\"FacebookAI/roberta-base\":str, device=\"cpu\":str, pretrained=True:bool, freeze=False:bool): super(TextEncoder).__init__() self.model = {True: AutoModel.from_pretrained(model_name), False: AutoModel.from_config(AutoConfig.from_pretrained(model_name))} self.out_dim = AutoConfig.from_pretrained(model_name).hidden_size # 768 self.freeze = freeze if freeze: for name ,param in self.model.named_parameters(): param.requires_grad = False def forward(self, inputs): if self.freeze: self.model.eval() else: pass feature = nn.functional.normalize(torch.mean(self.model(**inputs).last_hidden_state, axis=1), dim=1) return feature class ImageEncoder(nn.Module): def __init__(self, model_name=\"google/vit-base-patch16-224\":str, device=\"cpu\":str, pretrained=True:bool, freeze=False:bool): super(TextEncoder).__init__() self.model = {True: AutoModelForImageClassification.from_pretrained(model_name), False: AutoModelForImageClassification.from_config(AutoConfig.from_pretrained(model_name))} self.out_dim = 1000 # ViT is trained with ImageNet self.freeze = freeze if freeze: for name ,param in self.model.named_parameters(): param.requires_grad = False def forward(self, inputs): feature = self.model(**inputs).logits feature = F.normalize(feature, dim=-1) return feature Implementation of Linear Projection class LinearProject(nn.Module): def __init__(self, in_features, out_features): super(LinearProject).__init__() self.projection == nn.Sequential([ nn.Linear(in_features, out_features), nn.GELU(), nn.LayerNorm(out_features) ]) def forward(self, x): output = self.projection(x) return output Implementation of CLIP Model class CLIPModel(nn.Module): def __init__(self, model_name = {\"TEXT\":\"FacebookAI/roberta-base\", \"IMG\":\"google/vit-base-patch16-224\"}:dict, device=\"cpu\":str, pretrained=True:bool, freeze=False:bool, hidden_dim=256:int): super(CLIPModel).__init__() self.enc_t = TextEncoder(model_name=model_name[\"TEXT\"], device=device, freeze=freeze) self.enc_i = ImageEncoder(model_name=model_name[\"IMG\"], device=device, freeze=freeze) self.proj_t = LinearProject(self.enc_t.out_dim, hidden_dim) self.proj_i = LinearProject(self.enc_i.out_dim, hidden_dim) self.logit_scale = nn.Parameter(torch.ones([])) self.init_parameters() def init_parameters(self): # turn temperature into a learnable parameter nn.init.constant_(self.logit_scale, np.log(1 / 0.07)) def criterion(self, text, image): CE = nn.functional.cross_entropy labels = torch.arange(text.shape[0], device=str(text.device), dtype=torch.long) logits_t = text @ image.T * self.logit_scale.exp() logits_i = image @ text.T * self.logit_scale.exp() loss = (CE(logits_t, labels) + CE(logits_i, labels)) / 2 return loss def forward(self, text, image): feature_t, feature_i = self.proj_t(self.enc_t(text)), self.proj_i(self.enc_i(image)) loss = self.criterion(feature_t, feature_i) return feature_t, feature_i, self.logit_scale, loss Applications The first application of CLIP is classification. Since the CLIP model is relatively similar to the reitrival model, it can be easily implemented into the classification with zero-shot learning. The zero-shot learning is a task that the model can classify the unseen classes without any training data. See the second part of Figure 1, where for a given image, the model can classify the similarity between the given image and the prompted text, by calculating the similarity of image and each given sentence we could get the classification, vice versa. This can be seen as classification, or retrieval.\nThe second application is the generation. The CLIP model can be used to generate the image from the given text. Although it cannot generate the image directly since it does not have any decoder, however, the CLIP can be seen as the backbone and provide the embedding for image or text generation. After the CLIP came out, OpenAI also released the DALL¬∑E 2, where they provide a model called unCLIP [6], it is a text-to-image generation model2.\nFull Text Full Text BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation Unlike the CLIP‚Äôs siamese-shaped model, BLIP is a multimodal architecture that seperate the representation and generation into a two stage process. In the first stage, they generate the text embedding and image embedding, and for the second stage, they use the decoder that generates an image conditioned on the image embedding. This architecture is proposed based on these two research questions:\nMost existing pre-trained models only perform well on comprehension-based tasks or generation-based tasks, and few models can do both. Most existing pre-trained models extend the dataset using noisy image-text pairs collected from the network in order to improve performance. Although this improves performance, it is obvious that this noisy supervision signal is definitely not optimal3. Based on these two research gap, they proposed the Multimodel mixture of Encoder-Decoder(MED) architecture, shown in Fig. 2.\nFig. 2. The Architecture of ‚ÄòMultimodel mixture of Encoder-Decoder‚Äô (MED) architecture\nThe work shown above is Copyrighted to Salesforce Research.\nThe three tasks are proposed for the training:\nImage-text contrastive (ITC) learning: on the very left side of Fig. 2. is a Image Encoder, where it adapts the ViT[10] for the image representation. The second part is the Text Encoder, where it adapts the BERT[11] for the text representation. The ITC learning is to maximize the similarity between the image and text embedding, and minimize the similarity between the negative samples. The idea of the ITC is the same as CLIP where using the contrastive learning to align the text and image embedding. Image-text matching (ITM) learning: The third modules is Image-graouded Text Decoder, where it is a transformer-based decoder with cross-attention layer to include the image embedding, and the mixed embedding are used for the ITM learning to check whether the text and image are aligned. Language modeling (LM) learning: The last part is the Image-grounded text decoder. This decoder uses causal self-attention to replace the self-attetion layer in the BERT, and the image embedding also injected with the cross attention block for the text generation. To leverage the multi-task learning process, all the blocks with the same color are shareing the same parameters. This MED architecture shares some common modules with the ALBEF[5] and VLMO [7] where the ITC is adapted from the CLIP while the ITM can be seen as the refinement ore the constrastive learning, as it take the hardest negative samples (from the ITC) as the negative samples in ITM; and the LM is for the text generation, which is the same as the VLMO.\nFig. 3. The CapFilt module for solving the noisy supervision signal\nThe work shown above is Copyrighted to Salesforce Research.\nThe second contribution of the paper is the pipeline of eliminating the noisy superivion signal, shown in Fig. 3. where the Image-grounded Text Encoder and Image-grounded Text Decoder are used as Filter and Captioner. The Filter is to remove the noisy image-text pairs while the Captioner is used for generating synthetic captions given a web images. The process of Capfilt is as follows: 1) The training set $D$ is formed with web image-text pairs and clean image-text pairs, denoted as $D_w = (I_w, T_w)$ and $D_h = (I_h, T_h)$, and $D = D_w + D_h$. 2) Fine-tune the two encoder with $D_h$. 3) The Captioner generates caption with $D_w$‚Äôs images. 4) The Filter is used to remove the noisy text not matter the given text is scraped from the web or synthesis from the captioner . Thus, the model-based data cleansing is used to remove the noisy supervision signal and the cleansed data is used for the training of the next MED so call bootstraping .\nImplementation You may check BLIP-official Code from GitHub. You may see that med.py#97, they define the function BertSelfAttention to include the cross-attention layer into the BERT‚Äôs encoder.\nApplications The BLIP model is used for multiple downstream tasks: Image-text retrieval (with Image Encoder and Text Encoder), Image Captioning(with Image-grounded Text Decoder), Visual Question Answering(with Image Encoder), Natural Language Visual Reasoning(with Image Encoder and Text Encoder), and Visual Dialog(with Image Encoder and Image-grounded text Encoder). The detailed can be seen in the original paper.\nFull Text Full Text Related Researches BLIP‚Äôs Group allow continue their work on the multimodal representation learning, they proposed BLIP2[12] and InstructBLIP[13]:\nBLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models BLIP2 is a successor of BLIP, the overall framework is shown in Fig. 4., still, it contains three training tasks: ITC, ITM, and Text generation. Two-stage process is proposed: the first stage is visuion-lanaguge representation learning and the second stage is the vision-language generative learning:\nrepresentation learning: the Q-former contains three modules, where at the left is the pretrained image encoder(the parameter is frozen), the middle one is learnable query encoder and the right one is the text decoder. The query encoder and text decoder are adapted from the classic transformer architecture [14], the queries is a learnable vector set that is used to query the extracted visual representation most relevant to the given text Since the self-attention layer is shared, which may insure the coherence between query module and the text module. , and the text decoder is used for the alignment. generative learning: the downsteam module is the pretrained LLM model(the parameter is frozen). After the stage 1, the learned queries are the visual represenataions, and in this stage, a linear projection is needed to map the queries into the LLM(also, we need to reshape queries‚Äô dimension to fit the LLM). For the Decoder-based LLM(like GPT), we use query as the only input For the Encoder-Decoder-based LLM(like T5), we use the query and the prefix text as the output. Fig. 4. The overviewe of BLIP-2‚Äôs framework\nThe work shown above is Copyrighted to Salesforce Research.\nThis queries set contains ‚Äúalmost‚Äù all the imformation from the image, but make it looks like a text token. Since the authors want to align between the two pretrained models, it proposed this Q-former as the hidden process of the alignment. This idea is not new, as it is similar to the VQ-VAE[15]. The VQ-VAE uses a discrete latent variables to replace the continuous latent variables, where they called the trained discrete latent space as the ‚Äúcodebook‚Äù. This process of the generation can be seen as a retrieval process where using the activated code as the decoder‚Äôs input. the query in BLIP2 also can be seen as the ‚Äúcodebook‚Äù for transformer. Instead of learning the representation from the scratch, the only request for the queries learning is to compress the image embedding into a lower dimension space where can be more easily to align with the text embedding, reduce the redundency from the original image embedding, and lower the computational complexity.\nThe BLIP2 shows a possible way that instead of training from the scratch, the ‚Äúalign and fuse‚Äù can also solve the multimodal problem You may also check MiniGPT4[1] and MiniGPT5[2] .\nEmpirical Research Reference D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, MiniGPT-4: Enhancing vision-language understanding with advanced large language models, 2023. [Online]. Available: https://arxiv.org/abs/2304.10592 K. Zheng, X. He, and X. Wang, MiniGPT-5: Interleaved vision-and-language generation via generative vokens, 2023. [Online]. Available: https://arxiv.org/abs/2310.02239 T. Baltru≈°aitis, C. Ahuja, and L. Morency, Multimodal Machine Learning: A Survey and Taxonomy, 2017. [Online]. Available: http://arxiv.org/abs/1705.09406 [Accessed: Feb. 8, 2024]. A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning Transferable Visual Models From Natural Language Supervision, . J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. Hoi, Align before fuse: Vision and language representation learning with momentum distillation, Advances in neural information processing systems, vol. 34, pp. 9694‚Äì9705, 2021. A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents, 2022. [Online]. Available: http://arxiv.org/abs/2204.06125 [Accessed: Feb. 8, 2024]. H. Bao, W. Wang, L. Dong, Q. Liu, O. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, Advances in Neural Information Processing Systems, vol. 35, pp. 32897‚Äì32912, 2022. J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: A visual language model for few-shot learning, Advances in Neural Information Processing Systems, vol. 35, pp. 23716‚Äì23736, 2022. J. Li, D. Li, C. Xiong, and S. Hoi, BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, . A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. J. Devlin, M. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, arXiv preprint arXiv:2301.12597, 2023. W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, InstructBLIP: Towards general-purpose vision-language models with instruction tuning, 2023. [Online]. Available: https://arxiv.org/abs/2305.06500 A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, ≈Å. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. A. Oord, O. Vinyals, and K. Kavukcuoglu, Neural discrete representation learning, 2018. [Online]. Available: https://arxiv.org/abs/1711.00937 For more information, pleace check Spectrogram and Mel Spectrogram¬†‚Ü©Ô∏é\nWe would talk about the ‚Äúdownstream‚Äù research in the ‚ÄúEmpirical Research‚Äù section.¬†‚Ü©Ô∏é\nRemember that CLIP has a 400M dataset for the training process? They did not open-source it, leads they from OpenAI to ‚ÄúCloseAI‚Äù (of course, it is a business decision, but it may not be good for the research community).¬†‚Ü©Ô∏é\n",
  "wordCount" : "2666",
  "inLanguage": "en",
  "datePublished": "2024-02-08T00:15:00Z",
  "dateModified": "2024-02-11T19:37:16Z",
  "author":{
    "@type": "Person",
    "name": "Junjie(Terence) LIU"
  },
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://blog.cklau.cc/post/scientia/multi-modality/"
  },
  "publisher": {
    "@type": "Organization",
    "name": "ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú",
    "logo": {
      "@type": "ImageObject",
      "url": "https://blog.cklau.cc/favicon.ico"
    }
  }
}
</script>
    <link rel="icon" type="image/png" href="/images/icon.png" sizes="16x16">

<link rel="apple-touch-icon" href="/images/icon.png">

<link rel="manifest" href="/images/icon.png">
    
    
        <link href="https://cdn.bootcdn.net/ajax/libs/KaTeX/0.16.8/katex.min.css" rel="stylesheet">
<script src="https://cdn.bootcdn.net/ajax/libs/KaTeX/0.16.8/katex.js"></script>
<script src="https://cdn.bootcdn.net/ajax/libs/KaTeX/0.16.8/contrib/auto-render.js"></script>
<script src="https://cdn.bootcdn.net/ajax/libs/KaTeX/0.16.8/contrib/copy-tex.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true },
                { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                { left: "\\begin{align}", right: "\\end{align}", display: true },
                { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
                { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                { left: "\\[", right: "\\]", display: true }
            ],
            
            throwOnError: false,
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>
    

    
    
    
    <link rel="stylesheet" href="/css/main.min.ba43e469675cd3c20d9505dd369cdb9b90f180e533133bba2d49075370a64062.css" integrity="sha256-ukPkaWdc08INlQXdNpzbm5DxgOUzEzu6LUkHU3CmQGI=" crossorigin="anonymous" media="screen" />
    


    
    <link rel="stylesheet" href="/scss/highlight/github-dark.min.min.66034289ee9a113219a2c4aae0a8bd2095ab255c832a42efcf5863f10814e7a1.css" />

    
    <script src="/js/highlight.min.min.894ca9c68afab956438c4926a0dc7f5293e04e08595bd27abdb123e94801f684.js"></script>
    <script>hljs.highlightAll();</script>

    <script>(()=>{var t=window.matchMedia&&window.matchMedia("(prefers-color-scheme: dark)").matches,e=localStorage.getItem("theme");t&&e===null&&(localStorage.setItem("theme","dark"),document.documentElement.setAttribute("data-dark-mode","")),t&&e==="dark"&&document.documentElement.setAttribute("data-dark-mode",""),e==="dark"&&document.documentElement.setAttribute("data-dark-mode","")})()</script>
    
    <link href="https://cdn.bootcdn.net/ajax/libs/font-awesome/6.4.2/css/all.min.css" rel="stylesheet">
</head>
<body><nav class="navigation">
    <section class="container">
        <a class="navigation-brand" href="/">
            CKLAU&#39;s WEBSITE
        </a>
        <input type="checkbox" id="menu-toggle" />
        <label class="menu-button float-right" for="menu-toggle">
            <span></span><span></span><span></span>
        </label>
        
        <ul class="navigation-list" id="navigation-list">
            
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/projects">Project</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/post">Post</a>
            </li>
            
            <li class="navigation-item navigation-menu">
                <a class="navigation-link" href="/friends">Friends</a>
            </li>
            
            

            <li class="navigation-item menu-separator">
                <span>|</span>
            </li>

            
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://terenceliu98.github.io/index.xml"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
            </li>
            
            <li class="navigation-item navigation-social">
                <a class="navigation-link" href="https://github.com/TerenceLiu98"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-github"><path d="M9 19c-5 1.5-5-2.5-7-3m14 6v-3.87a3.37 3.37 0 0 0-.94-2.61c3.14-.35 6.44-1.54 6.44-7A5.44 5.44 0 0 0 20 4.77 5.07 5.07 0 0 0 19.91 1S18.73.65 16 2.48a13.38 13.38 0 0 0-7 0C6.27.65 5.09 1 5.09 1A5.07 5.07 0 0 0 5 4.77a5.44 5.44 0 0 0-1.5 3.78c0 5.42 3.3 6.61 6.44 7A3.37 3.37 0 0 0 9 18.13V22"></path></svg></a>
            </li>
            
            

            <li class="navigation-item navigation-dark">
                <button id="mode" type="button" aria-label="toggle user light or dark theme">
                    <span class="toggle-dark"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-moon"><path d="M21 12.79A9 9 0 1 1 11.21 3 7 7 0 0 0 21 12.79z"></path></svg></span>
                    <span class="toggle-light"><svg xmlns="http://www.w3.org/2000/svg" width="20" height="20" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-sun"><circle cx="12" cy="12" r="5"></circle><line x1="12" y1="1" x2="12" y2="3"></line><line x1="12" y1="21" x2="12" y2="23"></line><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"></line><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"></line><line x1="1" y1="12" x2="3" y2="12"></line><line x1="21" y1="12" x2="23" y2="12"></line><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"></line><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"></line></svg></span>
                </button>
            </li>

            
            
            
            
            
            
            
            <li class="navigation-item navigation-language">
                <a href="https://blog.cklau.cc/zh/">‰∏≠</a>
            </li>
            
            
            
            
        </ul>
        
    </section>
</nav>
<main class="wrapper">
        <div id="content">
<article class="blog-single">
  <header class="blog-title">
    <h1>üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image</h1>
  </header>

  <p>
  <small>
    
    
    
    <u># <a href="https://blog.cklau.cc/tags/artificial-intelligence/">Artificial Intelligence</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/text-image/">text-image</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/multi-modality/">multi-modality</a></u>
    
    <u># <a href="https://blog.cklau.cc/tags/long-read/">long-read</a></u>
    
  </small>
  <br>
  <small>
    Published On: <u>February 8, 2024</u> (Last updated on: <u>February 11, 2024)</u> <br>
    2666 words&nbsp;¬∑ 13 min</small>
</p>



  <div class="blog-toc">
    
        <details>
	<summary>Table of Content</summary>
	<nav id="TableOfContents">
  <ul>
    <li><a href="#before">Before</a></li>
    <li><a href="#clip-learning-transferable-visual-models-from-natural-language-supervision">CLIP: Learning Transferable Visual Models From Natural Language Supervision</a>
      <ul>
        <li><a href="#implementation">Implementation</a></li>
        <li><a href="#applications">Applications</a></li>
        <li><a href="#full-text">Full Text</a></li>
      </ul>
    </li>
    <li><a href="#blip-bootstrapping-language-image-pre-training-for-unified-vision-language-understanding-and-generation">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</a>
      <ul>
        <li><a href="#implementation-1">Implementation</a></li>
        <li><a href="#applications-1">Applications</a></li>
        <li><a href="#full-text-1">Full Text</a></li>
        <li><a href="#related-researches">Related Researches</a></li>
      </ul>
    </li>
    <li><a href="#empirical-research">Empirical Research</a></li>
    <li><a href="#reference">Reference</a></li>
  </ul>
</nav>
</details>
    
  </div>

  <section class="blog-content"><h2 id="before">Before</h2>
<p>For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.</p>
<p>However, different data mode has different representation, especially for the machine, i.e., different mode of data has different representation. Some of the modality can be approximated by the others, e.g. audio can be transformed into images via the Fourier Transformation <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Usually, the multimodal learning includes these five aspects [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-3" title="T. Baltru≈°aitis, C. Ahuja, and L. Morency, Multimodal Machine Learning: A Survey and Taxonomy, 2017. [Online]. Available: http://arxiv.org/abs/1705.09406 [Accessed: Feb. 8, 2024]. ">3</a>]:</p>
<ol>
<li>Representation: It is important for us to know how to represent each unimodality or multimodality</li>
<li>Translation: This is the mapping from one data mode to another, e.g. using spectrogram to transform audio to image</li>
<li>Alignment or Registration: This is the direct relation between elements from two or more different modalities. If they are in the same modality, we would call it <em>registration</em>.</li>
<li>Fusion: It is to join information from two or more modalities to perform a downstream task.</li>
<li>Co-learning: Transforming knowledge from different modalities</li>
</ol>
<p>For this series, we are not shaped by the five main aspects, instead, we would organized by modalities. In this post, we are focusing on the Text and Image, with such amounts of researchs and works, First, we would introduce the big short from OpenAI: CLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-4" title="A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning Transferable Visual Models From Natural Language Supervision, . ">4</a>] as it can be seen as one of the milstones of multimodality studies. After the CLIP models, there are multiple different kinds of multimodal-based models, e.g. ALBEF[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-5" title="J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. Hoi, Align before fuse: Vision and language representation learning with momentum distillation, Advances in neural information processing systems, vol. 34, pp. 9694‚Äì9705, 2021. ">5</a>], BLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-6" title="A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents, 2022. [Online]. Available: http://arxiv.org/abs/2204.06125 [Accessed: Feb. 8, 2024]. ">6</a>], VLMO [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-7" title="H. Bao, W. Wang, L. Dong, Q. Liu, O. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, Advances in Neural Information Processing Systems, vol. 35, pp. 32897‚Äì32912, 2022. ">7</a>], and Flamingo [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-8" title="J. Alayrac, J. Donahue, P. Luc, A. Miech, I. Barr, Y. Hasson, K. Lenc, A. Mensch, K. Millican, M. Reynolds, et al., Flamingo: A visual language model for few-shot learning, Advances in Neural Information Processing Systems, vol. 35, pp. 23716‚Äì23736, 2022. ">8</a>]. I chose BLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-9" title="J. Li, D. Li, C. Xiong, and S. Hoi, BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, . ">9</a>] as it can be seen as the supplimentary of the CLIP model, as it not only consider the representation, but also the understanding of the multimodal data.</p>
<h2 id="clip-learning-transferable-visual-models-from-natural-language-supervision">CLIP: Learning Transferable Visual Models From Natural Language Supervision</h2>
<p>CLIP&rsquo;s key contribution is its ability to map text and image into a shared embedding space based on the seperated text/image encoder. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier. Not meaning that it can directly used in these tasks, but providing a idea that how to do the multimodal representation.</p>
<div class="semwebdemostyle" style="width: 100%;"><figure class="figure_box txt_center">
      <div><a href="https://blog.cklau.cc" rel="">
          <picture><source srcset="https://blog.cklau.cc/image_1881611846882928792_hucb52bd991ea5ad84c0398ddcc4540233_0_2330x864_resize_q75_h2_box_3.webp" type="image/webp" /><img src="https://blog.cklau.cc/image_1881611846882928792.png" alt="ALT" type="image/png" style="max-width: 100%;" loading="lazy" decoding="async" />
          </picture>
          </a></div>
      <figcaption class="attribution_caption txt_center">
        
        <p><i>Fig. 1. The architecture of CLIP model</i></p><p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
            <small>The work shown above is Copyrighted to <a href="https://openai.com" rel="dct:creator ">OPENAI</a>.</small></p></figcaption>
    </figure></div><script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ImageObject",
  "@id": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/64cc5e48-d9a3-46aa-95b2-b6e0e9467fb3/image.png#image","caption": "Fig. 1. The architecture of CLIP model","representativeOfPage": false,
  "contentUrl": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/64cc5e48-d9a3-46aa-95b2-b6e0e9467fb3/image.png",
  "about": "Fig. 1. The architecture of CLIP model",
  "creativeWorkStatus": "Published",
  "isPartOf": [
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#article"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webpage"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webcontent"
    }
  ],"description": "Fig. 1. The architecture of CLIP model",
  "name": "Fig. 1. The architecture of CLIP model",
  "url": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/64cc5e48-d9a3-46aa-95b2-b6e0e9467fb3/image.png"
}
</script>

<p><em>Fig. 1.</em> shows the approach of CLIP model. Text and image are encoded by two different encoders $f_{\theta_i}$ and $g_{\theta_t}$, let $\mathbf{x} \in \mathbb{R}^{N \times H \times W \times C}$ as one batch of image, $\mathbf{y} \in \mathbb{R}^{N \times S}$ as one batch of text data,  the embedding of $\mathbf{x}$ and $\mathbf{y}$ then can be denoted as:</p>
<p>$$\begin{aligned} \mathbf{f} &amp;= f_{\theta_i}(\mathbf{x}) \in \mathbf{R}^{N \times D_i} \Rightarrow \mathbf{f}^e = L_i(\mathbf{f})  \cr  \mathbf{g} &amp;= g_{\theta_t}(\mathbf{y}) \in \mathbf{R}^{N \times D_t} \Rightarrow \mathbf{g}^e = L_t(\mathbf{g}) \end{aligned}$$</p>
<p>where $D_i$ and $D_t$ are the dimension of the image and text embedding, respectively. linear projectors $L_i$ and $L_t$ are used for mapping two embedding into the same dimension. The dot product between the image and text embedding is used to calculate the similarity between the text and image, i.e., $\mathcal{F} = \mathbf{f} \cdot \mathbf{g}$, where $\mathcal{F} \in \mathbb{R}^{N \times N}$ is the similarity matrix and the $\mathcal{F} \circ I_N$ is the positive sample set <span class="sidenote-number">
<small class="sidenote">
where $I_N$ is the identity matrix, and $\mathcal{F} \circ I_N$ is the matrix&rsquo;s diagonal
</small>
</span>, and the others are the negative samples (In total, there are $N^2 - N$ negatives samples). The pseudocode of the CLIP model is shown in Figure 2 (original papers). The idea of CLIP model is relative naive, as it is a classic negative sampling method called batch negative sampling. However, intuitively, it is hard for us to do the model explanation, i.e., why does it work? Still, we could explan that the corresponding text and image are sharing the same ontology, but this kind of explanation is not grounded. More or less, the main contribution is that they create a huge dataset with the text and image pairs, including 400 million (image, text) pairs collected form of a variety of publicly available sources on the Internet. With this dataset, they don&rsquo;t even need the pretrained encoder as the encoder can be trained simultaneously with the downstream alignment.</p>
<h3 id="implementation">Implementation</h3>
<p>Here is the simple implementation of CLIP:</p>
<details>
  <summary>Implementation of Encoders</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch 
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoConfig, AutoModel, AutoModelForImageClassification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TextEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;FacebookAI/roberta-base&#34;</span>:str, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool):
</span></span><span style="display:flex;"><span>        super(TextEncoder)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> {<span style="color:#66d9ef">True</span>: AutoModel<span style="color:#f92672">.</span>from_pretrained(model_name), 
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">False</span>: AutoModel<span style="color:#f92672">.</span>from_config(AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name))}
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out_dim <span style="color:#f92672">=</span> AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name)<span style="color:#f92672">.</span>hidden_size <span style="color:#75715e"># 768</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>freeze <span style="color:#f92672">=</span> freeze
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> freeze:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> name ,param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>                param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>freeze:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>normalize(torch<span style="color:#f92672">.</span>mean(self<span style="color:#f92672">.</span>model(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>last_hidden_state, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ImageEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;google/vit-base-patch16-224&#34;</span>:str, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool):
</span></span><span style="display:flex;"><span>        super(TextEncoder)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> {<span style="color:#66d9ef">True</span>: AutoModelForImageClassification<span style="color:#f92672">.</span>from_pretrained(model_name), 
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">False</span>: AutoModelForImageClassification<span style="color:#f92672">.</span>from_config(AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name))}
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span> <span style="color:#75715e"># ViT is trained with ImageNet</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>freeze <span style="color:#f92672">=</span> freeze
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> freeze:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> name ,param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>                param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>logits
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(feature, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature
</span></span></code></pre></div></details>
<details>
  <summary>Implementation of Linear Projection</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LinearProject</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> __init__(self, in_features, out_features):
</span></span><span style="display:flex;"><span>          super(LinearProject)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>          self<span style="color:#f92672">.</span>projection <span style="color:#f92672">==</span> nn<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>              nn<span style="color:#f92672">.</span>Linear(in_features, out_features), nn<span style="color:#f92672">.</span>GELU(), nn<span style="color:#f92672">.</span>LayerNorm(out_features)
</span></span><span style="display:flex;"><span>          ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>          output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>projection(x)
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div></details>
<details>
  <summary>Implementation of CLIP Model</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CLIPModel</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;TEXT&#34;</span>:<span style="color:#e6db74">&#34;FacebookAI/roberta-base&#34;</span>, <span style="color:#e6db74">&#34;IMG&#34;</span>:<span style="color:#e6db74">&#34;google/vit-base-patch16-224&#34;</span>}:dict, 
</span></span><span style="display:flex;"><span>                  device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>:int):
</span></span><span style="display:flex;"><span>        super(CLIPModel)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>enc_t <span style="color:#f92672">=</span> TextEncoder(model_name<span style="color:#f92672">=</span>model_name[<span style="color:#e6db74">&#34;TEXT&#34;</span>], device<span style="color:#f92672">=</span>device, freeze<span style="color:#f92672">=</span>freeze)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>enc_i <span style="color:#f92672">=</span> ImageEncoder(model_name<span style="color:#f92672">=</span>model_name[<span style="color:#e6db74">&#34;IMG&#34;</span>], device<span style="color:#f92672">=</span>device, freeze<span style="color:#f92672">=</span>freeze)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>proj_t <span style="color:#f92672">=</span> LinearProject(self<span style="color:#f92672">.</span>enc_t<span style="color:#f92672">.</span>out_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>proj_i <span style="color:#f92672">=</span> LinearProject(self<span style="color:#f92672">.</span>enc_i<span style="color:#f92672">.</span>out_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logit_scale <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones([]))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>init_parameters()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_parameters</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># turn temperature into a learnable parameter</span>
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(self<span style="color:#f92672">.</span>logit_scale, np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">0.07</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">criterion</span>(self, text, image):
</span></span><span style="display:flex;"><span>        CE <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(text<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], device<span style="color:#f92672">=</span>str(text<span style="color:#f92672">.</span>device), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)
</span></span><span style="display:flex;"><span>        logits_t <span style="color:#f92672">=</span> text <span style="color:#f92672">@</span> image<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>logit_scale<span style="color:#f92672">.</span>exp()
</span></span><span style="display:flex;"><span>        logits_i <span style="color:#f92672">=</span> image <span style="color:#f92672">@</span> text<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>logit_scale<span style="color:#f92672">.</span>exp()
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> (CE(logits_t, labels) <span style="color:#f92672">+</span> CE(logits_i, labels)) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, text, image):
</span></span><span style="display:flex;"><span>        feature_t, feature_i <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proj_t(self<span style="color:#f92672">.</span>enc_t(text)), self<span style="color:#f92672">.</span>proj_i(self<span style="color:#f92672">.</span>enc_i(image))
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterion(feature_t, feature_i)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature_t, feature_i, self<span style="color:#f92672">.</span>logit_scale, loss
</span></span></code></pre></div></details>
<h3 id="applications">Applications</h3>
<p>The first application of CLIP is classification. Since the CLIP model is relatively similar to the reitrival model, it can be easily implemented into the classification with zero-shot learning. The zero-shot learning is a task that the model can classify the unseen classes without any training data. See the second part of Figure 1, where for a given image, the model can classify the similarity between the given image and the prompted text, by calculating the similarity of image and each given sentence we could get the classification, vice versa. This can be seen as classification, or retrieval.</p>
<p>The second application is the generation. The CLIP model can be used to generate the image from the given text. Although it cannot generate the image directly since it does not have any decoder, however, the CLIP can be seen as the backbone and provide the embedding for image or text generation. After the CLIP came out, OpenAI also released the DALL¬∑E 2, where they provide a model called unCLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-6" title="A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents, 2022. [Online]. Available: http://arxiv.org/abs/2204.06125 [Accessed: Feb. 8, 2024]. ">6</a>], it is a text-to-image generation model<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<h3 id="full-text">Full Text</h3>
<details>
  <summary>Full Text</summary>
  <iframe src="https://share.cklau.cc/generic/web/viewer_readonly.html?file=webdrive/QZJJSWF2/Radford%20et%20al.%20-%20Learning%20Transferable%20Visual%20Models%20From%20Natural%20L.pdf" width="100%" height="1000" frameborder="0" allowfullscreen></iframe>
</details>
<h2 id="blip-bootstrapping-language-image-pre-training-for-unified-vision-language-understanding-and-generation">BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation</h2>
<p>Unlike the CLIP&rsquo;s siamese-shaped model, BLIP is a multimodal architecture that seperate the representation and generation into a two stage process. In the first stage, they generate the text embedding and image embedding, and for the second stage, they use the decoder that generates an image conditioned on the image embedding. This architecture is proposed based on these two research questions:</p>
<ol>
<li>Most existing pre-trained models only perform well on comprehension-based tasks or generation-based tasks, and few models can <strong>do both</strong>.</li>
<li>Most existing pre-trained models extend the dataset using noisy image-text pairs collected from the network in order to improve performance. Although this improves performance, it is obvious that this <strong>noisy supervision</strong> signal is definitely not optimal<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</li>
</ol>
<p>Based on these two research gap, they proposed the Multimodel mixture of Encoder-Decoder(MED) architecture, shown in Fig. 2.</p>
<div class="semwebdemostyle" style="width: 100%;"><figure class="figure_box txt_center">
      <div><a href="https://blog.cklau.cc" rel="">
          <picture><source srcset="https://blog.cklau.cc/image_16529421465121511918_hube63236cc42f7292387d716841c71cf6_0_2596x1086_resize_q75_h2_box_3.webp" type="image/webp" /><img src="https://blog.cklau.cc/image_16529421465121511918.png" alt="ALT" type="image/png" style="max-width: 100%;" loading="lazy" decoding="async" />
          </picture>
          </a></div>
      <figcaption class="attribution_caption txt_center">
        
        <p><i>Fig. 2. The Architecture of &lsquo;Multimodel mixture of Encoder-Decoder&rsquo; (MED) architecture</i></p><p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
            <small>The work shown above is Copyrighted to <a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/" rel="dct:creator ">Salesforce Research</a>.</small></p></figcaption>
    </figure></div><script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ImageObject",
  "@id": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/093cd5d4-5ce9-486f-87ed-5036f208a9f7/image.png?#image","caption": "Fig. 2. The Architecture of 'Multimodel mixture of Encoder-Decoder' (MED) architecture","representativeOfPage": false,
  "contentUrl": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/093cd5d4-5ce9-486f-87ed-5036f208a9f7/image.png?",
  "about": "Fig. 2. The Architecture of 'Multimodel mixture of Encoder-Decoder' (MED) architecture",
  "creativeWorkStatus": "Published",
  "isPartOf": [
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#article"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webpage"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webcontent"
    }
  ],"description": "Fig. 2. The Architecture of 'Multimodel mixture of Encoder-Decoder' (MED) architecture",
  "name": "Fig. 2. The Architecture of 'Multimodel mixture of Encoder-Decoder' (MED) architecture",
  "url": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/093cd5d4-5ce9-486f-87ed-5036f208a9f7/image.png?"
}
</script>

<p>The three tasks are proposed for the training:</p>
<ol>
<li>Image-text contrastive (ITC) learning: on the very left side of <em>Fig. 2.</em> is a Image Encoder, where it adapts the ViT[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-10" title="A. Dosovitskiy, L. Beyer, A. Kolesnikov, D. Weissenborn, X. Zhai, T. Unterthiner, M. Dehghani, M. Minderer, G. Heigold, S. Gelly, et al., An image is worth 16x16 words: Transformers for image recognition at scale, arXiv preprint arXiv:2010.11929, 2020. ">10</a>] for the image representation. The second part is the Text Encoder, where it adapts the BERT[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-11" title="J. Devlin, M. Chang, K. Lee, and K. Toutanova, Bert: Pre-training of deep bidirectional transformers for language understanding, arXiv preprint arXiv:1810.04805, 2018. ">11</a>] for the text representation. The ITC learning is to maximize the similarity between the image and text embedding, and minimize the similarity between the negative samples. The idea of the ITC is the same as CLIP where using the contrastive learning to align the text and image embedding.</li>
<li>Image-text matching (ITM) learning: The third modules is Image-graouded Text Decoder, where it is a transformer-based decoder with cross-attention layer to include the image embedding, and the mixed embedding are used for the ITM learning to check whether the text and image are aligned.</li>
<li>Language modeling (LM) learning: The last part is the Image-grounded text decoder. This decoder uses causal self-attention to replace the self-attetion layer in the BERT, and the image embedding also injected with the cross attention block for the text generation.</li>
</ol>
<p>To leverage the multi-task learning process, all the blocks with the same color are shareing the same parameters. This MED architecture shares some common modules with the ALBEF[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-5" title="J. Li, R. Selvaraju, A. Gotmare, S. Joty, C. Xiong, and S. Hoi, Align before fuse: Vision and language representation learning with momentum distillation, Advances in neural information processing systems, vol. 34, pp. 9694‚Äì9705, 2021. ">5</a>] and VLMO [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-7" title="H. Bao, W. Wang, L. Dong, Q. Liu, O. Mohammed, K. Aggarwal, S. Som, S. Piao, and F. Wei, Vlmo: Unified vision-language pre-training with mixture-of-modality-experts, Advances in Neural Information Processing Systems, vol. 35, pp. 32897‚Äì32912, 2022. ">7</a>] where the ITC is adapted from the CLIP while the ITM can be seen as the refinement ore the constrastive learning, as it take the hardest negative samples (from the ITC) as the negative samples in ITM; and the LM is for the text generation, which is the same as the VLMO.</p>
<div class="semwebdemostyle" style="width: 100%;"><figure class="figure_box txt_center">
      <div><a href="https://blog.cklau.cc" rel="">
          <picture><source srcset="https://blog.cklau.cc/image_911893906670887453_hu5ede98d7bdc2ebc0cf9725c421ebad41_0_2710x912_resize_q75_h2_box_3.webp" type="image/webp" /><img src="https://blog.cklau.cc/image_911893906670887453.png" alt="ALT" type="image/png" style="max-width: 100%;" loading="lazy" decoding="async" />
          </picture>
          </a></div>
      <figcaption class="attribution_caption txt_center">
        
        <p><i>Fig. 3. The CapFilt module for solving the noisy supervision signal</i></p><p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
            <small>The work shown above is Copyrighted to <a href="https://blog.salesforceairesearch.com/blip-bootstrapping-language-image-pretraining/" rel="dct:creator ">Salesforce Research</a>.</small></p></figcaption>
    </figure></div><script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ImageObject",
  "@id": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/86de6bd9-f8c6-427f-bccb-f9fed8edb919/image.png#image","caption": "Fig. 3. The CapFilt module for solving the noisy supervision signal","representativeOfPage": false,
  "contentUrl": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/86de6bd9-f8c6-427f-bccb-f9fed8edb919/image.png",
  "about": "Fig. 3. The CapFilt module for solving the noisy supervision signal",
  "creativeWorkStatus": "Published",
  "isPartOf": [
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#article"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webpage"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webcontent"
    }
  ],"description": "Fig. 3. The CapFilt module for solving the noisy supervision signal",
  "name": "Fig. 3. The CapFilt module for solving the noisy supervision signal",
  "url": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/86de6bd9-f8c6-427f-bccb-f9fed8edb919/image.png"
}
</script>

<p>The second contribution of the paper is the pipeline of eliminating the noisy superivion signal, shown in <em>Fig. 3</em>. where the Image-grounded Text Encoder and Image-grounded Text Decoder are used as <strong>Filter</strong> and <strong>Captioner</strong>. The Filter is to remove the noisy image-text pairs while the Captioner is used for generating synthetic captions given a web images. The process of Capfilt is as follows: 1) The training set $D$ is formed with web image-text pairs and clean image-text pairs, denoted as $D_w = (I_w, T_w)$ and $D_h = (I_h, T_h)$, and $D = D_w + D_h$. 2) Fine-tune the two encoder with $D_h$. 3) The <strong>Captioner</strong> generates caption with $D_w$&rsquo;s images. 4) The <strong>Filter</strong> is used to remove the noisy text <span class="sidenote-number">
<small class="sidenote">
not matter the given text is scraped from the web or synthesis from the captioner
</small>
</span>. Thus, the model-based data cleansing is used to remove the noisy supervision signal and the cleansed data is used for the training of the next MED<span class="sidenote-number">
<small class="sidenote">
so call <strong>bootstraping</strong>
</small>
</span>.</p>
<h3 id="implementation-1">Implementation</h3>
<p>You may check <a href="https://github.com/salesforce/BLIP">BLIP-official Code from GitHub</a>. You may see that <a href="https://github.com/salesforce/BLIP/blob/3a29b7410476bf5f2ba0955827390eb6ea1f4f9d/models/med.py#L97"><code>med.py</code>#97</a>, they define the function <code>BertSelfAttention</code> to include the cross-attention layer into the BERT&rsquo;s encoder.</p>
<h3 id="applications-1">Applications</h3>
<p>The BLIP model is used for multiple downstream tasks: Image-text retrieval (with Image Encoder and Text Encoder), Image Captioning(with Image-grounded Text Decoder), Visual Question Answering(with Image Encoder), Natural Language Visual Reasoning(with Image Encoder and Text Encoder), and Visual Dialog(with Image Encoder and Image-grounded text Encoder). The detailed can be seen in the original paper.</p>
<h3 id="full-text-1">Full Text</h3>
<details>
  <summary>Full Text</summary>
  <iframe src="https://share.cklau.cc/generic/web/viewer_readonly.html?file=webdrive/H479RWE4/Li%20et%20al.%20-%20BLIP%20Bootstrapping%20Language-Image%20Pre-training%20fo.pdf" width="100%" height="1000" frameborder="0" allowfullscreen></iframe>
</details>
<h3 id="related-researches">Related Researches</h3>
<p>BLIP&rsquo;s Group allow continue their work on the multimodal representation learning, they proposed BLIP2[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-12" title="J. Li, D. Li, S. Savarese, and S. Hoi, Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models, arXiv preprint arXiv:2301.12597, 2023. ">12</a>] and InstructBLIP[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-13" title="W. Dai, J. Li, D. Li, A. Tiong, J. Zhao, W. Wang, B. Li, P. Fung, and S. Hoi, InstructBLIP: Towards general-purpose vision-language models with instruction tuning, 2023. [Online]. Available: https://arxiv.org/abs/2305.06500  ">13</a>]:</p>
<h4 id="blip2-bootstrapping-language-image-pre-training-with-frozen-image-encoders-and-large-language-models">BLIP2: Bootstrapping Language-Image Pre-training with Frozen Image Encoders and Large Language Models</h4>
<p>BLIP2 is a successor of BLIP, the overall framework is shown in <em>Fig. 4.</em>, still, it contains three training tasks: ITC, ITM, and Text generation. Two-stage process is proposed: the first stage is visuion-lanaguge representation learning and the second stage is the vision-language generative learning:</p>
<ol>
<li>representation learning: the Q-former contains three modules, where at the left is the pretrained image encoder(the parameter is frozen), the middle one is learnable query encoder and the right one is the text decoder. The query encoder and text decoder are adapted from the classic transformer architecture [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-14" title="A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. Gomez, ≈Å. Kaiser, and I. Polosukhin, Attention is all you need, Advances in neural information processing systems, vol. 30, 2017. ">14</a>], the queries is a learnable vector set that is used to query the extracted visual representation most relevant to the given text<span class="sidenote-number">
<small class="sidenote">
Since the self-attention layer is shared, which may insure the coherence between query module and the text module.
</small>
</span>, and the text decoder is used for the alignment.</li>
<li>generative learning: the downsteam module is the pretrained LLM model(the parameter is frozen). After the stage 1, the learned queries are the visual represenataions, and in this stage, a linear projection is needed to map the queries into the LLM(also, we need to reshape queries&rsquo; dimension to fit the LLM).
<ol>
<li>For the Decoder-based LLM(like GPT), we use query as the only input</li>
<li>For the Encoder-Decoder-based LLM(like T5), we use the query and the prefix text as the output.</li>
</ol>
</li>
</ol>
<div class="semwebdemostyle" style="width: 100%;"><figure class="figure_box txt_center">
      <div><a href="https://blog.cklau.cc" rel="">
          <picture><source srcset="https://blog.cklau.cc/BLIP2_12918848847556169132_hu369eeb32a116bcd91912a9adffbca855_0_2334x933_resize_q75_h2_box_3.webp" type="image/webp" /><img src="https://blog.cklau.cc/BLIP2_12918848847556169132.png" alt="ALT" type="image/png" style="max-width: 100%;" loading="lazy" decoding="async" />
          </picture>
          </a></div>
      <figcaption class="attribution_caption txt_center">
        
        <p><i>Fig. 4. The overviewe of BLIP-2&rsquo;s framework</i></p><p xmlns:dct="http://purl.org/dc/terms/" xmlns:vcard="http://www.w3.org/2001/vcard-rdf/3.0#">
            <small>The work shown above is Copyrighted to <a href="https://blog.salesforceairesearch.com/blip-2/" rel="dct:creator ">Salesforce Research</a>.</small></p></figcaption>
    </figure></div><script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "ImageObject",
  "@id": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/a10471cb-7ea5-4133-936e-259d94da0b53/BLIP2.png#image","caption": "Fig. 4. The overviewe of BLIP-2's framework","representativeOfPage": false,
  "contentUrl": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/a10471cb-7ea5-4133-936e-259d94da0b53/BLIP2.png",
  "about": "Fig. 4. The overviewe of BLIP-2's framework",
  "creativeWorkStatus": "Published",
  "isPartOf": [
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#article"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webpage"
    },
    {
      "@id": "https://blog.cklau.cc/post/scientia/multi-modality/#webcontent"
    }
  ],"description": "Fig. 4. The overviewe of BLIP-2's framework",
  "name": "Fig. 4. The overviewe of BLIP-2's framework",
  "url": "https://s3.cklau.cc/outline/uploads/b6880a0c-d8fc-4d04-9621-1e308a59ebb4/a10471cb-7ea5-4133-936e-259d94da0b53/BLIP2.png"
}
</script>

<p>This queries set contains &ldquo;almost&rdquo; all the imformation from the image, but make it looks like a text token. Since the authors want to align between the two pretrained models, it proposed this Q-former as the hidden process of the alignment. This idea is not new, as it is similar to the VQ-VAE[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-15" title="A. Oord, O. Vinyals, and K. Kavukcuoglu, Neural discrete representation learning, 2018. [Online]. Available: https://arxiv.org/abs/1711.00937  ">15</a>]. The VQ-VAE uses a discrete latent variables to replace the continuous latent variables, where they called the trained discrete latent space as the &ldquo;codebook&rdquo;. This process of the generation can be seen as a retrieval process where using the activated code as the decoder&rsquo;s input.  the query in BLIP2 also can be seen as the &ldquo;codebook&rdquo; for transformer. Instead of learning the representation from the scratch, the only request for the queries learning is to compress the image embedding into a lower dimension space where can be more easily to align with the text embedding, reduce the redundency from the original image embedding, and lower the computational complexity.</p>
<p>The BLIP2 shows a possible way that instead of training from the scratch, the &ldquo;align and fuse&rdquo; can also solve the multimodal problem<span class="sidenote-number">
<small class="sidenote">
You may also check MiniGPT4[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-1" title="D. Zhu, J. Chen, X. Shen, X. Li, and M. Elhoseiny, MiniGPT-4: Enhancing vision-language understanding with advanced large language models, 2023. [Online]. Available: https://arxiv.org/abs/2304.10592  ">1</a>] and MiniGPT5[<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-2" title="K. Zheng, X. He, and X. Wang, MiniGPT-5: Interleaved vision-and-language generation via generative vokens, 2023. [Online]. Available: https://arxiv.org/abs/2310.02239  ">2</a>]
</small>
</span>.</p>
<h2 id="empirical-research">Empirical Research</h2>
<h2 id="reference">Reference</h2>
<ol class="hugo-simplecite-reference-list"><li class="hugo-simplecite-reference-list-item" id="bibreference-1">D. Zhu,&#32;J. Chen,&#32;X. Shen,&#32;X. Li, and&#32;M. Elhoseiny,&#32;<q>MiniGPT-4: Enhancing vision-language understanding with advanced large language models,</q>&#32;2023.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="https://arxiv.org/abs/2304.10592">https://arxiv.org/abs/2304.10592</a>&#32;
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-2">K. Zheng,&#32;X. He, and&#32;X. Wang,&#32;<q>MiniGPT-5: Interleaved vision-and-language generation via generative vokens,</q>&#32;2023.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="https://arxiv.org/abs/2310.02239">https://arxiv.org/abs/2310.02239</a>&#32;
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-3">T. Baltru≈°aitis,&#32;C. Ahuja, and&#32;L. Morency,&#32;<q>Multimodal Machine Learning: A Survey and Taxonomy,</q>&#32;2017.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="http://arxiv.org/abs/1705.09406">http://arxiv.org/abs/1705.09406</a>&#32;[Accessed:
Feb. 8, 2024].
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-4">A. Radford,&#32;J. Kim,&#32;C. Hallacy,&#32;A. Ramesh,&#32;G. Goh,&#32;S. Agarwal,&#32;G. Sastry,&#32;A. Askell,&#32;P. Mishkin,&#32;J. Clark,&#32;G. Krueger, and&#32;I. Sutskever,&#32;<q>Learning Transferable Visual Models From Natural Language Supervision,</q>&#32;.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-5">J. Li,&#32;R. Selvaraju,&#32;A. Gotmare,&#32;S. Joty,&#32;C. Xiong, and&#32;S. Hoi,&#32;<q>Align before fuse: Vision and language representation learning with momentum distillation,</q>&#32;<em>Advances in neural information processing systems</em>,&#32;vol. 34,&#32;pp. 9694‚Äì9705,&#32;2021.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-6">A. Ramesh,&#32;P. Dhariwal,&#32;A. Nichol,&#32;C. Chu, and&#32;M. Chen,&#32;<q>Hierarchical Text-Conditional Image Generation with CLIP Latents,</q>&#32;2022.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="http://arxiv.org/abs/2204.06125">http://arxiv.org/abs/2204.06125</a>&#32;[Accessed:
Feb. 8, 2024].
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-7">H. Bao,&#32;W. Wang,&#32;L. Dong,&#32;Q. Liu,&#32;O. Mohammed,&#32;K. Aggarwal,&#32;S. Som,&#32;S. Piao, and&#32;F. Wei,&#32;<q>Vlmo: Unified vision-language pre-training with mixture-of-modality-experts,</q>&#32;<em>Advances in Neural Information Processing Systems</em>,&#32;vol. 35,&#32;pp. 32897‚Äì32912,&#32;2022.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-8">J. Alayrac,&#32;J. Donahue,&#32;P. Luc,&#32;A. Miech,&#32;I. Barr,&#32;Y. Hasson,&#32;K. Lenc,&#32;A. Mensch,&#32;K. Millican,&#32;M. Reynolds,&#32;<em>et al.</em>,&#32;<q>Flamingo: A visual language model for few-shot learning,</q>&#32;<em>Advances in Neural Information Processing Systems</em>,&#32;vol. 35,&#32;pp. 23716‚Äì23736,&#32;2022.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-9">J. Li,&#32;D. Li,&#32;C. Xiong, and&#32;S. Hoi,&#32;<q>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation,</q>&#32;.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-10">A. Dosovitskiy,&#32;L. Beyer,&#32;A. Kolesnikov,&#32;D. Weissenborn,&#32;X. Zhai,&#32;T. Unterthiner,&#32;M. Dehghani,&#32;M. Minderer,&#32;G. Heigold,&#32;S. Gelly,&#32;<em>et al.</em>,&#32;<q>An image is worth 16x16 words: Transformers for image recognition at scale,</q>&#32;<em>arXiv preprint arXiv:2010.11929</em>,&#32;2020.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-11">J. Devlin,&#32;M. Chang,&#32;K. Lee, and&#32;K. Toutanova,&#32;<q>Bert: Pre-training of deep bidirectional transformers for language understanding,</q>&#32;<em>arXiv preprint arXiv:1810.04805</em>,&#32;2018.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-12">J. Li,&#32;D. Li,&#32;S. Savarese, and&#32;S. Hoi,&#32;<q>Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models,</q>&#32;<em>arXiv preprint arXiv:2301.12597</em>,&#32;2023.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-13">W. Dai,&#32;J. Li,&#32;D. Li,&#32;A. Tiong,&#32;J. Zhao,&#32;W. Wang,&#32;B. Li,&#32;P. Fung, and&#32;S. Hoi,&#32;<em>InstructBLIP: Towards general-purpose vision-language models with instruction tuning</em>, 2023.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="https://arxiv.org/abs/2305.06500">https://arxiv.org/abs/2305.06500</a>&#32;
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-14">A. Vaswani,&#32;N. Shazeer,&#32;N. Parmar,&#32;J. Uszkoreit,&#32;L. Jones,&#32;A. Gomez,&#32;≈Å. Kaiser, and&#32;I. Polosukhin,&#32;<q>Attention is all you need,</q>&#32;<em>Advances in neural information processing systems</em>,&#32;vol. 30,&#32;2017.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-15">A. Oord,&#32;O. Vinyals, and&#32;K. Kavukcuoglu,&#32;<q>Neural discrete representation learning,</q>&#32;2018.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="https://arxiv.org/abs/1711.00937">https://arxiv.org/abs/1711.00937</a>&#32;
</li></ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For more information, pleace check <a href="https://en.wikipedia.org/wiki/Spectrogram">Spectrogram</a> and <a href="https://ieeexplore.ieee.org/document/9859621">Mel Spectrogram</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>We would talk about the &ldquo;downstream&rdquo; research in the <a href="#empirical-research">&ldquo;Empirical Research&rdquo;</a> section.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Remember that CLIP has a 400M dataset for the training process? They did not open-source it, leads they from OpenAI to &ldquo;CloseAI&rdquo; (of course, it is a business decision, but it may not be good for the research community).&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</section>

  
  
  <div class="paginator">
    
    
    <a class="next" href="https://blog.cklau.cc/post/specification/"><span>ü§î A General Introduction of ML/DL Project Management and Knowledge Management - Project Management</span><span>&nbsp;&nbsp;&rarr;</span></a>
    
  </div>
  
  
  
  

<div class="related-resources">
  <h3>Related Resources</h3>
  
    
    
    
  
</div>



  
    
<div class="comments">
  <script>
      const getTheme = window.localStorage && window.localStorage.getItem("theme");
      let theme = getTheme === 'dark' ? 'github-dark' : 'github-light';
      let s = document.createElement('script');
      s.src = 'https://utteranc.es/client.js';
      s.setAttribute('repo', 'TerenceLiu98\/terenceliu98.github.io');
      s.setAttribute('issue-term', 'url');
      s.setAttribute('theme', theme);
      s.setAttribute('crossorigin', 'anonymous');
      s.setAttribute('async', '');
      document.querySelector('div.comments').innerHTML = '';
      document.querySelector('div.comments').appendChild(s);
  </script>
</div>

    
  

</article>

        </div>
      </main><footer class="footer">
  <p>&copy; 2024 <a href="https://blog.cklau.cc">ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</a> &nbsp;
    
        <span><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">Á≤§ICPÂ§á2022102668Âè∑</a></span>
    
Ô∏è  </p>
</footer>

<a href="#top" aria-label="go to top" title="Go to Top (Alt + G)" class="top-link" id="top-link" accesskey="g">
  <svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-arrow-up"><line x1="12" y1="19" x2="12" y2="5"></line><polyline points="5 12 12 5 19 12"></polyline></svg>
</a>

<script>
    var mybutton = document.getElementById("top-link");
    window.onscroll = function () {
        if (document.body.scrollTop > 800 || document.documentElement.scrollTop > 800) {
            mybutton.style.visibility = "visible";
            mybutton.style.opacity = "1";
        } else {
            mybutton.style.visibility = "hidden";
            mybutton.style.opacity = "0";
        }
    };
</script>

<script>
    document.querySelectorAll('pre > code').forEach((codeblock) => {
        const container = codeblock.parentNode.parentNode;

        const copybutton = document.createElement('button');
        copybutton.classList.add('copy-code');
        copybutton.innerHTML = 'Copy';

        function copyingDone() {
            copybutton.innerHTML = 'Copied';
            setTimeout(() => {
                copybutton.innerHTML = 'Copy';
            }, 2000);
        }

        copybutton.addEventListener('click', (cb) => {
            if ('clipboard' in navigator) {
                navigator.clipboard.writeText(codeblock.textContent);
                copyingDone();
                return;
            }

            const range = document.createRange();
            range.selectNodeContents(codeblock);
            const selection = window.getSelection();
            selection.removeAllRanges();
            selection.addRange(range);
            try {
                document.execCommand('copy');
                copyingDone();
            } catch (e) { };
            selection.removeRange(range);
        });
        codeblock.parentNode.appendChild(copybutton);
    });
</script></body><script src="https://cdnjs.cloudflare.com/ajax/libs/medium-zoom/1.0.6/medium-zoom.min.js" integrity="sha512-N9IJRoc3LaP3NDoiGkcPa4gG94kapGpaA5Zq9/Dr04uf5TbLFU5q0o8AbRhLKUUlp8QFS2u7S+Yti0U7QtuZvQ==" crossorigin="anonymous" referrerpolicy="no-referrer"></script>

  <script>
      const images = Array.from(document.querySelectorAll(".blog-content img"));
      images.forEach(img => {
          mediumZoom(img, {
              margin: 10,  
              scrollOffset: 40,  
              container: null,  
              template: null,  
              background: 'rgba(0, 0, 0, 0.5)'
          });
      });
  </script>

  
  <script src="/main.min.6bb26b69159420159c74dc9e097b06a578ed2b68c701466a91a44a9632d851bd0af167a1b30012387b4c512b48ad9ad4d3394e04d77ae38d57e1920fe4ed34fe.js" integrity="sha512-a7JraRWUIBWcdNyeCXsGpXjtK2jHAUZqkaRKljLYUb0K8WehswASOHtMUStIrZrU0zlOBNd6441X4ZIP5O00/g==" crossorigin="anonymous" defer></script></html>