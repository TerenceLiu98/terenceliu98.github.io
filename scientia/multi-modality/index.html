<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://blog.cklau.cc/images/logo.svg" />
<title>üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image | ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</title>
<meta name="title" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image" />
<meta name="description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio." />
<meta name="keywords" content="Artificial Intelligence,text-image,multi-modality,long-read," />


<meta property="og:title" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image" />
<meta property="og:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio." />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.cklau.cc/scientia/multi-modality/" /><meta property="article:section" content="scientia" />
<meta property="article:published_time" content="2024-02-08T00:15:00+00:00" />
<meta property="article:modified_time" content="2024-02-08T00:15:00+00:00" />




<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image"/>
<meta name="twitter:description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio."/>



<meta itemprop="name" content="üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image">
<meta itemprop="description" content="Before For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio."><meta itemprop="datePublished" content="2024-02-08T00:15:00+00:00" />
<meta itemprop="dateModified" content="2024-02-08T00:15:00+00:00" />
<meta itemprop="wordCount" content="1183">
<meta itemprop="keywords" content="Artificial Intelligence,text-image,multi-modality,long-read," />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: "Noto Serif SC", serif;
    font-weight: 500;
    font-style: normal;
    margin: auto;
    padding: 20px;
    max-width: 980px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #444;
    text-decoration: none;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  main content p a {
    word-wrap: break-word;
    border: none;
    box-shadow: inset 0 -2px #444;
    transition-property: box-shadow;
    transition-duration: .1s;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
    line-height: 2;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 6.5rem;
  }

  ul.blog-posts li time {
    color: #8e8d8d;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }


   


  .bg { background-color: #f0f0f0; }
  .chroma { background-color: #f0f0f0; }
  .chroma .x {  }
  .chroma .err {  }
  .chroma .cl {  }
  .chroma .lnlinks { outline: none; text-decoration: none; color: inherit }
  .chroma .lntd { vertical-align: top; padding: 0; margin: 0; border: 0; }
  .chroma .lntable { border-spacing: 0; padding: 0; margin: 0; border: 0; }
  .chroma .hl { background-color: #ffffcc }
  .chroma .lnt { white-space: pre; user-select: none; margin-right: 0.4em; padding: 0 0.4em 0 0.4em;color: #7f7f7f }
  .chroma .ln { white-space: pre; user-select: none; margin-right: 0.4em; padding: 0 0.4em 0 0.4em;color: #7f7f7f }
  .chroma .line { display: flex; }
  .chroma .k { color: #007020; font-weight: bold }
  .chroma .kc { color: #007020; font-weight: bold }
  .chroma .kd { color: #007020; font-weight: bold }
  .chroma .kn { color: #007020; font-weight: bold }
  .chroma .kp { color: #007020 }
  .chroma .kr { color: #007020; font-weight: bold }
  .chroma .kt { color: #902000 }
  .chroma .n {  }
  .chroma .na { color: #4070a0 }
  .chroma .nb { color: #007020 }
  .chroma .bp {  }
  .chroma .nc { color: #0e84b5; font-weight: bold }
  .chroma .no { color: #60add5 }
  .chroma .nd { color: #555555; font-weight: bold }
  .chroma .ni { color: #d55537; font-weight: bold }
  .chroma .ne { color: #007020 }
  .chroma .nf { color: #06287e }
  .chroma .fm {  }
  .chroma .nl { color: #002070; font-weight: bold }
  .chroma .nn { color: #0e84b5; font-weight: bold }
  .chroma .nx {  }
  .chroma .py {  }
  .chroma .nt { color: #062873; font-weight: bold }
  .chroma .nv { color: #bb60d5 }
  .chroma .vc {  }
  .chroma .vg {  }
  .chroma .vi {  }
  .chroma .vm {  }
  .chroma .l {  }
  .chroma .ld {  }
  .chroma .s { color: #4070a0 }
  .chroma .sa { color: #4070a0 }
  .chroma .sb { color: #4070a0 }
  .chroma .sc { color: #4070a0 }
  .chroma .dl { color: #4070a0 }
  .chroma .sd { color: #4070a0; font-style: italic }
  .chroma .s2 { color: #4070a0 }
  .chroma .se { color: #4070a0; font-weight: bold }
  .chroma .sh { color: #4070a0 }
  .chroma .si { color: #70a0d0 }
  .chroma .sx { color: #c65d09 }
  .chroma .sr { color: #235388 }
  .chroma .s1 { color: #4070a0 }
  .chroma .ss { color: #517918 }
  .chroma .m { color: #40a070 }
  .chroma .mb { color: #40a070 }
  .chroma .mf { color: #40a070 }
  .chroma .mh { color: #40a070 }
  .chroma .mi { color: #40a070 }
  .chroma .il { color: #40a070 }
  .chroma .mo { color: #40a070 }
  .chroma .o { color: #666666 }
  .chroma .ow { color: #007020; font-weight: bold }
  .chroma .p {  }
  .chroma .c { color: #60a0b0; font-style: italic }
  .chroma .ch { color: #60a0b0; font-style: italic }
  .chroma .cm { color: #60a0b0; font-style: italic }
  .chroma .c1 { color: #60a0b0; font-style: italic }
  .chroma .cs { color: #60a0b0; background-color: #fff0f0 }
  .chroma .cp { color: #007020 }
  .chroma .cpf { color: #007020 }
  .chroma .g {  }
  .chroma .gd { color: #a00000 }
  .chroma .ge { font-style: italic }
  .chroma .gr { color: #ff0000 }
  .chroma .gh { color: #000080; font-weight: bold }
  .chroma .gi { color: #00a000 }
  .chroma .go { color: #888888 }
  .chroma .gp { color: #c65d09; font-weight: bold }
  .chroma .gs { font-weight: bold }
  .chroma .gu { color: #800080; font-weight: bold }
  .chroma .gt { color: #0044dd }
  .chroma .gl { text-decoration: underline }
  .chroma .w { color: #bbbbbb }


 
.sidenote {
    font-size: 80%;         
    font-weight: normal;
    color: var(--theme-hl1-color);
    position: relative;     
}
 
@media (min-width: 1400px) {
    .sidenote {
        float: right;
        clear: right;            
        text-align: left;

         
            
        top: -3rem;             
        width: 20vw;            
        margin-right: -23vw;     
        margin-top: 1rem;       
    }
}
 
 
@media (max-width: 1400px) {
    .sidenote {
         
        float: right;
        text-align: left;

            
        width: 100%;  
        margin: 1rem 0;
        padding-left: 5%;  
    }
}
 
 
body {
    counter-reset: sidenote-counter;
}
.sidenote-number {
    counter-increment: sidenote-counter;
}
 
.sidenote::before {
    content: "\2020";
    position: relative;
    vertical-align: super;
    font-size: 0.9em;
    font-weight: bold;
}
 
.sidenote-number::after {
    content: "\2020";
    position: relative;
    vertical-align: super;
    font-size: 0.7em;
    font-weight: bold;
    color: var(--theme-body-color);
    display: inline;
    margin-right: 0.2rem;
}
@media (min-width: 1400px) {
     
    .sidenote-number:hover .sidenote {
        background-color: var(--theme-color-light);
    }
}

  
.card-container {
    display: flex;
    flex-direction: column;
    align-items: center;  
    gap: 5px;  
    padding: 5px;
}

.card {
    width: 80%;  
    box-shadow: 0 0px 0px rgba(0,0,0,0.1);  
    display: flex;
    align-items: center;  
    padding: 10px;
    background: white;  
    text-decoration: none;  
    color: inherit;  
}

.card img {
    width: 80px;  
    height: 80px;
    border-radius: 50%;  
    margin-right: 20px;  
}

.card .info h2 {
    margin: 0;
    font-size: 1.2em;  
}

.card .info p {
    margin: 5px 0 0;  
    font-size: 0.9em;
    color: #444;  
}
.card .info span{
    font-size: 0.5em;
    color: #666;
}


.research-card {
  border: 1px solid #ddd;
  padding: 20px;
  margin-bottom: 20px;
  border-radius: 8px;
  background-color: #fff;
  box-shadow: 0 2px 4px rgba(0, 0, 0, 0.1);
}

.research-card h2 {
  margin-top: 0;
}

.research-card .subtitle {
  font-size: 14px;
  color: #555;
}

.research-card .description {
  font-size: 16px;
  margin: 10px 0;
}

.research-card .authors {
  font-size: 14px;
  color: #777;
}

.research-card .buttons {
  margin-top: 10px;
}

.research-card .btn {
  display: inline-block;
  margin-right: 10px;
  padding: 8px 12px;
  font-size: 14px;
  text-decoration: none;
  border-radius: 4px;
  transition: background-color 0.3s ease;
}

.research-card .btn-primary {
  background-color: #007bff;
  color: #fff;
}

.research-card .btn-secondary {
  background-color: #6c757d;
  color: #fff;
}

.research-card .btn-tertiary {
  background-color: #28a745;
  color: #fff;
}

.research-card .btn:hover {
  opacity: 0.9;
}


</style>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500&display=swap" rel="stylesheet">
<script defer data-domain="nova.moe" src="https://possible.knat.network/js/script.js"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1844674035384472" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://blog.cklau.cc/feed.xml" title="ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú">


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true },
                { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                { left: "\\begin{align}", right: "\\end{align}", display: true },
                { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
                { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                { left: "\\[", right: "\\]", display: true }
            ],
            
            throwOnError: false,
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>
</head>

<body>
  <header><a href="/" class="title">
  <h2>ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</h2>
</a>
<nav><a href="/">Home</a>

<a href="/post">[Blog]</a>

<a href="/scientia">[Scientia]</a>

<a href="/projects">[Projects]</a>

<a href="/links">[Links]</a>

</nav>
</header>
  <main>
<h1>üßëüèø‚Äçüíª Multimodal Representation Leraning from both Text and Image</h1>
<p>
  <i>
    <time datetime='2024-02-08' pubdate>
      08 Feb, 2024
    </time>
  </i>
</p>

<content>
  <h2 id="before">Before</h2>
<p>For a long time, the machine learning model (deep learning model) cannot understand more than one modality, i.e., whether they knows how to do the text-based task or they know how to play with the image. As artificial intelligence, the researchers would like to the models have the cability of manipulating the multimodal data as the natural intelligence is not limited to just a single modality. Such that the AI shell read and write text while they could also see images and watch videos and hear the audio.</p>
<p>However, different data mode has different representation, especially for the machine, i.e., different mode of data has different representation. Some of the modality can be approximated by the others, e.g. audio can be transformed into images via the Fourier Transformation <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<p>Usually, the multimodal learning includes these five aspects [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-1" title="T. Baltru≈°aitis, C. Ahuja, and L. Morency, Multimodal Machine Learning: A Survey and Taxonomy. arXiv, 2017. [Online]. Available: http://arxiv.org/abs/1705.09406 [Accessed: Feb. 8, 2024]. ">1</a>]:</p>
<ol>
<li>Representation: It is important for us to know how to represent each unimodality or multimodality</li>
<li>Translation: This is the mapping from one data mode to another, e.g. using spectrogram to transform audio to image</li>
<li>Alignment or Registration: This is the direct relation between elements from two or more different modalities. If they are in the same modality, we would call it <em>registration</em>.</li>
<li>Fusion: It is to join information from two or more modalities to perform a downstream task.</li>
<li>Co-learning: Transforming knowledge from different modalities</li>
</ol>
<p>For this series, we are not shaped by the five main aspects, instead, we would organized by modalities. In this post, we are focusing on the Text and Image, with such amounts of researchs and works, I decided to follow on the two papers: CLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-2" title="A. Radford, J. Kim, C. Hallacy, A. Ramesh, G. Goh, S. Agarwal, G. Sastry, A. Askell, P. Mishkin, J. Clark, G. Krueger, and I. Sutskever, Learning Transferable Visual Models From Natural Language Supervision, . ">2</a>] and BLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-3" title="J. Li, D. Li, C. Xiong, and S. Hoi, BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation, . ">3</a>], some of their variants would be mentioned.</p>
<h2 id="clip-learning-transferable-visual-models-from-natural-language-supervision">CLIP: Learning Transferable Visual Models From Natural Language Supervision</h2>
<p>CLIP&rsquo;s key contribution is its ability to map text and image into a shared embedding space based on the seperated text/image encoder. This shared multimodal embedding space makes text-to-image and image-to-text tasks so much easier. Not meaning that it can directly used in these tasks, but providing a idea that how to do the multimodal representation.</p>
<p>Figure 1 shows the approach of CLIP model. Text and image are encoded by two different encoders $f_{\theta_i}$ and $g_{\theta_t}$, let $\mathbf{x} \in \mathbb{R}^{N \times H \times W \times C}$ as one batch of image, $\mathbf{y} \in \mathbb{R}^{N \times S}$ as one batch of text data,  the embedding of $\mathbf{x}$ and $\mathbf{y}$ then can be denoted as:</p>
<p>$$\begin{aligned} \mathbf{f} &amp;= f_{\theta_i}(\mathbf{x}) \in \mathbf{R}^{N \times D_i} \Rightarrow \mathbf{f}^e = L_i(\mathbf{f})  \cr  \mathbf{g} &amp;= g_{\theta_t}(\mathbf{y}) \in \mathbf{R}^{N \times D_t} \Rightarrow \mathbf{g}^e = L_t(\mathbf{g}) \end{aligned}$$</p>
<p>where $D_i$ and $D_t$ are the dimension of the image and text embedding, respectively. linear projectors $L_i$ and $L_t$ are used for mapping two embedding into the same dimension. The dot product between the image and text embedding is used to calculate the similarity between the text and image, i.e., $\mathcal{F} = \mathbf{f} \cdot \mathbf{g}$, where $\mathcal{F} \in \mathbb{R}^{N \times N}$ is the similarity matrix and the $\mathcal{F} \circ I_N$ is the positive sample set, and the others are the negative samples (In total, there are $N^2 - N$ negatives samples). The pseudocode of the CLIP model is shown in Figure 2 (original papers). The idea of CLIP model is relative naive, as it is a classic negative sampling method called batch negative sampling. However, intuitively, it is hard for us to do the model explanation, i.e., why does it work? Still, we could explan that the corresponding text and image are sharing the same ontology, but this kind of explanation is not grounded. More or less, the main contribution is that they create a huge dataset with the text and image pairs, including 400 million (image, text) pairs collected form of a variety of publicly available sources on the Internet. With this dataset, they don&rsquo;t even need the pretrained encoder as the encoder can be trained simultaneously with the downstream alignment.</p>
<h3 id="implementation">Implementation</h3>
<p>Here is the simple implementation of CLIP:</p>
<details>
  <summary>Implementation of Encoders</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#f92672">import</span> torch 
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> torch.nn <span style="color:#66d9ef">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#f92672">from</span> transformers <span style="color:#f92672">import</span> AutoConfig, AutoModel, AutoModelForImageClassification
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#f92672">import</span> numpy <span style="color:#66d9ef">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">TextEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;FacebookAI/roberta-base&#34;</span>:str, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool):
</span></span><span style="display:flex;"><span>        super(TextEncoder)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> {<span style="color:#66d9ef">True</span>: AutoModel<span style="color:#f92672">.</span>from_pretrained(model_name), 
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">False</span>: AutoModel<span style="color:#f92672">.</span>from_config(AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name))}
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out_dim <span style="color:#f92672">=</span> AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name)<span style="color:#f92672">.</span>hidden_size <span style="color:#75715e"># 768</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>freeze <span style="color:#f92672">=</span> freeze
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> freeze:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> name ,param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>                param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> self<span style="color:#f92672">.</span>freeze:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>eval()
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">pass</span>
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>normalize(torch<span style="color:#f92672">.</span>mean(self<span style="color:#f92672">.</span>model(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>last_hidden_state, axis<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>), dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ImageEncoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;google/vit-base-patch16-224&#34;</span>:str, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool):
</span></span><span style="display:flex;"><span>        super(TextEncoder)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> {<span style="color:#66d9ef">True</span>: AutoModelForImageClassification<span style="color:#f92672">.</span>from_pretrained(model_name), 
</span></span><span style="display:flex;"><span>                      <span style="color:#66d9ef">False</span>: AutoModelForImageClassification<span style="color:#f92672">.</span>from_config(AutoConfig<span style="color:#f92672">.</span>from_pretrained(model_name))}
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>out_dim <span style="color:#f92672">=</span> <span style="color:#ae81ff">1000</span> <span style="color:#75715e"># ViT is trained with ImageNet</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>freeze <span style="color:#f92672">=</span> freeze
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> freeze:
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> name ,param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>named_parameters():
</span></span><span style="display:flex;"><span>                param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, inputs):
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(<span style="color:#f92672">**</span>inputs)<span style="color:#f92672">.</span>logits
</span></span><span style="display:flex;"><span>        feature <span style="color:#f92672">=</span> F<span style="color:#f92672">.</span>normalize(feature, dim<span style="color:#f92672">=-</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature
</span></span></code></pre></div></details>
<details>
  <summary>Implementation of Linear Projection</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span>  <span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LinearProject</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> __init__(self, in_features, out_features):
</span></span><span style="display:flex;"><span>          super(LinearProject)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>          self<span style="color:#f92672">.</span>projection <span style="color:#f92672">==</span> nn<span style="color:#f92672">.</span>Sequential([
</span></span><span style="display:flex;"><span>              nn<span style="color:#f92672">.</span>Linear(in_features, out_features), nn<span style="color:#f92672">.</span>GELU(), nn<span style="color:#f92672">.</span>LayerNorm(out_features)
</span></span><span style="display:flex;"><span>          ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>      <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>          output <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>projection(x)
</span></span><span style="display:flex;"><span>          <span style="color:#66d9ef">return</span> output
</span></span></code></pre></div></details>
<details>
  <summary>Implementation of CLIP Model</summary>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">CLIPModel</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, model_name <span style="color:#f92672">=</span> {<span style="color:#e6db74">&#34;TEXT&#34;</span>:<span style="color:#e6db74">&#34;FacebookAI/roberta-base&#34;</span>, <span style="color:#e6db74">&#34;IMG&#34;</span>:<span style="color:#e6db74">&#34;google/vit-base-patch16-224&#34;</span>}:dict, 
</span></span><span style="display:flex;"><span>                  device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>:str, pretrained<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>:bool, freeze<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>:bool, hidden_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">256</span>:int):
</span></span><span style="display:flex;"><span>        super(CLIPModel)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>enc_t <span style="color:#f92672">=</span> TextEncoder(model_name<span style="color:#f92672">=</span>model_name[<span style="color:#e6db74">&#34;TEXT&#34;</span>], device<span style="color:#f92672">=</span>device, freeze<span style="color:#f92672">=</span>freeze)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>enc_i <span style="color:#f92672">=</span> ImageEncoder(model_name<span style="color:#f92672">=</span>model_name[<span style="color:#e6db74">&#34;IMG&#34;</span>], device<span style="color:#f92672">=</span>device, freeze<span style="color:#f92672">=</span>freeze)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>proj_t <span style="color:#f92672">=</span> LinearProject(self<span style="color:#f92672">.</span>enc_t<span style="color:#f92672">.</span>out_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>proj_i <span style="color:#f92672">=</span> LinearProject(self<span style="color:#f92672">.</span>enc_i<span style="color:#f92672">.</span>out_dim, hidden_dim)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>logit_scale <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Parameter(torch<span style="color:#f92672">.</span>ones([]))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>init_parameters()
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">init_parameters</span>(self):
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># turn temperature into a learnable parameter</span>
</span></span><span style="display:flex;"><span>        nn<span style="color:#f92672">.</span>init<span style="color:#f92672">.</span>constant_(self<span style="color:#f92672">.</span>logit_scale, np<span style="color:#f92672">.</span>log(<span style="color:#ae81ff">1</span> <span style="color:#f92672">/</span> <span style="color:#ae81ff">0.07</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">criterion</span>(self, text, image):
</span></span><span style="display:flex;"><span>        CE <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>functional<span style="color:#f92672">.</span>cross_entropy
</span></span><span style="display:flex;"><span>        labels <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>arange(text<span style="color:#f92672">.</span>shape[<span style="color:#ae81ff">0</span>], device<span style="color:#f92672">=</span>str(text<span style="color:#f92672">.</span>device), dtype<span style="color:#f92672">=</span>torch<span style="color:#f92672">.</span>long)
</span></span><span style="display:flex;"><span>        logits_t <span style="color:#f92672">=</span> text <span style="color:#f92672">@</span> image<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>logit_scale<span style="color:#f92672">.</span>exp()
</span></span><span style="display:flex;"><span>        logits_i <span style="color:#f92672">=</span> image <span style="color:#f92672">@</span> text<span style="color:#f92672">.</span>T <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>logit_scale<span style="color:#f92672">.</span>exp()
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> (CE(logits_t, labels) <span style="color:#f92672">+</span> CE(logits_i, labels)) <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, text, image):
</span></span><span style="display:flex;"><span>        feature_t, feature_i <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>proj_t(self<span style="color:#f92672">.</span>enc_t(text)), self<span style="color:#f92672">.</span>proj_i(self<span style="color:#f92672">.</span>enc_i(image))
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>criterion(feature_t, feature_i)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> feature_t, feature_i, self<span style="color:#f92672">.</span>logit_scale, loss
</span></span></code></pre></div></details>
<h3 id="applications">Applications</h3>
<p>The first application of CLIP is classification. Since the CLIP model is relatively similar to the reitrival model, it can be easily implemented into the classification with zero-shot learning. The zero-shot learning is a task that the model can classify the unseen classes without any training data. See the second part of Figure 1, where for a given image, the model can classify the similarity between the given image and the prompted text, by calculating the similarity of image and each given sentence we could get the classification, vice versa. This can be seen as classification, or retrieval.</p>
<p>The second application is the generation. The CLIP model can be used to generate the image from the given text. Although it cannot generate the image directly since it does not have any decoder, however, the CLIP can be seen as the backbone and provide the embedding for image or text generation. After the CLIP came out, OpenAI also released the DALL-E2, where they provide a model called unCLIP [<a class="hugo-simplecite-cite-hyperlink" href="#bibreference-4" title="A. Ramesh, P. Dhariwal, A. Nichol, C. Chu, and M. Chen, Hierarchical Text-Conditional Image Generation with CLIP Latents. arXiv, 2022. [Online]. Available: http://arxiv.org/abs/2204.06125 [Accessed: Feb. 8, 2024]. ">4</a>], it is a text-to-image generation model<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<h2 id="blip-hierarchical-text-conditional-image-generation-with-clip-latents">BLIP: Hierarchical Text-Conditional Image Generation with CLIP Latents</h2>
<h2 id="empirical-research">Empirical Research</h2>
<h2 id="reference">Reference</h2>
<ol class="hugo-simplecite-reference-list"><li class="hugo-simplecite-reference-list-item" id="bibreference-1">T. Baltru≈°aitis,&#32;C. Ahuja, and&#32;L. Morency,&#32;<em>Multimodal Machine Learning: A Survey and Taxonomy</em>.&#32;arXiv, 2017.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="http://arxiv.org/abs/1705.09406">http://arxiv.org/abs/1705.09406</a>&#32;[Accessed:
Feb. 8, 2024].
</li><li class="hugo-simplecite-reference-list-item" id="bibreference-2">A. Radford,&#32;J. Kim,&#32;C. Hallacy,&#32;A. Ramesh,&#32;G. Goh,&#32;S. Agarwal,&#32;G. Sastry,&#32;A. Askell,&#32;P. Mishkin,&#32;J. Clark,&#32;G. Krueger, and&#32;I. Sutskever,&#32;<q>Learning Transferable Visual Models From Natural Language Supervision,</q>&#32;.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-3">J. Li,&#32;D. Li,&#32;C. Xiong, and&#32;S. Hoi,&#32;<q>BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation,</q>&#32;.&#32;</li><li class="hugo-simplecite-reference-list-item" id="bibreference-4">A. Ramesh,&#32;P. Dhariwal,&#32;A. Nichol,&#32;C. Chu, and&#32;M. Chen,&#32;<em>Hierarchical Text-Conditional Image Generation with CLIP Latents</em>.&#32;arXiv, 2022.&#32;[Online]. Available: <a class="hugo-simplecite-url-hyperlink" rel="noopener" target="_blank" href="http://arxiv.org/abs/2204.06125">http://arxiv.org/abs/2204.06125</a>&#32;[Accessed:
Feb. 8, 2024].
</li></ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For more information, pleace check <a href="https://en.wikipedia.org/wiki/Spectrogram">Spectrogram</a> and <a href="https://ieeexplore.ieee.org/document/9859621">Mel Spectrogram</a>&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>We would talk about the &ldquo;downstream&rdquo; research in the <a href="#empirical-research">&ldquo;Empirical Research&rdquo;</a> section.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

</content>

<p>
  
  <a href="https://blog.cklau.cc/tags/artificial-intelligence/">#Artificial Intelligence</a>
  
  <a href="https://blog.cklau.cc/tags/text-image/">#text-image</a>
  
  <a href="https://blog.cklau.cc/tags/multi-modality/">#multi-modality</a>
  
  <a href="https://blog.cklau.cc/tags/long-read/">#long-read</a>
  
</p>

  </main>
  <footer>


<p>
    <span><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">Á≤§ICPÂ§á2022102668Âè∑</a></span>

&copy; 2025 <a href="https://blog.cklau.cc">Junjie LIU</a>
    <a href="https://blog.cklau.cc/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
    <br>
Hosting and images served by <a href="https://cloud.tencent.com/">Tencent Cloud</a> / <a href="https://cloudflare.com/">Cloudflare</a> / <a href="https://webp.se/">WebP Cloud Services</a>
</p>

</footer>

    
</body>

</html>
