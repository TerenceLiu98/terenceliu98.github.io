<!DOCTYPE html>
<html lang="en">

<head>
  <meta http-equiv="X-Clacks-Overhead" content="GNU Terry Pratchett" />
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
<link rel="shortcut icon" href="https://blog.cklau.cc/images/logo.svg" />
<title>🥼 RetiGAN: A GAN-based model on retinal Image synthesis | 特倫蘇的日與夜</title>
<meta name="title" content="🥼 RetiGAN: A GAN-based model on retinal Image synthesis" />
<meta name="description" content="" />
<meta name="keywords" content="Artificial Intelligence,Medical Image,Fundus Image," />


<meta property="og:title" content="🥼 RetiGAN: A GAN-based model on retinal Image synthesis" />
<meta property="og:description" content="" />
<meta property="og:type" content="article" />
<meta property="og:url" content="https://blog.cklau.cc/scientia/10.1016-j.bspc.2022.104004/" /><meta property="article:section" content="scientia" />
<meta property="article:published_time" content="2023-04-02T00:13:00+08:00" />
<meta property="article:modified_time" content="2023-04-02T00:13:00+08:00" />




<meta name="twitter:card" content="summary"/><meta name="twitter:title" content="🥼 RetiGAN: A GAN-based model on retinal Image synthesis"/>
<meta name="twitter:description" content=""/>



<meta itemprop="name" content="🥼 RetiGAN: A GAN-based model on retinal Image synthesis">
<meta itemprop="description" content=""><meta itemprop="datePublished" content="2023-04-02T00:13:00+08:00" />
<meta itemprop="dateModified" content="2023-04-02T00:13:00+08:00" />
<meta itemprop="wordCount" content="2595">
<meta itemprop="keywords" content="Artificial Intelligence,Medical Image,Fundus Image," />
<meta name="referrer" content="no-referrer-when-downgrade" />

  <style>
  body {
    font-family: "Noto Serif SC", serif;
    font-weight: 500;
    font-style: normal;
    margin: auto;
    padding: 20px;
    max-width: 980px;
    text-align: left;
    background-color: #fff;
    word-wrap: break-word;
    overflow-wrap: break-word;
    line-height: 1.5;
    color: #444;
  }

  h1,
  h2,
  h3,
  h4,
  h5,
  h6,
  strong,
  b {
    color: #222;
  }

  a {
    color: #444;
    text-decoration: none;
     
  }

  .title {
    text-decoration: none;
    border: 0;
  }

  .title span {
    font-weight: 400;
  }

  nav a {
    margin-right: 10px;
  }

  textarea {
    width: 100%;
    font-size: 16px;
  }

  input {
    font-size: 16px;
  }

  content {
    line-height: 1.6;
  }

  main content p a {
    word-wrap: break-word;
    border: none;
    box-shadow: inset 0 -2px #444;
    transition-property: box-shadow;
    transition-duration: .1s;
  }

  table {
    width: 100%;
  }

  img {
    max-width: 100%;
  }

  code {
    padding: 2px 5px;
    background-color: #f2f2f2;
  }

  pre code {
    color: #222;
    display: block;
    padding: 20px;
    white-space: pre-wrap;
    font-size: 14px;
    overflow-x: auto;
  }

  div.highlight pre {
    background-color: initial;
    color: initial;
  }

  div.highlight code {
    background-color: unset;
    color: unset;
  }

  blockquote {
    border-left: 1px solid #999;
    color: #222;
    padding-left: 20px;
    font-style: italic;
  }

  footer {
    padding: 25px;
    text-align: center;
  }

  .helptext {
    color: #777;
    font-size: small;
  }

  .errorlist {
    color: #eba613;
    font-size: small;
  }

   
  ul.blog-posts {
    list-style-type: none;
    padding: unset;
    line-height: 2;
  }

  ul.blog-posts li {
    display: flex;
  }

  ul.blog-posts li span {
    flex: 0 0 6.5rem;
  }

  ul.blog-posts li time {
    color: #8e8d8d;
  }

  ul.blog-posts li a:visited {
    color: #8b6fcb;
  }


   


  .bg { background-color: #f0f0f0; }
  .chroma { background-color: #f0f0f0; }
  .chroma .x {  }
  .chroma .err {  }
  .chroma .cl {  }
  .chroma .lnlinks { outline: none; text-decoration: none; color: inherit }
  .chroma .lntd { vertical-align: top; padding: 0; margin: 0; border: 0; }
  .chroma .lntable { border-spacing: 0; padding: 0; margin: 0; border: 0; }
  .chroma .hl { background-color: #ffffcc }
  .chroma .lnt { white-space: pre; user-select: none; margin-right: 0.4em; padding: 0 0.4em 0 0.4em;color: #7f7f7f }
  .chroma .ln { white-space: pre; user-select: none; margin-right: 0.4em; padding: 0 0.4em 0 0.4em;color: #7f7f7f }
  .chroma .line { display: flex; }
  .chroma .k { color: #007020; font-weight: bold }
  .chroma .kc { color: #007020; font-weight: bold }
  .chroma .kd { color: #007020; font-weight: bold }
  .chroma .kn { color: #007020; font-weight: bold }
  .chroma .kp { color: #007020 }
  .chroma .kr { color: #007020; font-weight: bold }
  .chroma .kt { color: #902000 }
  .chroma .n {  }
  .chroma .na { color: #4070a0 }
  .chroma .nb { color: #007020 }
  .chroma .bp {  }
  .chroma .nc { color: #0e84b5; font-weight: bold }
  .chroma .no { color: #60add5 }
  .chroma .nd { color: #555555; font-weight: bold }
  .chroma .ni { color: #d55537; font-weight: bold }
  .chroma .ne { color: #007020 }
  .chroma .nf { color: #06287e }
  .chroma .fm {  }
  .chroma .nl { color: #002070; font-weight: bold }
  .chroma .nn { color: #0e84b5; font-weight: bold }
  .chroma .nx {  }
  .chroma .py {  }
  .chroma .nt { color: #062873; font-weight: bold }
  .chroma .nv { color: #bb60d5 }
  .chroma .vc {  }
  .chroma .vg {  }
  .chroma .vi {  }
  .chroma .vm {  }
  .chroma .l {  }
  .chroma .ld {  }
  .chroma .s { color: #4070a0 }
  .chroma .sa { color: #4070a0 }
  .chroma .sb { color: #4070a0 }
  .chroma .sc { color: #4070a0 }
  .chroma .dl { color: #4070a0 }
  .chroma .sd { color: #4070a0; font-style: italic }
  .chroma .s2 { color: #4070a0 }
  .chroma .se { color: #4070a0; font-weight: bold }
  .chroma .sh { color: #4070a0 }
  .chroma .si { color: #70a0d0 }
  .chroma .sx { color: #c65d09 }
  .chroma .sr { color: #235388 }
  .chroma .s1 { color: #4070a0 }
  .chroma .ss { color: #517918 }
  .chroma .m { color: #40a070 }
  .chroma .mb { color: #40a070 }
  .chroma .mf { color: #40a070 }
  .chroma .mh { color: #40a070 }
  .chroma .mi { color: #40a070 }
  .chroma .il { color: #40a070 }
  .chroma .mo { color: #40a070 }
  .chroma .o { color: #666666 }
  .chroma .ow { color: #007020; font-weight: bold }
  .chroma .p {  }
  .chroma .c { color: #60a0b0; font-style: italic }
  .chroma .ch { color: #60a0b0; font-style: italic }
  .chroma .cm { color: #60a0b0; font-style: italic }
  .chroma .c1 { color: #60a0b0; font-style: italic }
  .chroma .cs { color: #60a0b0; background-color: #fff0f0 }
  .chroma .cp { color: #007020 }
  .chroma .cpf { color: #007020 }
  .chroma .g {  }
  .chroma .gd { color: #a00000 }
  .chroma .ge { font-style: italic }
  .chroma .gr { color: #ff0000 }
  .chroma .gh { color: #000080; font-weight: bold }
  .chroma .gi { color: #00a000 }
  .chroma .go { color: #888888 }
  .chroma .gp { color: #c65d09; font-weight: bold }
  .chroma .gs { font-weight: bold }
  .chroma .gu { color: #800080; font-weight: bold }
  .chroma .gt { color: #0044dd }
  .chroma .gl { text-decoration: underline }
  .chroma .w { color: #bbbbbb }


 
.sidenote {
    font-size: 80%;         
    font-weight: normal;
    color: var(--theme-hl1-color);
    position: relative;     
}
 
@media (min-width: 1400px) {
    .sidenote {
        float: right;
        clear: right;            
        text-align: left;

         
            
        top: -3rem;             
        width: 20vw;            
        margin-right: -23vw;     
        margin-top: 1rem;       
    }
}
 
 
@media (max-width: 1400px) {
    .sidenote {
         
        float: right;
        text-align: left;

            
        width: 100%;  
        margin: 1rem 0;
        padding-left: 5%;  
    }
}
 
 
body {
    counter-reset: sidenote-counter;
}
.sidenote-number {
    counter-increment: sidenote-counter;
}
 
.sidenote::before {
    content: "\2020";
    position: relative;
    vertical-align: super;
    font-size: 0.9em;
    font-weight: bold;
}
 
.sidenote-number::after {
    content: "\2020";
    position: relative;
    vertical-align: super;
    font-size: 0.7em;
    font-weight: bold;
    color: var(--theme-body-color);
    display: inline;
    margin-right: 0.2rem;
}
@media (min-width: 1400px) {
     
    .sidenote-number:hover .sidenote {
        background-color: var(--theme-color-light);
    }
}

  
.card-container {
    display: flex;
    flex-direction: column;
    align-items: center;  
    gap: 5px;  
    padding: 5px;
}

.card {
    width: 80%;  
    box-shadow: 0 0px 0px rgba(0,0,0,0.1);  
    display: flex;
    align-items: center;  
    padding: 10px;
    background: white;  
    text-decoration: none;  
    color: inherit;  
}

.card img {
    width: 80px;  
    height: 80px;
    border-radius: 50%;  
    margin-right: 20px;  
}

.card .info h2 {
    margin: 0;
    font-size: 1.2em;  
}

.card .info p {
    margin: 5px 0 0;  
    font-size: 0.9em;
    color: #444;  
}
.card .info span{
    font-size: 0.5em;
    color: #666;
}
</style>

<link rel="preconnect" href="https://fonts.googleapis.com">
<link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
<link href="https://fonts.googleapis.com/css2?family=Noto+Serif+SC:wght@400;500&display=swap" rel="stylesheet">
<script defer data-domain="nova.moe" src="https://possible.knat.network/js/script.js"></script>
<script async src="https://pagead2.googlesyndication.com/pagead/js/adsbygoogle.js?client=ca-pub-1844674035384472" crossorigin="anonymous"></script>
<link rel="alternate" type="application/rss+xml" href="https://blog.cklau.cc/feed.xml" title="特倫蘇的日與夜">


  <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.css">
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/katex.min.js"></script>
<script defer src="https://cdn.jsdelivr.net/npm/katex@0.16.18/dist/contrib/auto-render.min.js"></script>

<script>
    document.addEventListener("DOMContentLoaded", function () {
        renderMathInElement(document.body, {
            
            
            delimiters: [
                { left: '$$', right: '$$', display: true },
                { left: '$', right: '$', display: false },
                { left: '\\(', right: '\\)', display: false },
                { left: '\\[', right: '\\]', display: true },
                { left: "\\begin{equation}", right: "\\end{equation}", display: true },
                { left: "\\begin{align}", right: "\\end{align}", display: true },
                { left: "\\begin{alignat}", right: "\\end{alignat}", display: true },
                { left: "\\begin{gather}", right: "\\end{gather}", display: true },
                { left: "\\begin{CD}", right: "\\end{CD}", display: true },
                { left: "\\[", right: "\\]", display: true }
            ],
            
            throwOnError: false,
            trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
            macros: {
                "\\eqref": "\\href{###1}{(\\text{#1})}",
                "\\ref": "\\href{###1}{\\text{#1}}",
                "\\label": "\\htmlId{#1}{}"
            }
        });
    });
</script>
</head>

<body>
  <header><a href="/" class="title">
  <h2>特倫蘇的日與夜</h2>
</a>
<nav><a href="/">Home</a>

<a href="/post">[Blog]</a>

<a href="/scientia">[Scientia]</a>

<a href="/projects">[Projects]</a>

<a href="/links">[Links]</a>

</nav>
</header>
  <main>
<h1>🥼 RetiGAN: A GAN-based model on retinal Image synthesis</h1>
<p>
  <i>
    <time datetime='2023-04-02' pubdate>
      02 Apr, 2023
    </time>
  </i>
</p>

<content>
  <h2 id="before-tldr">Before (TL;DR)</h2>
<p>The field of medical imaging is rapidly adopting artificial intelligence (AI) as a promising technology, with a particular focus on deep learning techniques. These AI methods have the potential to be applied across a range of tasks in medical imaging, from image acquisition and reconstruction to analysis and interpretation. For instance, Computed Tomography(CT), Magnetic Resonance Imaging, Positron Emission Tomography (PET), Mammography, Ultrasound, and X-ray, haved be used for early detection, diagnosis, and treatment of diseases. In the clinic, medical image interpretation in the clinic has relied on human experts, such as radiologists and physicians. However, due to the wide range of pathologies and the risk of human fatigue, researchers and doctors have started to explore the potential of computer-assisted interventions. While progress in computational medical image analysis has not been as rapid as in medical imaging technologies, the situation is improving with the introduction of machine learning techniques.</p>
<p>Machine learning techniques, such as deep learning, have shown promise in assisting human experts in medical image interpretation. By training algorithms on large datasets of medical images, these techniques can help identify patterns and features that are difficult for human experts to detect. As a result, they have the potential to improve diagnostic accuracy and reduce the risk of errors.</p>
<p>As the field of machine learning continues to evolve, it is likely that we will see more applications of these techniques in medical imaging. While they will not replace the need for human experts, they can provide valuable assistance and help improve patient outcomes.</p>
<p>As the very first blog on Medical CV/Medical Image processing, I am trying to introduce a paper working on retinal image synthesis, The authors of this paper have combined multiple classical deep learning techniques that were developed before 2021.</p>
<p>For those who are new to the field of deep learning and medical computer vision, the content of the paper is easily understandable. The authors have used these techniques to synthesize retinal images, which has the potential to aid in the diagnosis and treatment of various eye diseases.</p>
<p>Overall, this paper highlights the potential of deep learning techniques in medical image processing and provides a valuable contribution to the field. As a newcomer to this field, it is an excellent resource for gaining a better understanding of the applications of deep learning in medical computer vision.</p>
<h2 id="a-novel-retinal-image-generation-model-with-the-preservation-of-structural-similarity-and-high-resolution">A novel retinal image generation model with the preservation of structural similarity and high-resolution</h2>
<p>The paper can be found in <a href="https://doi.org/10.1016/j.bspc.2022.104004">10.1016/j.bspc.2022.104004</a>.</p>
<p>In this paper, the proposed a new retinal image generation model call <strong>RetiGAN</strong> based on the adversarial learning. The goal of this model is to <strong>generate high-resolution synthetic images</strong> that preserve the structual similarity of the original images. To achieve this, they embed the VGG network into their RetiGAN to guarantee the high-level semantic information can be extracted and used in the content loss to guide the model to retain the semantic contents of the original image. To solved the problem of blurring and incpomplete edge, the embed the smoothed images into the training set to improve the edge sharpness of the generated images.</p>
<p>In general, the RetiGAN can be divied into four modules:</p>
<ol>
<li>U-Net: for the Vessel tree segmentation</li>
<li>Generator $G$: learns to generate plausible retinal image</li>
<li>Discriminator $D$: learns to distinguish the generator&rsquo;s fake image from real image</li>
<li>VGG modules: perform high-level feature extraction on the generated retinal image and the original image. In this way, the global semantic features of the retinal image can be well preserved.</li>
</ol>
<p>Basically, the idea/architecture of the $G$ and $D$ are from the <a href="https://tcwang0509.github.io/pix2pixHD/">pix2pixHD</a></p>
<h3 id="u-net">U-Net</h3>
<p>The U-Net network is based on a fully convolutional neural network, whose characteristic is that a small amount of training images can be used to obtain a good segmentation effect, very suitable for the field of medical images. The architecture of U-Net is similar to the <em>encoder-decoder</em> architecture, where the left-side of the &ldquo;U&rdquo; is the encoder block, and the right-side is the decoder block<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>.</p>
<ul>
<li>Encoder: is a 4-layer architecture:
<ul>
<li>extracts a meaningful feature map from the input image</li>
<li>each layer includes two convolution layers and one max-pooling for the down-sampling</li>
</ul>
</li>
<li>Decoder: is a 4-layer architecture:
<ul>
<li>up-sampling the feature map</li>
<li>each layer includes two convolution layers</li>
<li>for each layer, it concatenates the corresponding encoder&rsquo;s features (using <code>torchvision.transforms.CenterCrop</code> to maintain the size)</li>
</ul>
</li>
</ul>
<p>In this article, they use U-Net as the tool to separate the background and the vessel tree, as the vessel tree is treated as the input for the generator $G$.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e"># code example for the U-Net</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">DualConv</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel, out_channel):
</span></span><span style="display:flex;"><span>        super(DualConv, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>conv <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReflectionPad2d(padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channel, out_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(out_channel, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(out_channel, out_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">0</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(out_channel, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>conv(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Encoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#75715e"># 3 is for RGB, 1 is for grayscale</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, channels <span style="color:#f92672">=</span> [<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">1024</span>]):
</span></span><span style="display:flex;"><span>        super(Encoder, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encblocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([DualConv(channels[i], channels[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(channels) <span style="color:#f92672">-</span> <span style="color:#ae81ff">1</span>)])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>maxpool2d <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MaxPool2d(<span style="color:#ae81ff">2</span>)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        output <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> encblock <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>encblocks:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> encblock(x)
</span></span><span style="display:flex;"><span>            output<span style="color:#f92672">.</span>append(x)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>maxpool2d(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> output
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Decoder</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, channels<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>], mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trans&#34;</span>):
</span></span><span style="display:flex;"><span>        super()<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>channels <span style="color:#f92672">=</span> channels
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">if</span> mode <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;bilinear&#34;</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>upsampling <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([nn<span style="color:#f92672">.</span>Sequential(nn<span style="color:#f92672">.</span>Upsample(scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bilinear&#34;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>), 
</span></span><span style="display:flex;"><span>                                                            nn<span style="color:#f92672">.</span>Conv2d(channels[i], channels[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>], kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(channels)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>upsampling <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([nn<span style="color:#f92672">.</span>ConvTranspose2d(channels[i], channels[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>], kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(channels)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decblocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([DualConv(channels[i], channels[i<span style="color:#f92672">+</span><span style="color:#ae81ff">1</span>]) <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(channels)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>)]) 
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x, encoder_features):
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(len(self<span style="color:#f92672">.</span>channels)<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>upsampling[i](x)
</span></span><span style="display:flex;"><span>            enc_ftrs <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>crop(encoder_features[i], x)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>cat([x, enc_ftrs], dim<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>)
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decblocks[i](x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> x
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">crop</span>(self, enc_ftrs, x):
</span></span><span style="display:flex;"><span>        _, _, H, W <span style="color:#f92672">=</span> x<span style="color:#f92672">.</span>shape
</span></span><span style="display:flex;"><span>        enc_ftrs   <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>transforms<span style="color:#f92672">.</span>CenterCrop([H, W])(enc_ftrs)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> enc_ftrs
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">UNet</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, enc_channels<span style="color:#f92672">=</span>[<span style="color:#ae81ff">3</span>, <span style="color:#ae81ff">64</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">1024</span>], dec_channels<span style="color:#f92672">=</span>[<span style="color:#ae81ff">1024</span>, <span style="color:#ae81ff">512</span>, <span style="color:#ae81ff">256</span>, <span style="color:#ae81ff">128</span>, <span style="color:#ae81ff">64</span>], num_class<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trans&#34;</span>):
</span></span><span style="display:flex;"><span>        super(UNet, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> Encoder(channels<span style="color:#f92672">=</span>enc_channels)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder <span style="color:#f92672">=</span> Decoder(channels<span style="color:#f92672">=</span>dec_channels, mode<span style="color:#f92672">=</span>mode)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>output <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(nn<span style="color:#f92672">.</span>Conv2d(dec_channels[<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>], num_class, <span style="color:#ae81ff">1</span>), nn<span style="color:#f92672">.</span>Sigmoid())
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        enc_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(x)
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder(enc_out[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">0</span>], enc_out[::<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>][<span style="color:#ae81ff">1</span>:])
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>output(out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h3 id="generator-g">Generator $G$</h3>
<p>The gneerator is is a standard encoder-decoder architecture, which can be seperated into three parts:</p>
<ol>
<li>down-sampling module</li>
<li>residual blocks</li>
<li>up-sampling module</li>
</ol>
<p>where it is the <em>GlobalGenerator</em> in pix2pixHD model, focusing on the coarse high-resolution image. In pix2pixHD, they have seconde generator called <em>LocalEnhancer</em>, focusing on the feature encoding and decoding enhancement. The <em>LocalEnhancer</em> is used to refine the image by adding more details to improve the image quality. The input of the <em>GlobalGenerator</em> in pix2pixHD is the downsampled label map $s_{\text{down}}$, and the output is the corse generated image $\hat{x}_{\text{down}}$. With the addition operation with the feature map $Enc_{\text{L}}(s)$: $(Enc_{\text{L}}(s) + \hat{x}_{\text{down}})$, the <em>LocalEnhancer</em> output the final $\hat{x}$.</p>
<p>In RetiGAN, the simply using the <em>GlobalGenerator</em> as the generator, since the retinal image are more simplier than the image using in pix2pixHD, where the fundus image are all indomain information, thus, using one generator can reduce model complexity.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## ResBlock ##</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">ResBlock</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel):
</span></span><span style="display:flex;"><span>        super(ResBlock, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>resblock <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channel, in_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(in_channel),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channel, in_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(in_channel)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> x <span style="color:#f92672">+</span> self<span style="color:#f92672">.</span>resblock(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Generator ##</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GlobalGenerator</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, out_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_enc<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_bottle<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;trans&#34;</span>):
</span></span><span style="display:flex;"><span>        super(GlobalGenerator, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>in_channel, out_channels<span style="color:#f92672">=</span>filters, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(filters, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_enc):
</span></span><span style="display:flex;"><span>            multiplier <span style="color:#f92672">=</span> filters <span style="color:#f92672">*</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">**</span> i)
</span></span><span style="display:flex;"><span>            encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>multiplier, out_channels<span style="color:#f92672">=</span>multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>))
</span></span><span style="display:flex;"><span>            encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>InstanceNorm2d(multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>            encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        resblocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_bottle):
</span></span><span style="display:flex;"><span>            resblocks<span style="color:#f92672">.</span>append(ResBlock(multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        decoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([])
</span></span><span style="display:flex;"><span>        <span style="color:#75715e">#multiplier = multiplier * 2</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_enc):
</span></span><span style="display:flex;"><span>            multiplier <span style="color:#f92672">=</span> int(filters <span style="color:#f92672">*</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">**</span> (n_enc <span style="color:#f92672">-</span> i)))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> mode <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;trans&#34;</span>:
</span></span><span style="display:flex;"><span>                decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels<span style="color:#f92672">=</span>multiplier, out_channels<span style="color:#f92672">=</span>int(multiplier <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, output_padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">elif</span> mode <span style="color:#f92672">==</span> <span style="color:#e6db74">&#34;shuffle&#34;</span>:
</span></span><span style="display:flex;"><span>                decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>multiplier, out_channels<span style="color:#f92672">=</span>int(multiplier <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>) <span style="color:#f92672">*</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">**</span> <span style="color:#ae81ff">2</span>), kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>                decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>PixelShuffle(<span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">else</span>:
</span></span><span style="display:flex;"><span>                decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Upsample(scale_factor<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;bilinear&#34;</span>, align_corners<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>                decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>multiplier, out_channels<span style="color:#f92672">=</span>int(multiplier <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>))
</span></span><span style="display:flex;"><span>            decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>InstanceNorm2d(int(multiplier <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>            decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>        decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>int(multiplier <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), out_channels<span style="color:#f92672">=</span>out_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>))
</span></span><span style="display:flex;"><span>        decoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Tanh())
</span></span><span style="display:flex;"><span>        model <span style="color:#f92672">=</span> encoder <span style="color:#f92672">+</span> resblocks <span style="color:#f92672">+</span> decoder
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>model)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">LocalEnhancer</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, out_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_enc<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_bottle<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>):
</span></span><span style="display:flex;"><span>        super(LocalEnhancer, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># G2 Encoder </span>
</span></span><span style="display:flex;"><span>        encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>in_channel, out_channels<span style="color:#f92672">=</span>int(filters <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>InstanceNorm2d(int(filters <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>),
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>int(filters <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), out_channels<span style="color:#f92672">=</span>filters, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>))
</span></span><span style="display:flex;"><span>        encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>InstanceNorm2d(filters, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>        encoder<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>ReLU(inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>encoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>encoder)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># G1 Encoder-Decoder</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>generator <span style="color:#f92672">=</span> GlobalGenerator(in_channel<span style="color:#f92672">=</span>in_channel, out_channel<span style="color:#f92672">=</span>out_channel, filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_enc<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_bottle<span style="color:#f92672">=</span><span style="color:#ae81ff">9</span>)<span style="color:#f92672">.</span>model[:<span style="color:#f92672">-</span><span style="color:#ae81ff">3</span>]
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># G2 ResBlock</span>
</span></span><span style="display:flex;"><span>        resblocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(<span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>            resblocks<span style="color:#f92672">.</span>append(ResBlock(filters))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>resblocks <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>resblocks)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># G2 Decoder</span>
</span></span><span style="display:flex;"><span>        decoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>ConvTranspose2d(in_channels<span style="color:#f92672">=</span>filters, out_channels<span style="color:#f92672">=</span>int(filters <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, output_padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>int(filters <span style="color:#f92672">/</span> <span style="color:#ae81ff">2</span>), out_channels<span style="color:#f92672">=</span>out_channel, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">7</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, padding_mode<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;reflect&#34;</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Tanh()
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>decoder <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>decoder)
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># Downsampling for G1</span>
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>downsample <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(<span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, count_include_pad<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        enc_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>encoder(x)
</span></span><span style="display:flex;"><span>        gen_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>generator(self<span style="color:#f92672">.</span>downsample(x))
</span></span><span style="display:flex;"><span>        res_out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>resblocks(torch<span style="color:#f92672">.</span>add(enc_out, gen_out))
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>decoder(res_out)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h3 id="discriminator-d">Discriminator $D$</h3>
<p>The basic idea of the discriminator is following this two equation $D(\hat{s}, \hat{x}) \mapsto \mathbf{0}$, and $D(s, x) \mapsto \mathbf{1}$, where $D$ measures the distance between fake image pairs with zero, and real image pairs with one.</p>
<p>In this paper (or in pix2pixHD), they adapted the discriminator architecutre proposed by <a href="https://arxiv.org/abs/1611.07004v3">PatchGAN</a>, where the PatchGAN discriminator takes in image patches rather than an entire image. These patches are typically small square regions of the input image, such as 70x70 or 256x256 pixels. The PatchGAN discriminator produces a matrix of scalar values that represent the probability of each patch being real or fake. By operating at the patch level, the PatchGAN discriminator is able to capture more fine-grained details of the image and provide more precise feedback to the generator. This can lead to higher quality image generation.</p>
<p>Instead of using one discriminator, they use two discriminator, $D_1$ is for the original size images and $D_2$ is for the down-sampled images.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## PatchDiscriminator</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PatchDiscriminator</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_blocks<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>        super(PatchDiscriminator, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>Conv2d(in_channels<span style="color:#f92672">=</span>in_channel, out_channels<span style="color:#f92672">=</span>filters, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>),
</span></span><span style="display:flex;"><span>            nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>)
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_blocks<span style="color:#f92672">-</span><span style="color:#ae81ff">1</span>):
</span></span><span style="display:flex;"><span>            multiplier <span style="color:#f92672">=</span> filters <span style="color:#f92672">*</span> (<span style="color:#ae81ff">2</span> <span style="color:#f92672">**</span> i)
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(multiplier, multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>InstanceNorm2d(multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, affine<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>))
</span></span><span style="display:flex;"><span>            self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>LeakyReLU(<span style="color:#ae81ff">0.2</span>, inplace<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">2</span>, multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model<span style="color:#f92672">.</span>append(nn<span style="color:#f92672">.</span>Conv2d(multiplier <span style="color:#f92672">*</span> <span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>, kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">4</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>))
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>Sequential(<span style="color:#f92672">*</span>self<span style="color:#f92672">.</span>model)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>model(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## MultiScaleDiscriminator</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">MultiScaleDiscriminator</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    By default, there are three separate sub-discriminator(D1, D2, D3) to generate prediction
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    They all have the same architectures but D2 and D3 operate on inputs downsampled by 2x and 4x, respectively
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, in_channel<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, filters<span style="color:#f92672">=</span><span style="color:#ae81ff">64</span>, n_blocks<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, n_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>        super(MultiScaleDiscriminator, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_dim <span style="color:#f92672">=</span> n_dim
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>downsample <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>AvgPool2d(kernel_size<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>, stride<span style="color:#f92672">=</span><span style="color:#ae81ff">2</span>, padding<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, count_include_pad<span style="color:#f92672">=</span><span style="color:#66d9ef">False</span>)
</span></span><span style="display:flex;"><span>        model_template <span style="color:#f92672">=</span> PatchDiscriminator(in_channel<span style="color:#f92672">=</span>in_channel, filters<span style="color:#f92672">=</span>filters, n_blocks<span style="color:#f92672">=</span>n_blocks)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(n_dim):
</span></span><span style="display:flex;"><span>            setattr(self, <span style="color:#e6db74">&#34;model</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i), model_template)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> i <span style="color:#f92672">in</span> range(self<span style="color:#f92672">.</span>n_dim):
</span></span><span style="display:flex;"><span>            model <span style="color:#f92672">=</span> getattr(self, <span style="color:#e6db74">&#34;model</span><span style="color:#e6db74">{}</span><span style="color:#e6db74">&#34;</span><span style="color:#f92672">.</span>format(i))
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">if</span> i <span style="color:#f92672">!=</span> <span style="color:#ae81ff">0</span>:
</span></span><span style="display:flex;"><span>                x <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>downsample(x)
</span></span><span style="display:flex;"><span>            out<span style="color:#f92672">.</span>append(model(x))
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h3 id="vgg-perceptual-information-extraction">VGG (Perceptual Information Extraction)</h3>
<p>The VGG architecture consists of a series of convolutional layers with small 3x3 filters, followed by max pooling layers. The number of filters in each convolutional layer is gradually increased as the network gets deeper. The final layers of the network consist of fully connected layers that perform the classification task.</p>
<p>VGG is often used in conjunction with perceptual loss in deep learning applications, particularly in image synthesis and style transfer tasks.</p>
<p>Perceptual loss<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> is a type of loss function that is based on the idea of using a pre-trained deep neural network, such as VGG, to measure the similarity between two images. In the context of image synthesis, the goal is to generate an image that is visually similar to a target image. Perceptual loss can be used to measure the difference between the generated image and the target image in terms of their high-level features, such as texture, color, and object structure.</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">Perception</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Compute the perceptual information based on VGG pretained model 
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    </span><span style="color:#ae81ff">\b</span><span style="color:#e6db74">egin{align*}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        \mathcal</span><span style="color:#e6db74">{L}</span><span style="color:#e6db74">_{</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">ext</span><span style="color:#e6db74">{VGG}</span><span style="color:#e6db74">} = \mathbb</span><span style="color:#e6db74">{E}</span><span style="color:#e6db74">_{s,x}\left[\sum_{i=1}^N\dfrac</span><span style="color:#e6db74">{1}{M_i}</span><span style="color:#e6db74">\left|\left|F^i(x) - F^i(G(s))</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight|</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight|_1</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    \end{align*}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(Perception, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        vgg19 <span style="color:#f92672">=</span> torchvision<span style="color:#f92672">.</span>models<span style="color:#f92672">.</span>vgg19(weights<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;DEFAULT&#34;</span>)<span style="color:#f92672">.</span>features
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>models <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>ModuleList([
</span></span><span style="display:flex;"><span>            vgg19[:<span style="color:#ae81ff">2</span>],
</span></span><span style="display:flex;"><span>            vgg19[<span style="color:#ae81ff">2</span>:<span style="color:#ae81ff">7</span>],
</span></span><span style="display:flex;"><span>            vgg19[<span style="color:#ae81ff">7</span>:<span style="color:#ae81ff">12</span>],
</span></span><span style="display:flex;"><span>            vgg19[<span style="color:#ae81ff">12</span>:<span style="color:#ae81ff">21</span>],
</span></span><span style="display:flex;"><span>            vgg19[<span style="color:#ae81ff">21</span>:<span style="color:#ae81ff">30</span>]
</span></span><span style="display:flex;"><span>        ])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#75715e"># no need to update the parameters</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> param <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>parameters():
</span></span><span style="display:flex;"><span>            param<span style="color:#f92672">.</span>requires_grad <span style="color:#f92672">=</span> <span style="color:#66d9ef">False</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, x):
</span></span><span style="display:flex;"><span>        out <span style="color:#f92672">=</span> []
</span></span><span style="display:flex;"><span>        out<span style="color:#f92672">.</span>append(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> model <span style="color:#f92672">in</span> self<span style="color:#f92672">.</span>models:
</span></span><span style="display:flex;"><span>            x <span style="color:#f92672">=</span> model(x)
</span></span><span style="display:flex;"><span>            out<span style="color:#f92672">.</span>append(x)
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> out
</span></span></code></pre></div><h3 id="loss-function">Loss function</h3>
<p>In general, there are two losses: <em>Generato Loss</em> $\mathcal{L}_G$ and <em>Discriminator Loss</em> $\mathcal{L}_D$.</p>
<p>The <em>Generator Loss</em> includes these three sub-loss:</p>
<ol>
<li>
<p>Adversarial loss: $\mathcal{L}_{\text{adv}}$: is used to train the generator network by making it generate synthetic data that can fool the discriminator network into thinking that it is real data. The adversarial loss is computed based on the output of the discriminator network.$\mathcal{L}_{\text{adv}}= \left|\left|D(G(s)), 1\right|\right|_2$</p>
</li>
<li>
<p>Feature Matching Loss $\mathcal{L}_{\text{FM}}$: In pix2pixHD, the authors found this to stabilize training. In this case, this forces the generator to produce natural statistics at multiple scales. This feature-matching loss is similar to StyleGAN’s<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> perceptual loss. For some semantic label maps s and corresponding image $x$: $\mathcal{L}_{\text{FM}} = \mathbb{E}_{s,x}\left[\sum_{i=1}^T\dfrac{1}{N_i}\left|\left|D^{(i)}_k(s, x) - D^{(i)}_k(s, G(s))\right|\right|_1\right]$, where $T$ is the total number of layers, $N_i$ is the number of elements at layer $i$ and $D_k^{(i)}$ denotes the $i$-th layer in discriminator $k$.</p>
</li>
<li>
<p>Peceptual Loss $\mathcal{L}_{\text{VGG}}$: In pix2pixHD, the authors report minor performance improvements when adding perceptual loss, formulated as $\mathcal{L}_{\text{VGG}} = \mathbb{E}_{s,x}\left[\sum_{i=1}^N\dfrac{1}{M_i}\left|\left|F^i(x) - F^i(G(s))\right|\right|_1\right]$, where $F^i$ denotes the $i$th layer with $M_i$ elements of the VGG19 network.</p>
</li>
</ol>
<p>Overall, $\mathcal{L}_G = \lambda_0\mathcal{L}_{\text{GAN}} + \lambda_1 \mathcal{L}_{\text{FM}} + \lambda_2 \mathcal{L}_{\text{VGG}}$, where $\lambda_i$ are the parameters.</p>
<p>The Discriminator Loss in this paper is similar to the PatchGAN’s. However, to solve the blurring and incomplete edges, they include an additional set of images $x_b$ , where $x_b$ are generated by removing precise edges in training images x (Canny edge detector &amp; Gaussian filter). After the smoothed images x_b are included into the training set, the main task for the discriminator is not only to improve the ability of discriminating the generated from the real ones, but also to discriminate the smoothed and the clear ones.</p>
<p>$\mathcal{L}_D = \mathbb{E}[logD(x)] + \mathbb{E}[log(1 - D(\hat{x}))] + \mathbb{E}[log(1 - D(\hat{x_b}))]$</p>
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#272822;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#75715e">## Perceptual Loss</span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">PerceptualLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    compute perceptual loss with VGG network from both real and fake images (updating the Generator)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, device<span style="color:#f92672">=</span><span style="color:#e6db74">&#34;cpu&#34;</span>):
</span></span><span style="display:flex;"><span>        super(PerceptualLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>model <span style="color:#f92672">=</span> Perception()<span style="color:#f92672">.</span>to(device)
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>L1Loss()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>LAMBDA <span style="color:#f92672">=</span> [<span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">32</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">16</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">8</span>, <span style="color:#ae81ff">1</span><span style="color:#f92672">/</span><span style="color:#ae81ff">4</span>, <span style="color:#ae81ff">1</span>]
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, predict, target):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> real, fake, weight <span style="color:#f92672">in</span> zip(self<span style="color:#f92672">.</span>model(target), self<span style="color:#f92672">.</span>model(predict), self<span style="color:#f92672">.</span>LAMBDA):
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">+=</span> weight <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>criterion(real<span style="color:#f92672">.</span>detach(), fake)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#75715e">## Adversarial Loss </span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">AdversarialLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    computes adversarial loss from nested list of fakes outputs from discriminator. (updating the Generator)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(AdversarialLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>MSELoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, predict, is_real<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        target <span style="color:#f92672">=</span> torch<span style="color:#f92672">.</span>ones_like <span style="color:#66d9ef">if</span> is_real <span style="color:#66d9ef">else</span> torch<span style="color:#f92672">.</span>zeros_like
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> preds <span style="color:#f92672">in</span> predict:
</span></span><span style="display:flex;"><span>            loss <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>criterion(preds, target(preds))
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">FeatureMatchLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Compute feature matching loss from nested lists of fake and real outputs from discriminator. (updating the Ganerator)
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    https://proceedings.neurips.cc/paper/2016/file/8a3363abe792db2d8761d6403605aeb7-Paper.pdf
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    </span><span style="color:#ae81ff">\b</span><span style="color:#e6db74">egin{align*}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">        \mathcal</span><span style="color:#e6db74">{L}</span><span style="color:#e6db74">_{</span><span style="color:#ae81ff">\t</span><span style="color:#e6db74">ext</span><span style="color:#e6db74">{FM}</span><span style="color:#e6db74">} = \mathbb</span><span style="color:#e6db74">{E}</span><span style="color:#e6db74">_{s,x}\left[\sum_{i=1}^T\dfrac</span><span style="color:#e6db74">{1}{N_i}</span><span style="color:#e6db74">\left|\left|D^{(i)}_k(s, x) - D^{(i)}_k(s, G(s))</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight|</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight|_1</span><span style="color:#ae81ff">\r</span><span style="color:#e6db74">ight]
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    \end{align*}
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self):
</span></span><span style="display:flex;"><span>        super(FeatureMatchLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>criterion <span style="color:#f92672">=</span> nn<span style="color:#f92672">.</span>L1Loss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, predict, target):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> <span style="color:#ae81ff">0.0</span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">for</span> real_feature, fake_feature <span style="color:#f92672">in</span> zip(target, predict):
</span></span><span style="display:flex;"><span>            <span style="color:#66d9ef">for</span> real, fake <span style="color:#f92672">in</span> zip(real_feature, fake_feature):
</span></span><span style="display:flex;"><span>                loss <span style="color:#f92672">+=</span> self<span style="color:#f92672">.</span>criterion(real<span style="color:#f92672">.</span>detach(), fake)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#66d9ef">class</span> <span style="color:#a6e22e">GeneratorLoss</span>(nn<span style="color:#f92672">.</span>Module):
</span></span><span style="display:flex;"><span>    <span style="color:#e6db74">&#39;&#39;&#39;
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    Combine the Adversarial-Loss, Feature Maching Loss, and Perceptual Loss together with different weights
</span></span></span><span style="display:flex;"><span><span style="color:#e6db74">    &#39;&#39;&#39;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> __init__(self, LAMBDA0<span style="color:#f92672">=</span><span style="color:#ae81ff">1</span>, LAMBDA1<span style="color:#f92672">=</span><span style="color:#ae81ff">100</span>, LAMBDA2<span style="color:#f92672">=</span><span style="color:#ae81ff">10</span>, n_dim<span style="color:#f92672">=</span><span style="color:#ae81ff">3</span>):
</span></span><span style="display:flex;"><span>        super(GeneratorLoss, self)<span style="color:#f92672">.</span>__init__()
</span></span><span style="display:flex;"><span>        SCALE <span style="color:#f92672">=</span> LAMBDA0 <span style="color:#f92672">+</span> LAMBDA1 <span style="color:#f92672">+</span> LAMBDA2
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>LAMBDA <span style="color:#f92672">=</span> [LAMBDA0 <span style="color:#f92672">/</span> SCALE, LAMBDA1 <span style="color:#f92672">/</span> SCALE, LAMBDA2 <span style="color:#f92672">/</span> SCALE]
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>n_dim <span style="color:#f92672">=</span> n_dim
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>adv_loss <span style="color:#f92672">=</span> AdversarialLoss()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>per_loss <span style="color:#f92672">=</span> PerceptualLoss()
</span></span><span style="display:flex;"><span>        self<span style="color:#f92672">.</span>fm_loss <span style="color:#f92672">=</span> FeatureMatchLoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#66d9ef">def</span> <span style="color:#a6e22e">forward</span>(self, fake, real, predict, target):
</span></span><span style="display:flex;"><span>        loss <span style="color:#f92672">=</span> self<span style="color:#f92672">.</span>LAMBDA[<span style="color:#ae81ff">0</span>] <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>adv_loss(predict<span style="color:#f92672">=</span>predict, is_real<span style="color:#f92672">=</span><span style="color:#66d9ef">True</span>) <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>         self<span style="color:#f92672">.</span>LAMBDA[<span style="color:#ae81ff">1</span>] <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>per_loss(predict<span style="color:#f92672">=</span>fake, target<span style="color:#f92672">=</span>real) <span style="color:#f92672">+</span> \
</span></span><span style="display:flex;"><span>         self<span style="color:#f92672">.</span>LAMBDA[<span style="color:#ae81ff">2</span>] <span style="color:#f92672">*</span> self<span style="color:#f92672">.</span>fm_loss(predict<span style="color:#f92672">=</span>predict, target<span style="color:#f92672">=</span>target) <span style="color:#f92672">/</span> self<span style="color:#f92672">.</span>n_dim
</span></span><span style="display:flex;"><span>        <span style="color:#66d9ef">return</span> loss
</span></span></code></pre></div><div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The U-Net is not a <em>encoder-decoder</em> architecture, as it does not contain a &ldquo;latant&rdquo; layer, however, the shapes are similar, to simply, I just call it as <em>encoder-decoder</em>.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>To see more about perceptual loss, you may check <a href="https://arxiv.org/abs/1603.08155">Perceptual Losses for Real-Time Style Transfer and Super-Resolution</a>&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p><a href="https://nvlabs.github.io/stylegan2/versions.html">StyleGAN, from 1 to 3</a>&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>
</content>

<p>
  
  <a href="https://blog.cklau.cc/tags/artificial-intelligence/">#Artificial Intelligence</a>
  
  <a href="https://blog.cklau.cc/tags/medical-image/">#Medical Image</a>
  
  <a href="https://blog.cklau.cc/tags/fundus-image/">#Fundus Image</a>
  
</p>

  </main>
  <footer>


<p>
    <span><a href="https://beian.miit.gov.cn/" rel="noopener" target="_blank">粤ICP备2022102668号</a></span>

&copy; 2024 <a href="https://blog.cklau.cc">Junjie LIU</a>
    <a href="https://blog.cklau.cc/index.xml" target="_blank" title="rss"><svg xmlns="http://www.w3.org/2000/svg" width="24" height="24" viewBox="0 0 24 24" fill="none" stroke="currentColor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round" class="feather feather-rss"><path d="M4 11a9 9 0 0 1 9 9"></path><path d="M4 4a16 16 0 0 1 16 16"></path><circle cx="5" cy="19" r="1"></circle></svg></a>
    <br>
Hosting and images served by <a href="https://cloud.tencent.com/">Tencent Cloud</a> / <a href="https://cloudflare.com/">Cloudflare</a> / <a href="https://webp.se/">WebP Cloud Services</a>
</p>

</footer>

    
</body>

</html>
