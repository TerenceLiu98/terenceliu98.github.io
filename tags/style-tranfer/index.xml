<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Style Tranfer | ÁâπÂÄ´ËòáÁöÑÊó•ËàáÂ§ú</title>
    <link>https://blog.cklau.cc/tags/style-tranfer/</link>
      <atom:link href="https://blog.cklau.cc/tags/style-tranfer/index.xml" rel="self" type="application/rss+xml" />
    <description>Style Tranfer</description>
    <generator>Source Themes Academic (https://sourcethemes.com/academic/)</generator><language>en</language><lastBuildDate>Sat, 17 Jun 2023 00:13:00 +0800</lastBuildDate>
    <item>
      <title>üë®‚Äçüíª Style Transfer and Synthesis (1/3): Style Transfer in Image Synthesis</title>
      <link>https://blog.cklau.cc/post/scientia/dl-gan-1/</link>
      <pubDate>Sat, 17 Jun 2023 00:13:00 +0800</pubDate>
      <guid>https://blog.cklau.cc/post/scientia/dl-gan-1/</guid>
      <description>&lt;p&gt;There are multiple papers in the area of style transfer and image inversion or reconstruction. Here are some papers I read and would like to share with you:&lt;/p&gt;
&lt;h3 id=&#34;progressive-gan&#34;&gt;Progressive GAN&lt;/h3&gt;
&lt;ul&gt;
&lt;li&gt;Paper URL: &lt;a href=&#34;https://arxiv.org/abs/1710.10196&#34;&gt;ICLR 2018 - PROGRESSIVE GROWING OF GANS FOR IMPROVED QUALITY, STABILITY, AND VARIATION&lt;/a&gt; |&lt;/li&gt;
&lt;li&gt;Code URL: &lt;a href=&#34;https://git.cklau.cc/terenceliu/gans-models/-/tree/main/PGGAN&#34;&gt;https://git.cklau.cc/terenceliu/gans-models/-/tree/main/PGGAN&lt;/a&gt; |&lt;/li&gt;
&lt;/ul&gt;
&lt;blockquote&gt;
&lt;p&gt;&lt;strong&gt;Key point&lt;/strong&gt;: They propose a novel training process of GAN model: progressive training, i.e. from small model to big model/from low resolution to high resolution&lt;/p&gt;
&lt;/blockquote&gt;
&lt;h4 id=&#34;some-hightlights&#34;&gt;Some Hightlights&lt;/h4&gt;
&lt;ol&gt;
&lt;li&gt;The training (see Figure A) is from left to right,
&lt;ol&gt;
&lt;li&gt;start from  feature map, the model produce a (3x4x4) output from the generator $G_1$ and as the input of the discriminator $D_1$&lt;/li&gt;
&lt;li&gt;the second process is to upsample the feature map from $4 \times 4$ to $8 \times 8$ and produce a ($3 \times 8 \times 8$) output from the generator $G_2$ and as the input of the discriminator $D_2$&lt;/li&gt;
&lt;li&gt;continue with the process, for the last part of the progression, the model output the $1024 \times 1024$ image from the generator and as the input of the discriminator $D_m$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;To avoid the influence/damage of the transition from low resolution to high resolution, they fade in the new layer smoothly (see Figure B):
&lt;ol&gt;
&lt;li&gt;they treat the layers that operate on the higher resolution like a residual block, whose weight $\alpha$ increases linearly from $0$ to $1$.&lt;/li&gt;
&lt;li&gt;By adjusting the weight of convolution-based output and upsampling-based output to control the final output: $(\alpha \cdot O_{\text{convolution layer}} + (1 - \alpha) \cdot O_{\text{upsample layer}})$&lt;/li&gt;
&lt;/ol&gt;
&lt;/li&gt;
&lt;li&gt;Minibatch Standard Deviation&lt;/li&gt;
&lt;li&gt;Equalize the Learning Rate&lt;/li&gt;
&lt;li&gt;Pixelwised Noramlization&lt;/li&gt;
&lt;/ol&gt;</description>
    </item>
    
  </channel>
</rss>